{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperJAV Colab Installation Diagnostic\n",
    "\n",
    "This notebook tests the `install_colab.sh` script by cloning the repo and running locally.\n",
    "\n",
    "**Instructions:**\n",
    "1. Run each cell **one at a time** (Shift+Enter)\n",
    "2. Note which cell fails and what error message you see\n",
    "3. Report findings for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Check Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU, CUDA, and Python environment\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COLAB ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# GPU Info\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=name,driver_version,memory.total\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(f\"GPU: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"ERROR: nvidia-smi failed - no GPU available?\")\n",
    "    print(\"Go to Runtime → Change runtime type → T4 GPU\")\n",
    "    raise SystemExit(\"No GPU detected\")\n",
    "\n",
    "print(f\"\\nPython: {sys.version}\")\n",
    "\n",
    "# Check Colab's pre-installed PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nColab PyTorch: {torch.__version__}\")\n",
    "    print(f\"Colab PyTorch CUDA: {torch.version.cuda}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"\\nPyTorch not pre-installed (unusual for Colab)\")\n",
    "\n",
    "# Check numpy version (this is the conflict source)\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"\\nColab numpy: {np.__version__}\")\n",
    "    if np.__version__.startswith(\"2.\"):\n",
    "        print(\"⚠ numpy 2.x detected - this is why we need an isolated venv\")\n",
    "except ImportError:\n",
    "    print(\"numpy not installed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Environment check: PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repo & Run Installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone WhisperJAV repo and run installer locally\n# Uses Popen for real-time output streaming in Jupyter\n\nimport subprocess\nimport os\nimport sys\nimport time\n\nREPO_URL = \"https://github.com/meizhong986/WhisperJAV.git\"\nREPO_PATH = \"/content/WhisperJAV\"\nSCRIPT_PATH = f\"{REPO_PATH}/installer/install_colab.sh\"\n\nprint(\"=\"*60)\nprint(\"INSTALL WHISPERJAV\")\nprint(\"=\"*60)\n\n# Step 1: Clone repo (or pull if exists)\nif os.path.exists(REPO_PATH):\n    print(f\"Repo already exists at {REPO_PATH}\")\n    print(\"Pulling latest changes...\")\n    result = subprocess.run([\"git\", \"-C\", REPO_PATH, \"pull\"], capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"Warning: git pull failed: {result.stderr}\")\n    else:\n        print(f\"✓ Pulled latest\")\nelse:\n    print(f\"Cloning {REPO_URL}...\")\n    result = subprocess.run([\"git\", \"clone\", REPO_URL, REPO_PATH], capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"ERROR: git clone failed\")\n        print(result.stderr)\n        raise SystemExit(\"Failed to clone repository\")\n    print(f\"✓ Cloned to {REPO_PATH}\")\n\n# Step 2: Verify script exists\nif not os.path.exists(SCRIPT_PATH):\n    print(f\"\\nERROR: Install script not found at {SCRIPT_PATH}\")\n    print(\"\\nThis means the script hasn't been committed to the repo yet.\")\n    print(\"The install_colab.sh must be pushed to GitHub first.\")\n    raise SystemExit(\"Install script not found in repo\")\n\nprint(f\"✓ Script found: {SCRIPT_PATH}\")\n\n# Step 3: Run installer with real-time output streaming\nprint(\"\\n\" + \"-\"*60)\nprint(\"Running install_colab.sh...\")\nprint(\"-\"*60 + \"\\n\")\nsys.stdout.flush()\n\nstart_time = time.time()\n\n# Use Popen for real-time output streaming in Jupyter\nenv = {**os.environ, \"PATH\": f\"{os.environ.get('PATH', '')}:{os.path.expanduser('~/.local/bin')}\"}\nprocess = subprocess.Popen(\n    [\"bash\", SCRIPT_PATH],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    bufsize=1,\n    text=True,\n    env=env\n)\n\n# Stream output line by line\nfor line in iter(process.stdout.readline, ''):\n    print(line, end='', flush=True)\n\nprocess.wait()\nelapsed = time.time() - start_time\n\nprint(\"\\n\" + \"=\"*60)\nif process.returncode != 0:\n    print(f\"ERROR: Installation FAILED (exit code {process.returncode})\")\n    print(\"=\"*60)\n    raise SystemExit(f\"Installation failed with exit code {process.returncode}\")\nelse:\n    print(f\"✓ Installation SUCCEEDED ({elapsed:.0f} seconds)\")\n    print(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Verify Venv Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify the venv was created correctly\nimport os\n\nVENV_PATH = \"/content/whisperjav_env\"\n\nprint(\"=\"*60)\nprint(\"VENV STRUCTURE CHECK\")\nprint(\"=\"*60)\n\n# Required components\nrequired_checks = [\n    (f\"{VENV_PATH}/bin/python\", \"Python interpreter\"),\n    (f\"{VENV_PATH}/bin/whisperjav\", \"whisperjav CLI\"),\n    (f\"{VENV_PATH}/bin/whisperjav-translate\", \"whisperjav-translate CLI\"),\n]\n\n# Optional components (uv venvs don't include pip by default)\noptional_checks = [\n    (f\"{VENV_PATH}/bin/pip\", \"pip (optional with uv)\"),\n]\n\nfailed = []\nfor path, name in required_checks:\n    exists = os.path.exists(path)\n    status = \"✓\" if exists else \"✗\"\n    print(f\"{status} {name}: {path}\")\n    if not exists:\n        failed.append(name)\n\nfor path, name in optional_checks:\n    exists = os.path.exists(path)\n    status = \"✓\" if exists else \"○\"  # Circle for optional missing\n    print(f\"{status} {name}: {path}\")\n\nprint(\"\\n\" + \"=\"*60)\nif failed:\n    print(f\"ERROR: Venv structure check FAILED\")\n    print(f\"Missing: {', '.join(failed)}\")\n    print(\"=\"*60)\n    raise SystemExit(\"Venv structure incomplete\")\nelse:\n    print(\"✓ Venv structure: PASSED\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Verify Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify packages are installed in the venv\nimport subprocess\nimport os\n\n# Fix matplotlib backend conflict with isolated venv\n# Colab sets MPLBACKEND to 'matplotlib_inline' which isn't in the venv\nos.environ['MPLBACKEND'] = 'Agg'\n\nVENV_PYTHON = \"/content/whisperjav_env/bin/python\"\n\nprint(\"=\"*60)\nprint(\"VENV PACKAGE VERIFICATION\")\nprint(\"=\"*60)\n\npackages_to_check = [\n    (\"torch\", \"import torch; print(f'PyTorch {torch.__version__}, CUDA {torch.version.cuda}, available={torch.cuda.is_available()}')\"),\n    (\"numpy\", \"import numpy; print(f'numpy {numpy.__version__}')\"),\n    (\"whisperjav\", \"import whisperjav; print('whisperjav OK')\"),\n    (\"whisper\", \"import whisper; print('whisper OK')\"),\n    (\"stable_whisper\", \"import stable_whisper; print('stable_whisper OK')\"),\n    (\"faster_whisper\", \"import faster_whisper; print('faster_whisper OK')\"),\n]\n\nfailed = []\nfor name, check_cmd in packages_to_check:\n    result = subprocess.run([VENV_PYTHON, \"-c\", check_cmd], capture_output=True, text=True)\n    if result.returncode == 0:\n        print(f\"✓ {name}: {result.stdout.strip()}\")\n    else:\n        print(f\"✗ {name}: FAILED\")\n        if result.stderr:\n            print(f\"  Error: {result.stderr.strip()[:200]}\")\n        failed.append(name)\n\n# Verify numpy is < 2.0 in venv\nprint(\"\\n\" + \"-\"*60)\nresult = subprocess.run(\n    [VENV_PYTHON, \"-c\", \"import numpy; v=numpy.__version__; print(v); exit(0 if v.startswith('1.') else 1)\"],\n    capture_output=True, text=True\n)\nif result.returncode == 0:\n    print(f\"✓ numpy version: {result.stdout.strip()} (< 2.0 as required)\")\nelse:\n    print(f\"✗ numpy version: {result.stdout.strip()} (expected < 2.0)\")\n    failed.append(\"numpy version\")\n\nprint(\"\\n\" + \"=\"*60)\nif failed:\n    print(f\"ERROR: Package verification FAILED: {', '.join(failed)}\")\n    raise SystemExit(\"Package verification failed\")\nelse:\n    print(\"✓ Package verification: PASSED\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Test CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the CLI commands\n",
    "import subprocess\n",
    "\n",
    "VENV_PATH = \"/content/whisperjav_env\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLI TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = subprocess.run([f\"{VENV_PATH}/bin/whisperjav\", \"--help\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"whisperjav --help:\")\n",
    "    print(\"\\n\".join(result.stdout.strip().split(\"\\n\")[:15]))\n",
    "    print(\"...\\n\")\n",
    "else:\n",
    "    print(f\"ERROR: whisperjav --help failed\")\n",
    "    raise SystemExit(\"CLI test failed\")\n",
    "\n",
    "result = subprocess.run([f\"{VENV_PATH}/bin/whisperjav-translate\", \"--help\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"whisperjav-translate --help:\")\n",
    "    print(\"\\n\".join(result.stdout.strip().split(\"\\n\")[:10]))\n",
    "    print(\"...\")\n",
    "else:\n",
    "    print(f\"ERROR: whisperjav-translate --help failed\")\n",
    "    raise SystemExit(\"CLI test failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ CLI test: PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 6: Test llama-cpp-python Lazy Download\n\nThis cell tests the lazy download of llama-cpp-python from HuggingFace (`mei986/whisperjav-wheels`).\n\nExpected behavior:\n1. Detects CUDA 12.6 on Colab\n2. Downloads prebuilt wheel from HuggingFace (cu126)\n3. Installs and verifies GPU support"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test llama-cpp-python lazy download from HuggingFace\nimport subprocess\nimport os\n\nVENV_PYTHON = \"/content/whisperjav_env/bin/python\"\n\nprint(\"=\"*60)\nprint(\"LLAMA-CPP-PYTHON LAZY DOWNLOAD TEST\")\nprint(\"=\"*60)\n\n# First check if already installed\nresult = subprocess.run(\n    [VENV_PYTHON, \"-c\", \"import llama_cpp; print(f'Version: {llama_cpp.__version__}')\"],\n    capture_output=True, text=True\n)\n\nif result.returncode == 0:\n    print(f\"Already installed: {result.stdout.strip()}\")\nelse:\n    print(\"Not installed yet. Testing lazy download...\")\n    print(\"\")\n    \n    # Trigger lazy download via ensure_llama_cpp_installed\n    test_code = '''\nimport sys\nsys.path.insert(0, \"/content/WhisperJAV\")\nfrom whisperjav.translate.local_backend import ensure_llama_cpp_installed\n\nprint(\"Triggering lazy download...\")\nresult = ensure_llama_cpp_installed()\nprint(f\"Result: {result}\")\n\nif result:\n    import llama_cpp\n    print(f\"Version installed: {llama_cpp.__version__}\")\n'''\n    \n    result = subprocess.run(\n        [VENV_PYTHON, \"-c\", test_code],\n        capture_output=False,  # Show output in real-time\n        text=True\n    )\n\n# Final verification\nprint(\"\\n\" + \"-\"*60)\nprint(\"Final verification:\")\nresult = subprocess.run(\n    [VENV_PYTHON, \"-c\", \"\"\"\nimport llama_cpp\nprint(f\"llama-cpp-python version: {llama_cpp.__version__}\")\n\n# Check if CUDA/GPU support is available\ntry:\n    from llama_cpp import Llama\n    import inspect\n    sig = inspect.signature(Llama.__init__)\n    if 'n_gpu_layers' in sig.parameters:\n        print(\"GPU support: YES (n_gpu_layers available)\")\n    else:\n        print(\"GPU support: NO (CPU only)\")\nexcept Exception as e:\n    print(f\"GPU check error: {e}\")\n\"\"\"],\n    capture_output=True, text=True\n)\n\nif result.returncode == 0:\n    print(result.stdout)\n    print(\"=\"*60)\n    print(\"LLAMA-CPP-PYTHON: PASSED\")\nelse:\n    print(f\"FAILED: {result.stderr[:200] if result.stderr else 'unknown error'}\")\n    print(\"=\"*60)\n    print(\"LLAMA-CPP-PYTHON: FAILED\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "VENV_PYTHON = \"/content/whisperjav_env/bin/python\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INSTALLATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get driver info\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=driver_version,name\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    parts = result.stdout.strip().split(\", \")\n",
    "    print(f\"Driver: {parts[0]}\")\n",
    "    print(f\"GPU: {parts[1] if len(parts) > 1 else 'unknown'}\")\n",
    "\n",
    "print(f\"Colab Python: {sys.version.split()[0]}\")\n",
    "\n",
    "# Venv info\n",
    "venv_info = subprocess.run(\n",
    "    [VENV_PYTHON, \"-c\", \"\"\"\n",
    "import torch\n",
    "import numpy\n",
    "print(f\"Venv PyTorch: {torch.__version__}\")\n",
    "print(f\"Venv PyTorch CUDA: {torch.version.cuda}\")\n",
    "print(f\"Venv numpy: {numpy.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\"\"\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print(venv_info.stdout)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ ALL DIAGNOSTICS COMPLETE\")\n",
    "print(\"\\nTo use WhisperJAV:\")\n",
    "print(\"  /content/whisperjav_env/bin/whisperjav <video_path>\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}