{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_edition_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# WhisperJAV Colab Edition v1.7.4 (Expert)\n",
    "\n",
    "| User Mode | What it does | Speed |\n",
    "|------|--------------|-------|\n",
    "| **Standard** | Processes your video once | Faster |\n",
    "| **Two-Step** | Processes twice and combines for better accuracy | Slower |\n",
    "\n",
    "| Option | What it controls |\n",
    "|--------|------------------|\n",
    "| **Scene Detection** | How to split audio into chunks (auditok, silero, semantic) |\n",
    "| **Speech Segmenter** | How to detect speech in audio (silero, ten) |\n",
    "| **Speech Enhancer** | Audio cleanup for noisy sources (ffmpeg-dsp, clearvoice, etc.) |\n",
    "| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n",
    "\n",
    "---\n",
    "<div style=\"font-size: 8px; line-height: 1.0;\">\n",
    "1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n",
    "2. Run <b>Step 1: Settings</b> (required)<br>\n",
    "3. Run <b>Step 1.5: Expert Options</b> (optional - skip if unsure)<br>\n",
    "4. Run <b>Step 2: Transcribe</b> and wait for completion<br>\n",
    "5. Run <b>Step 3: AI Translation</b> (if selected)\n",
    "</div>\n",
    "\n",
    "<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure"
   },
   "outputs": [],
   "source": [
    "#@title Step 1: Settings { display-mode: \"form\" }\n",
    "\n",
    "#@markdown **Transcription**\n",
    "quality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "speech_detection = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Two-Step Processing** *(slower but more accurate)*\n",
    "use_two_step = False #@param {type:\"boolean\"}\n",
    "secondpass_quality = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "secondpass_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "secondpass_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "secondpass_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "merge_method = \"prefer first step\" #@param [\"automatic\", \"keep all\", \"prefer first step\", \"prefer second step\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Files & Output**\n",
    "folder_name = \"WhisperJAV\" #@param {type:\"string\"}\n",
    "subtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **AI Translation** *(if selected \"English (AI translate)\")*\n",
    "translation_service = \"deepseek\" #@param [\"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\n",
    "api_key = \"\" #@param {type:\"string\"}\n",
    "translation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Credits**\n",
    "opening_credit = \"\" #@param {type:\"string\"}\n",
    "closing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Session**\n",
    "auto_disconnect = True #@param {type:\"boolean\"}\n",
    "#@markdown ☝️ Auto-disconnect when done (saves GPU credits)\n",
    "\n",
    "# Mapping dictionaries\n",
    "combine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n",
    "               \"prefer first step\": \"pass1_primary\", \"prefer second step\": \"pass2_primary\"}\n",
    "language_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n",
    "                \"English (AI translate)\": \"llm\"}\n",
    "tone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n",
    "\n",
    "# Speech segmenter mapping (None = use pipeline default)\n",
    "segmenter_map = {\"automatic\": None, \"silero\": \"silero\", \"ten\": \"ten\", \"none\": \"none\"}\n",
    "\n",
    "# Model mapping (None = use pipeline default)\n",
    "model_map = {\n",
    "    \"automatic\": None,\n",
    "    \"large-v2\": \"large-v2\",\n",
    "    \"large-v3\": \"large-v3\",\n",
    "    \"turbo\": \"large-v3-turbo\",\n",
    "    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n",
    "    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n",
    "    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n",
    "    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n",
    "}\n",
    "\n",
    "# Define model compatibility:\n",
    "# - Kotoba models (HuggingFace) ONLY work with \"transformers\" pipeline\n",
    "# - Legacy models (large-v2/v3/turbo) work with ALL pipelines (faster, fast, balanced, fidelity, transformers)\n",
    "KOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\n",
    "LEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n",
    "\n",
    "# Auto-correct incompatible model-pipeline combinations\n",
    "warnings_list = []\n",
    "\n",
    "# Check Pass 1 compatibility\n",
    "if model in KOTOBA_MODELS and quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 1: {model} requires 'transformers' pipeline. Auto-correcting from '{quality}' to 'transformers'.\")\n",
    "    quality = \"transformers\"\n",
    "\n",
    "# Check Pass 2 compatibility (only relevant if two-step is enabled)\n",
    "if use_two_step and secondpass_model in KOTOBA_MODELS and secondpass_quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 2: {secondpass_model} requires 'transformers' pipeline. Auto-correcting from '{secondpass_quality}' to 'transformers'.\")\n",
    "    secondpass_quality = \"transformers\"\n",
    "\n",
    "WHISPERJAV_CONFIG = {\n",
    "    'use_two_step': use_two_step,\n",
    "    'pass1_pipeline': quality,\n",
    "    'pass1_sensitivity': speech_detection,\n",
    "    'pass1_speech_segmenter': segmenter_map[speech_segmenter],\n",
    "    'pass1_model': model_map[model],\n",
    "    'pass2_pipeline': secondpass_quality,\n",
    "    'pass2_sensitivity': secondpass_sensitivity,\n",
    "    'pass2_speech_segmenter': segmenter_map[secondpass_speech_segmenter],\n",
    "    'pass2_model': model_map[secondpass_model],\n",
    "    'merge_strategy': combine_map[merge_method],\n",
    "    'folder_name': folder_name,\n",
    "    'subtitle_language': language_map[subtitle_language],\n",
    "    'translation_service': translation_service,\n",
    "    'api_key': api_key,\n",
    "    'translation_style': tone_map[translation_style],\n",
    "    'opening_credit': opening_credit,\n",
    "    'closing_credit': closing_credit,\n",
    "    'auto_disconnect': auto_disconnect,\n",
    "    # Display values (for status messages)\n",
    "    '_quality': quality,\n",
    "    '_speech_detection': speech_detection,\n",
    "    '_speech_segmenter': speech_segmenter,\n",
    "    '_model': model,\n",
    "    '_secondpass_quality': secondpass_quality,\n",
    "    '_secondpass_sensitivity': secondpass_sensitivity,\n",
    "    '_secondpass_speech_segmenter': secondpass_speech_segmenter,\n",
    "    '_secondpass_model': secondpass_model,\n",
    "    '_merge_method': merge_method,\n",
    "    '_subtitle_language': subtitle_language,\n",
    "    '_translation_style': translation_style,\n",
    "}\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display any auto-correction warnings\n",
    "for warning in warnings_list:\n",
    "    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>⚠ Auto-corrected:</b> {warning}</div>'))\n",
    "\n",
    "# Build status display\n",
    "if use_two_step:\n",
    "    mode_text = \"Two-Step\"\n",
    "    p1_info = f\"{quality}\"\n",
    "    if speech_segmenter != \"automatic\":\n",
    "        p1_info += f\"/{speech_segmenter}\"\n",
    "    if model != \"automatic\":\n",
    "        p1_info += f\"/{model}\"\n",
    "    p2_info = f\"{secondpass_quality}\"\n",
    "    if secondpass_speech_segmenter != \"automatic\":\n",
    "        p2_info += f\"/{secondpass_speech_segmenter}\"\n",
    "    if secondpass_model != \"automatic\":\n",
    "        p2_info += f\"/{secondpass_model}\"\n",
    "    details = f\"{p1_info} → {p2_info}\"\n",
    "else:\n",
    "    mode_text = \"Standard\"\n",
    "    details = f\"{quality}/{speech_detection}\"\n",
    "    if speech_segmenter != \"automatic\":\n",
    "        details += f\"/{speech_segmenter}\"\n",
    "    if model != \"automatic\":\n",
    "        details += f\"/{model}\"\n",
    "\n",
    "display(HTML(f'<div style=\"padding:6px 10px;background:#f0f9ff;border-radius:4px;font-size:10px\"><b>Settings:</b> {mode_text} ({details}) | Folder: {folder_name} | Output: {subtitle_language}</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1.5: Expert Options (Optional) { display-mode: \"form\" }\n",
    "#@markdown <font color=\"gray\">*Skip this cell if unsure. Default settings work well for most videos.*</font>\n",
    "#@markdown\n",
    "#@markdown <font color=\"orange\">⚠️ **Memory Notice:** Speech enhancement uses additional GPU memory. If you encounter OOM errors, disable enhancement or use `ffmpeg-dsp`.</font>\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Pass 1 Settings\n",
    "\n",
    "#@markdown **Scene Detection** *(how to split audio into chunks)*\n",
    "pass1_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\n",
    "#@markdown <font size=\"1\">auditok=energy-based (fast), silero=VAD-based, semantic=texture clustering (best for complex audio)</font>\n",
    "\n",
    "#@markdown **Speech Segmenter** *(how to detect speech within chunks)*\n",
    "pass1_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "\n",
    "#@markdown **Speech Enhancer** *(audio cleanup for noisy sources)*\n",
    "pass1_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n",
    "#@markdown <font size=\"1\">none=skip, ffmpeg-dsp=filters (no GPU), clearvoice=denoise (48kHz), zipenhancer=lightweight, bs-roformer=vocal isolation</font>\n",
    "\n",
    "#@markdown **FFmpeg DSP Filters** *(only applies when ffmpeg-dsp selected above)*\n",
    "pass1_ffmpeg_amplify = True #@param {type:\"boolean\"}\n",
    "#@markdown ↳ Amplify quiet audio (recommended)\n",
    "pass1_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n",
    "#@markdown ↳ Loudness normalization\n",
    "pass1_ffmpeg_compress = False #@param {type:\"boolean\"}\n",
    "#@markdown ↳ Dynamic range compression\n",
    "pass1_ffmpeg_highpass = False #@param {type:\"boolean\"}\n",
    "#@markdown ↳ Remove low rumble (<80Hz)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Pass 2 Settings *(only applies when Two-Step is enabled in Step 1)*\n",
    "\n",
    "#@markdown **Scene Detection**\n",
    "pass2_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\n",
    "\n",
    "#@markdown **Speech Segmenter**\n",
    "pass2_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "\n",
    "#@markdown **Speech Enhancer**\n",
    "pass2_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n",
    "\n",
    "#@markdown **FFmpeg DSP Filters** *(only applies when ffmpeg-dsp selected above)*\n",
    "pass2_ffmpeg_amplify = True #@param {type:\"boolean\"}\n",
    "#@markdown ↳ Amplify quiet audio (recommended)\n",
    "pass2_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n",
    "#@markdown ↳ Loudness normalization\n",
    "pass2_ffmpeg_compress = False #@param {type:\"boolean\"}\n",
    "#@markdown ↳ Dynamic range compression\n",
    "pass2_ffmpeg_highpass = False #@param {type:\"boolean\"}\n",
    "#@markdown ↳ Remove low rumble (<80Hz)\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# BUILD EXPERT CONFIG\n",
    "# ═══════════════════════════════════════════\n",
    "\n",
    "def build_ffmpeg_filters(amplify, loudnorm, compress, highpass):\n",
    "    \"\"\"Combine selected FFmpeg filters into comma-separated string.\"\"\"\n",
    "    filters = []\n",
    "    if amplify:\n",
    "        filters.append(\"amplify\")\n",
    "    if loudnorm:\n",
    "        filters.append(\"loudnorm\")\n",
    "    if compress:\n",
    "        filters.append(\"compress\")\n",
    "    if highpass:\n",
    "        filters.append(\"highpass\")\n",
    "    return \",\".join(filters) if filters else None\n",
    "\n",
    "# Map \"automatic\" to None (let CLI use pipeline defaults)\n",
    "def map_value(val):\n",
    "    return None if val == \"automatic\" else val\n",
    "\n",
    "WHISPERJAV_EXPERT_CONFIG = {\n",
    "    # Pass 1\n",
    "    'pass1_scene_detector': map_value(pass1_scene_detector),\n",
    "    'pass1_speech_segmenter': map_value(pass1_speech_segmenter),\n",
    "    'pass1_speech_enhancer': None if pass1_speech_enhancer == \"none\" else pass1_speech_enhancer,\n",
    "    'pass1_ffmpeg_filters': build_ffmpeg_filters(pass1_ffmpeg_amplify, pass1_ffmpeg_loudnorm, pass1_ffmpeg_compress, pass1_ffmpeg_highpass) if pass1_speech_enhancer == \"ffmpeg-dsp\" else None,\n",
    "    # Pass 2\n",
    "    'pass2_scene_detector': map_value(pass2_scene_detector),\n",
    "    'pass2_speech_segmenter': map_value(pass2_speech_segmenter),\n",
    "    'pass2_speech_enhancer': None if pass2_speech_enhancer == \"none\" else pass2_speech_enhancer,\n",
    "    'pass2_ffmpeg_filters': build_ffmpeg_filters(pass2_ffmpeg_amplify, pass2_ffmpeg_loudnorm, pass2_ffmpeg_compress, pass2_ffmpeg_highpass) if pass2_speech_enhancer == \"ffmpeg-dsp\" else None,\n",
    "    # Display values\n",
    "    '_pass1_scene_detector': pass1_scene_detector,\n",
    "    '_pass1_speech_segmenter': pass1_speech_segmenter,\n",
    "    '_pass1_speech_enhancer': pass1_speech_enhancer,\n",
    "    '_pass2_scene_detector': pass2_scene_detector,\n",
    "    '_pass2_speech_segmenter': pass2_speech_segmenter,\n",
    "    '_pass2_speech_enhancer': pass2_speech_enhancer,\n",
    "}\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Validation warnings\n",
    "warnings = []\n",
    "\n",
    "# Check for memory-intensive combinations\n",
    "heavy_enhancers = {'clearvoice', 'bs-roformer', 'zipenhancer'}\n",
    "if pass1_speech_enhancer in heavy_enhancers and pass2_speech_enhancer in heavy_enhancers:\n",
    "    warnings.append(\"Using GPU-based enhancement on both passes may cause OOM on T4 GPU. Consider using 'none' or 'ffmpeg-dsp' for one pass.\")\n",
    "\n",
    "# Display warnings\n",
    "for w in warnings:\n",
    "    display(HTML(f'<div style=\"padding:4px 8px;background:#fef9c3;border-radius:4px;font-size:9px;margin:2px 0\"><b>⚠️</b> {w}</div>'))\n",
    "\n",
    "# Build summary\n",
    "def summarize_pass(scene, seg, enh, ffmpeg_filters):\n",
    "    parts = []\n",
    "    if scene != \"automatic\":\n",
    "        parts.append(f\"scene:{scene}\")\n",
    "    if seg != \"automatic\":\n",
    "        parts.append(f\"seg:{seg}\")\n",
    "    if enh != \"none\":\n",
    "        if enh == \"ffmpeg-dsp\" and ffmpeg_filters:\n",
    "            parts.append(f\"enh:{enh}({ffmpeg_filters})\")\n",
    "        else:\n",
    "            parts.append(f\"enh:{enh}\")\n",
    "    return \", \".join(parts) if parts else \"defaults\"\n",
    "\n",
    "p1_summary = summarize_pass(pass1_scene_detector, pass1_speech_segmenter, pass1_speech_enhancer,\n",
    "                            WHISPERJAV_EXPERT_CONFIG['pass1_ffmpeg_filters'])\n",
    "p2_summary = summarize_pass(pass2_scene_detector, pass2_speech_segmenter, pass2_speech_enhancer,\n",
    "                            WHISPERJAV_EXPERT_CONFIG['pass2_ffmpeg_filters'])\n",
    "\n",
    "display(HTML(f'<div style=\"padding:6px 10px;background:#f5f3ff;border-radius:4px;font-size:10px\"><b>Expert Settings:</b> Pass 1: {p1_summary} | Pass 2: {p2_summary}</div>'))\n",
    "print(\"\\n✓ Expert options configured. Run Step 2 to transcribe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transcribe"
   },
   "outputs": [],
   "source": [
    "#@title Step 2: Transcribe { display-mode: \"form\" }\n",
    "\n",
    "\n",
    "#@markdown Connect Drive → Install → Transcribe all media files → Add credits\n",
    "\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"✓\" if ok else \"✗\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n",
    "\n",
    "\n",
    "# Check config\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "\n",
    "\n",
    "# Check for expert config (optional - defaults if not run)\n",
    "expert = WHISPERJAV_EXPERT_CONFIG if 'WHISPERJAV_EXPERT_CONFIG' in dir() else None\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CONNECT GOOGLE DRIVE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"CONNECTING GOOGLE DRIVE\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    status(f\"Connected: {folder_path}\")\n",
    "except Exception as e:\n",
    "    status(f\"Failed to connect: {e}\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK GPU\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"CHECKING GPU\")\n",
    "gpu_check = subprocess.run(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\", shell=True, capture_output=True, text=True)\n",
    "if gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n",
    "    status(\"No GPU detected. Go to Runtime → Change runtime type → T4 GPU\", False)\n",
    "    raise SystemExit()\n",
    "status(f\"GPU: {gpu_check.stdout.strip()}\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# INSTALL WHISPERJAV\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"INSTALLING (2-3 min)\")\n",
    "install_start = time.time()\n",
    "\n",
    "\n",
    "# Base installation steps\n",
    "steps = [\n",
    "    (\"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev libc++1 libc++abi1 > /dev/null 2>&1\", \"System tools\"),\n",
    "    (\"pip install -q tqdm numba tiktoken ffmpeg-python soundfile auditok numpy scipy pysrt srt aiofiles jsonschema Pillow colorama librosa matplotlib pyloudnorm requests faster-whisper transformers optimum accelerate huggingface-hub pydantic ten-vad silero-vad pydub regex modelscope addict\", \"Python packages\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n",
    "    (\"pip install -q git+https://github.com/meizhong986/WhisperJAV.git@main\", \"WhisperJAV\")\n",
    "]\n",
    "\n",
    "\n",
    "for cmd, name in steps:\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        status(f\"{name} failed\", False)\n",
    "        raise SystemExit()\n",
    "    status(name)\n",
    "\n",
    "\n",
    "# Conditional installation of speech enhancer dependencies\n",
    "if expert:\n",
    "    extra_packages = set()\n",
    "    for enhancer in [expert.get('pass1_speech_enhancer'), expert.get('pass2_speech_enhancer')]:\n",
    "        if enhancer == 'clearvoice':\n",
    "            extra_packages.add('clearvoice')\n",
    "        elif enhancer == 'zipenhancer':\n",
    "            # zipenhancer uses modelscope which is already installed above\n",
    "            pass\n",
    "        elif enhancer == 'bs-roformer':\n",
    "            extra_packages.add('bs-roformer-infer')\n",
    "    \n",
    "    if extra_packages:\n",
    "        pkg_list = ' '.join(extra_packages)\n",
    "        result = subprocess.run(f\"pip install -q {pkg_list}\", shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            status(f\"Speech enhancer packages failed (continuing anyway)\", False)\n",
    "        else:\n",
    "            status(f\"Speech enhancer packages ({', '.join(extra_packages)})\")\n",
    "\n",
    "\n",
    "status(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# FIND MEDIA FILES\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"SCANNING FILES\")\n",
    "video_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\n",
    "videos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n",
    "\n",
    "\n",
    "if not videos:\n",
    "    status(f\"No media files in {cfg['folder_name']}/\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "status(f\"Found {len(videos)} file(s)\")\n",
    "for v in videos[:5]:\n",
    "    print(f\"  • {v.name}\")\n",
    "if len(videos) > 5:\n",
    "    print(f\"  ... and {len(videos)-5} more\")\n",
    "\n",
    "\n",
    "# Record existing SRT files before transcription\n",
    "existing_srts = set(folder_path.glob('*.srt'))\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSCRIBE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"TRANSCRIBING\")\n",
    "\n",
    "\n",
    "# Build command - always transcribe to native (Japanese) first\n",
    "# For \"direct-to-english\", use Whisper's built-in translation\n",
    "# For \"llm\", transcribe to Japanese and translate in Step 3\n",
    "cmd = ['whisperjav', str(folder_path), '--output-dir', str(folder_path)]\n",
    "\n",
    "\n",
    "if cfg['use_two_step']:\n",
    "    # Two-step ensemble mode\n",
    "    cmd.extend(['--ensemble',\n",
    "        '--pass1-pipeline', cfg['pass1_pipeline'],\n",
    "        '--pass1-sensitivity', cfg['pass1_sensitivity'],\n",
    "        '--pass2-pipeline', cfg['pass2_pipeline'],\n",
    "        '--pass2-sensitivity', cfg['pass2_sensitivity'],\n",
    "        '--merge-strategy', cfg['merge_strategy']])\n",
    "\n",
    "\n",
    "    # Add speech segmenters from basic config if not automatic\n",
    "    if cfg['pass1_speech_segmenter']:\n",
    "        cmd.extend(['--pass1-speech-segmenter', cfg['pass1_speech_segmenter']])\n",
    "    if cfg['pass2_speech_segmenter']:\n",
    "        cmd.extend(['--pass2-speech-segmenter', cfg['pass2_speech_segmenter']])\n",
    "\n",
    "\n",
    "    # Add models if not automatic\n",
    "    if cfg['pass1_model']:\n",
    "        cmd.extend(['--pass1-model', cfg['pass1_model']])\n",
    "    if cfg['pass2_model']:\n",
    "        cmd.extend(['--pass2-model', cfg['pass2_model']])\n",
    "\n",
    "\n",
    "    # Add expert options if provided\n",
    "    if expert:\n",
    "        # Pass 1 expert settings\n",
    "        if expert.get('pass1_scene_detector'):\n",
    "            cmd.extend(['--pass1-scene-detector', expert['pass1_scene_detector']])\n",
    "        if expert.get('pass1_speech_segmenter'):\n",
    "            # Expert speech segmenter overrides basic config\n",
    "            # Remove existing --pass1-speech-segmenter if present and add new one\n",
    "            if '--pass1-speech-segmenter' in cmd:\n",
    "                idx = cmd.index('--pass1-speech-segmenter')\n",
    "                cmd.pop(idx)  # Remove flag\n",
    "                cmd.pop(idx)  # Remove value\n",
    "            cmd.extend(['--pass1-speech-segmenter', expert['pass1_speech_segmenter']])\n",
    "        if expert.get('pass1_speech_enhancer'):\n",
    "            if expert['pass1_speech_enhancer'] == 'ffmpeg-dsp':\n",
    "                effects = expert.get('pass1_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'loudnorm'\n",
    "                cmd.extend(['--pass1-speech-enhancer', f'ffmpeg-dsp:{effects_str}'])\n",
    "            else:\n",
    "                cmd.extend(['--pass1-speech-enhancer', expert['pass1_speech_enhancer']])\n",
    "        \n",
    "        # Pass 2 expert settings\n",
    "        if expert.get('pass2_scene_detector'):\n",
    "            cmd.extend(['--pass2-scene-detector', expert['pass2_scene_detector']])\n",
    "        if expert.get('pass2_speech_segmenter'):\n",
    "            # Expert speech segmenter overrides basic config\n",
    "            if '--pass2-speech-segmenter' in cmd:\n",
    "                idx = cmd.index('--pass2-speech-segmenter')\n",
    "                cmd.pop(idx)\n",
    "                cmd.pop(idx)\n",
    "            cmd.extend(['--pass2-speech-segmenter', expert['pass2_speech_segmenter']])\n",
    "        if expert.get('pass2_speech_enhancer'):\n",
    "            if expert['pass2_speech_enhancer'] == 'ffmpeg-dsp':\n",
    "                effects = expert.get('pass2_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'loudnorm'\n",
    "                cmd.extend(['--pass2-speech-enhancer', f'ffmpeg-dsp:{effects_str}'])\n",
    "            else:\n",
    "                cmd.extend(['--pass2-speech-enhancer', expert['pass2_speech_enhancer']])\n",
    "\n",
    "\n",
    "    # Display mode info\n",
    "    p1_info = cfg['_quality']\n",
    "    if cfg['_speech_segmenter'] != 'automatic':\n",
    "        p1_info += f\" + {cfg['_speech_segmenter']}\"\n",
    "    if cfg['_model'] != 'automatic':\n",
    "        p1_info += f\" ({cfg['_model']})\"\n",
    "    if expert:\n",
    "        if expert.get('_pass1_scene_detector') != 'automatic':\n",
    "            p1_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n",
    "        if expert.get('_pass1_speech_enhancer') != 'none':\n",
    "            p1_info += f\" [enh:{expert['_pass1_speech_enhancer']}]\"\n",
    "\n",
    "\n",
    "    p2_info = cfg['_secondpass_quality']\n",
    "    if cfg['_secondpass_speech_segmenter'] != 'automatic':\n",
    "        p2_info += f\" + {cfg['_secondpass_speech_segmenter']}\"\n",
    "    if cfg['_secondpass_model'] != 'automatic':\n",
    "        p2_info += f\" ({cfg['_secondpass_model']})\"\n",
    "    if expert:\n",
    "        if expert.get('_pass2_scene_detector') != 'automatic':\n",
    "            p2_info += f\" [scene:{expert['_pass2_scene_detector']}]\"\n",
    "        if expert.get('_pass2_speech_enhancer') != 'none':\n",
    "            p2_info += f\" [enh:{expert['_pass2_speech_enhancer']}]\"\n",
    "\n",
    "\n",
    "    print(f\"Mode: Two-Step\")\n",
    "    print(f\"  Pass 1: {p1_info}\")\n",
    "    print(f\"  Pass 2: {p2_info}\")\n",
    "    print(f\"  Merge: {cfg['_merge_method']}\")\n",
    "else:\n",
    "    # Single-pass mode\n",
    "    cmd.extend(['--mode', cfg['pass1_pipeline'], '--sensitivity', cfg['pass1_sensitivity']])\n",
    "\n",
    "\n",
    "    # Add speech segmenter from basic config if not automatic\n",
    "    if cfg['pass1_speech_segmenter']:\n",
    "        cmd.extend(['--speech-segmenter', cfg['pass1_speech_segmenter']])\n",
    "\n",
    "\n",
    "    # Add model if specified\n",
    "    if cfg['pass1_model']:\n",
    "        cmd.extend(['--model', cfg['pass1_model']])\n",
    "\n",
    "\n",
    "    # Add expert options if provided (single-pass uses pass1 settings)\n",
    "    if expert:\n",
    "        if expert.get('pass1_scene_detector'):\n",
    "            cmd.extend(['--scene-detection-method', expert['pass1_scene_detector']])\n",
    "        if expert.get('pass1_speech_segmenter'):\n",
    "            # Expert speech segmenter overrides basic config\n",
    "            if '--speech-segmenter' in cmd:\n",
    "                idx = cmd.index('--speech-segmenter')\n",
    "                cmd.pop(idx)\n",
    "                cmd.pop(idx)\n",
    "            cmd.extend(['--speech-segmenter', expert['pass1_speech_segmenter']])\n",
    "        # Note: Single-pass speech enhancer support depends on CLI implementation\n",
    "\n",
    "\n",
    "    mode_info = f\"{cfg['_quality']}/{cfg['_speech_detection']}\"\n",
    "    if cfg['_speech_segmenter'] != 'automatic':\n",
    "        mode_info += f\" + {cfg['_speech_segmenter']}\"\n",
    "    if cfg['_model'] != 'automatic':\n",
    "        mode_info += f\" ({cfg['_model']})\"\n",
    "    if expert:\n",
    "        if expert.get('_pass1_scene_detector') != 'automatic':\n",
    "            mode_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n",
    "        if expert.get('_pass1_speech_enhancer') != 'none':\n",
    "            mode_info += f\" [enh:{expert['_pass1_speech_enhancer']}]\"\n",
    "    print(f\"Mode: Standard ({mode_info})\")\n",
    "\n",
    "\n",
    "# Set subtitle language for transcription\n",
    "if cfg['subtitle_language'] == 'direct-to-english':\n",
    "    cmd.extend(['--subs-language', 'direct-to-english'])\n",
    "    print(f\"Output: English (Whisper auto-translate)\")\n",
    "else:\n",
    "    # For both 'native' and 'llm', transcribe to Japanese first\n",
    "    cmd.extend(['--subs-language', 'native'])\n",
    "    if cfg['subtitle_language'] == 'llm':\n",
    "        print(f\"Output: Japanese (AI translation will follow in Step 3)\")\n",
    "    else:\n",
    "        print(f\"Output: Japanese\")\n",
    "\n",
    "\n",
    "print(f\"Input: {folder_path}\\n\")\n",
    "\n",
    "\n",
    "full_cmd = shlex.join(cmd)\n",
    "process = subprocess.Popen(full_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, universal_newlines=True)\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "process.wait()\n",
    "\n",
    "\n",
    "if process.returncode != 0:\n",
    "    status(\"Transcription failed\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# IDENTIFY NEW SRT FILES\n",
    "# ═══════════════════════════════════════════\n",
    "all_srts = set(folder_path.glob('*.srt'))\n",
    "new_srts = list(all_srts - existing_srts)\n",
    "new_srts.sort(key=lambda x: x.name)\n",
    "\n",
    "\n",
    "# Store for Step 3\n",
    "WHISPERJAV_NEW_SRTS = new_srts\n",
    "WHISPERJAV_FOLDER_PATH = folder_path\n",
    "\n",
    "\n",
    "status(f\"Created {len(new_srts)} new subtitle file(s)\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# ADD CREDITS\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"ADDING CREDITS\")\n",
    "\n",
    "\n",
    "if cfg['opening_credit'] or cfg['closing_credit']:\n",
    "    credits_count = 0\n",
    "    for srt_file in new_srts:\n",
    "        try:\n",
    "            content = srt_file.read_text(encoding='utf-8')\n",
    "            if cfg['opening_credit']:\n",
    "                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n",
    "            if cfg['closing_credit']:\n",
    "                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n",
    "            srt_file.write_text(content, encoding='utf-8')\n",
    "            credits_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n",
    "    status(f\"Credits added to {credits_count} file(s)\")\n",
    "else:\n",
    "    status(\"No credits configured\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSCRIPTION COMPLETE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"TRANSCRIPTION COMPLETE\")\n",
    "\n",
    "\n",
    "if cfg['subtitle_language'] == 'llm' and cfg['api_key']:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>✓ Transcription done!</b> {len(new_srts)} file(s). Run Step 3 next to start AI Translation.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ Done!</b> {len(new_srts)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "    if cfg['subtitle_language'] == 'llm' and not cfg['api_key']:\n",
    "        print(\"Note: AI translation skipped (no API key provided)\")\n",
    "\n",
    "\n",
    "    # Auto-disconnect if no AI translation needed\n",
    "    if cfg['auto_disconnect']:\n",
    "        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            from google.colab import runtime\n",
    "            runtime.unassign()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation { display-mode: \"form\" }\n",
    "\n",
    "\n",
    "#@markdown Translate the newly generated SRTs using `whisperjav-translate` (runs only if you selected **English (AI translate)** in Step 1).\n",
    "\n",
    "\n",
    "import os, shlex, subprocess, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"✓\" if ok else \"✗\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n",
    "\n",
    "\n",
    "# Preconditions\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir() or 'WHISPERJAV_FOLDER_PATH' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "folder_path = WHISPERJAV_FOLDER_PATH\n",
    "\n",
    "\n",
    "# Check if AI translation is needed\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>ℹ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "if not cfg['api_key']:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for AI translation</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "# Set up API key env var for provider\n",
    "env_map = {\n",
    "    \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "    \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "    \"gemini\": \"GEMINI_API_KEY\",\n",
    "    \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "    \"gpt\": \"OPENAI_API_KEY\",\n",
    "}\n",
    "os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n",
    "\n",
    "\n",
    "section(\"AI TRANSLATION\")\n",
    "print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['_translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "\n",
    "    translate_cmd = [\n",
    "        'whisperjav-translate',\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream',\n",
    "    ]\n",
    "\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Stream progress from stderr\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "section(\"COMPLETE\")\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>⚠ Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "\n",
    "\n",
    "# Auto-disconnect\n",
    "if cfg.get('auto_disconnect'):\n",
    "    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
