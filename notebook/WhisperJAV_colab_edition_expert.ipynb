{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_edition_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# WhisperJAV Colab Edition v1.7.4 (Expert)\n",
    "\n",
    "| User Mode | What it does | Speed |\n",
    "|------|--------------|-------|\n",
    "| **Standard** | Processes your video once | Faster |\n",
    "| **Two-Step** | Processes twice and combines for better accuracy | Slower |\n",
    "\n",
    "| Option | What it controls |\n",
    "|--------|------------------|\n",
    "| **Scene Detection** | How to split audio into chunks (auditok, silero, semantic) |\n",
    "| **Speech Segmenter** | How to detect speech in audio (silero, ten) |\n",
    "| **Speech Enhancer** | Audio cleanup for noisy sources (ffmpeg-dsp, clearvoice, etc.) |\n",
    "| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n",
    "\n",
    "---\n",
    "<div style=\"font-size: 8px; line-height: 1.0;\">\n",
    "1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n",
    "2. Run <b>Step 1: Expert Configuration</b><br>\n",
    "3. Run <b>Step 2: Transcribe</b> and wait for completion<br>\n",
    "4. Run <b>Step 3: AI Translation</b> (if selected)\n",
    "</div>\n",
    "\n",
    "<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure"
   },
   "outputs": [],
   "source": "#@title Step 1: Expert Configuration { display-mode: \"form\" }\n\n#@markdown ## ğŸ“ Files & Output\nfolder_name = \"WhisperJAV\" #@param {type:\"string\"}\nsubtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n\n#@markdown ---\n#@markdown ## 1ï¸âƒ£ Pass 1 Settings\npass1_pipeline = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\npass1_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\npass1_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n\n#@markdown **Expert Audio Setup**\npass1_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\npass1_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\npass1_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n#@markdown <font size=\"1\">auditok=energy (fast), silero=VAD, semantic=texture (complex audio) | enhancer: ffmpeg-dsp(no GPU), clearvoice(48k), bs-roformer(vocal)</font>\n\n#@markdown **FFmpeg Filters** *(only if enhancer is ffmpeg-dsp)*\npass1_ffmpeg_amplify = True #@param {type:\"boolean\"}\npass1_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\npass1_ffmpeg_compress = False #@param {type:\"boolean\"}\npass1_ffmpeg_highpass = False #@param {type:\"boolean\"}\n\n#@markdown ---\n#@markdown ## ğŸ”„ Two-Step Processing\nuse_two_step = False #@param {type:\"boolean\"}\nmerge_method = \"prefer first step\" #@param [\"automatic\", \"keep all\", \"prefer first step\", \"prefer second step\"]\n\n#@markdown ---\n#@markdown ## 2ï¸âƒ£ Pass 2 Settings *(if enabled)*\npass2_pipeline = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\npass2_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\npass2_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n\n#@markdown **Expert Audio Setup**\npass2_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\npass2_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\npass2_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n\n#@markdown **FFmpeg Filters** *(only if enhancer is ffmpeg-dsp)*\npass2_ffmpeg_amplify = True #@param {type:\"boolean\"}\npass2_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\npass2_ffmpeg_compress = False #@param {type:\"boolean\"}\npass2_ffmpeg_highpass = False #@param {type:\"boolean\"}\n\n#@markdown ---\n#@markdown ## ğŸ¤– AI Translation *(if selected)*\ntranslation_service = \"local\" #@param [\"local\", \"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\nlocal_model = \"gemma-9b\" #@param [\"gemma-9b\", \"llama-8b\", \"llama-3b\", \"auto\"]\n#@markdown <font size=\"1\">local: Free, runs on GPU. gemma-9b (8GB+ VRAM), llama-8b (6GB+), llama-3b (3GB+). Cloud providers require API key.</font>\napi_key = \"\" #@param {type:\"string\"}\ntranslation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n\n#@markdown ---\n#@markdown ## âš™ï¸ Session\nopening_credit = \"\" #@param {type:\"string\"}\nclosing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\nauto_disconnect = True #@param {type:\"boolean\"}\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CONFIGURATION LOGIC\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Venv path for WhisperJAV commands\nVENV_PATH = \"/content/whisperjav_env\"\nWHISPERJAV_CMD = f\"{VENV_PATH}/bin/whisperjav\"\nWHISPERJAV_TRANSLATE_CMD = f\"{VENV_PATH}/bin/whisperjav-translate\"\n\n# Mappings\ncombine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n               \"prefer first step\": \"pass1_primary\", \"prefer second step\": \"pass2_primary\"}\nlanguage_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n                \"English (AI translate)\": \"llm\"}\ntone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n\nmodel_map = {\n    \"automatic\": None,\n    \"large-v2\": \"large-v2\",\n    \"large-v3\": \"large-v3\",\n    \"turbo\": \"large-v3-turbo\",\n    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n}\n\n# Auto-correction\nKOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\nLEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\nwarnings_list = []\n\nif pass1_model in KOTOBA_MODELS and pass1_pipeline in LEGACY_PIPELINES:\n    warnings_list.append(f\"Pass 1: {pass1_model} requires 'transformers' pipeline. Auto-correcting.\")\n    pass1_pipeline = \"transformers\"\n\nif use_two_step and pass2_model in KOTOBA_MODELS and pass2_pipeline in LEGACY_PIPELINES:\n    warnings_list.append(f\"Pass 2: {pass2_model} requires 'transformers' pipeline. Auto-correcting.\")\n    pass2_pipeline = \"transformers\"\n\n# Memory warning\nheavy_enhancers = {'clearvoice', 'bs-roformer', 'zipenhancer'}\nif pass1_speech_enhancer in heavy_enhancers and pass2_speech_enhancer in heavy_enhancers:\n    warnings_list.append(\"Using GPU enhancers on both passes may cause OOM on T4. Suggest using ffmpeg-dsp for one.\")\n\n# Helpers\ndef build_ffmpeg_filters(amplify, loudnorm, compress, highpass):\n    filters = []\n    if amplify: filters.append(\"amplify\")\n    if loudnorm: filters.append(\"loudnorm\")\n    if compress: filters.append(\"compress\")\n    if highpass: filters.append(\"highpass\")\n    return \",\".join(filters) if filters else None\n\ndef map_value(val):\n    return None if val == \"automatic\" else val\n\ndef map_segmenter(val):\n    return \"none\" if val == \"none\" else map_value(val)\n\n# Build Configs\nWHISPERJAV_CONFIG = {\n    'use_two_step': use_two_step,\n    'pass1_pipeline': pass1_pipeline,\n    'pass1_sensitivity': pass1_sensitivity,\n    'pass1_speech_segmenter': map_segmenter(pass1_speech_segmenter),\n    'pass1_model': model_map[pass1_model],\n    'pass2_pipeline': pass2_pipeline,\n    'pass2_sensitivity': pass2_sensitivity,\n    'pass2_speech_segmenter': map_segmenter(pass2_speech_segmenter),\n    'pass2_model': model_map[pass2_model],\n    'merge_strategy': combine_map[merge_method],\n    'folder_name': folder_name,\n    'subtitle_language': language_map[subtitle_language],\n    'translation_service': translation_service,\n    'local_model': local_model,\n    'api_key': api_key,\n    'translation_style': tone_map[translation_style],\n    'opening_credit': opening_credit,\n    'closing_credit': closing_credit,\n    'auto_disconnect': auto_disconnect,\n    # Venv paths\n    'venv_path': VENV_PATH,\n    'whisperjav_cmd': WHISPERJAV_CMD,\n    'whisperjav_translate_cmd': WHISPERJAV_TRANSLATE_CMD,\n    # Helpers for Step 2 display\n    '_quality': pass1_pipeline,\n    '_speech_detection': pass1_sensitivity,\n    '_speech_segmenter': pass1_speech_segmenter,\n    '_model': pass1_model,\n    '_secondpass_quality': pass2_pipeline,\n    '_merge_method': merge_method,\n    '_secondpass_model': pass2_model,\n    '_translation_style': translation_style,\n}\n\nWHISPERJAV_EXPERT_CONFIG = {\n    'pass1_scene_detector': map_value(pass1_scene_detector),\n    'pass1_speech_segmenter': map_segmenter(pass1_speech_segmenter),\n    'pass1_speech_enhancer': None if pass1_speech_enhancer == \"none\" else pass1_speech_enhancer,\n    'pass1_ffmpeg_filters': build_ffmpeg_filters(pass1_ffmpeg_amplify, pass1_ffmpeg_loudnorm, pass1_ffmpeg_compress, pass1_ffmpeg_highpass) if pass1_speech_enhancer == \"ffmpeg-dsp\" else None,\n    'pass2_scene_detector': map_value(pass2_scene_detector),\n    'pass2_speech_segmenter': map_segmenter(pass2_speech_segmenter),\n    'pass2_speech_enhancer': None if pass2_speech_enhancer == \"none\" else pass2_speech_enhancer,\n    'pass2_ffmpeg_filters': build_ffmpeg_filters(pass2_ffmpeg_amplify, pass2_ffmpeg_loudnorm, pass2_ffmpeg_compress, pass2_ffmpeg_highpass) if pass2_speech_enhancer == \"ffmpeg-dsp\" else None,\n    '_pass1_scene_detector': pass1_scene_detector,\n    '_pass1_speech_enhancer': pass1_speech_enhancer,\n    '_pass2_scene_detector': pass2_scene_detector,\n    '_pass2_speech_enhancer': pass2_speech_enhancer,\n}\n\n# Display Status\nfrom IPython.display import display, HTML\nfor w in warnings_list:\n    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>âš ï¸ Warning:</b> {w}</div>'))\n\ndisplay(HTML(f'<div style=\"padding:10px;background:#f0f9ff;border-radius:4px;font-size:11px\">'\n             f'<b>Configuration Loaded</b><br>'\n             f'Mode: {\"Two-Step\" if use_two_step else \"Standard\"} | '\n             f'Model: {pass1_model} ({pass1_pipeline}) | '\n             f'Output: {subtitle_language}'\n             f'</div>'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transcribe"
   },
   "outputs": [],
   "source": "#@title Step 2: Transcribe { display-mode: \"form\" }\n#@markdown Connect Drive â†’ Install â†’ Transcribe all media files â†’ Add credits\n\nimport os, sys, subprocess, shlex, time\nfrom pathlib import Path\nfrom IPython.display import display, HTML\n\ndef status(msg, ok=True):\n    icon = \"âœ“\" if ok else \"âœ—\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'â”€'*40}\\n{title}\\n{'â”€'*40}\")\n\n# Check config\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\ncfg = WHISPERJAV_CONFIG\nexpert = WHISPERJAV_EXPERT_CONFIG if 'WHISPERJAV_EXPERT_CONFIG' in dir() else None\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CONNECT GOOGLE DRIVE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nsection(\"CONNECTING GOOGLE DRIVE\")\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n    folder_path.mkdir(parents=True, exist_ok=True)\n    status(f\"Connected: {folder_path}\")\nexcept Exception as e:\n    status(f\"Failed to connect: {e}\", False)\n    raise SystemExit(\"Google Drive connection failed\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CHECK GPU\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nsection(\"CHECKING GPU\")\ngpu_check = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"], capture_output=True, text=True)\nif gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n    status(\"No GPU detected. Go to Runtime â†’ Change runtime type â†’ T4 GPU\", False)\n    raise SystemExit(\"No GPU detected\")\nstatus(f\"GPU: {gpu_check.stdout.strip()}\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# INSTALL WHISPERJAV\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nsection(\"INSTALLING WHISPERJAV\")\ninstall_start = time.time()\n\nREPO_URL = \"https://github.com/meizhong986/WhisperJAV.git\"\nREPO_PATH = \"/content/WhisperJAV\"\nSCRIPT_PATH = f\"{REPO_PATH}/installer/install_colab.sh\"\n\n# Check if already installed\nvenv_python = f\"{cfg['venv_path']}/bin/python\"\nif os.path.exists(venv_python):\n    check = subprocess.run([venv_python, \"-c\", \"import whisperjav\"], capture_output=True)\n    if check.returncode == 0:\n        status(\"WhisperJAV already installed (skipping)\")\n    else:\n        status(\"Existing venv corrupt, reinstalling...\")\n        subprocess.run([\"rm\", \"-rf\", cfg['venv_path']], capture_output=True)\n        if not os.path.exists(REPO_PATH):\n            subprocess.run([\"git\", \"clone\", REPO_URL, REPO_PATH], capture_output=True)\n        result = subprocess.run([\"bash\", SCRIPT_PATH],\n            env={**os.environ, \"PATH\": f\"{os.environ.get('PATH', '')}:{os.path.expanduser('~/.local/bin')}\"})\n        if result.returncode != 0:\n            status(\"Installation failed\", False)\n            raise SystemExit(\"Installation failed\")\nelse:\n    # Fresh install: clone repo and run installer\n    print(\"Installing WhisperJAV (uv-accelerated)...\\n\")\n    if not os.path.exists(REPO_PATH):\n        print(f\"Cloning {REPO_URL}...\")\n        result = subprocess.run([\"git\", \"clone\", REPO_URL, REPO_PATH], capture_output=True, text=True)\n        if result.returncode != 0:\n            status(f\"Failed to clone: {result.stderr}\", False)\n            raise SystemExit(\"Clone failed\")\n    \n    if not os.path.exists(SCRIPT_PATH):\n        status(f\"Install script not found at {SCRIPT_PATH}\", False)\n        raise SystemExit(\"Install script missing from repo\")\n    \n    result = subprocess.run([\"bash\", SCRIPT_PATH],\n        env={**os.environ, \"PATH\": f\"{os.environ.get('PATH', '')}:{os.path.expanduser('~/.local/bin')}\"})\n    if result.returncode != 0:\n        status(\"Installation failed\", False)\n        raise SystemExit(\"Installation failed\")\n\n# Install speech enhancer packages if needed\nif expert:\n    extra_packages = set()\n    for enhancer in [expert.get('pass1_speech_enhancer'), expert.get('pass2_speech_enhancer')]:\n        if enhancer == 'clearvoice':\n            extra_packages.add('clearvoice')\n        elif enhancer == 'bs-roformer':\n            extra_packages.add('bs-roformer-infer')\n\n    if extra_packages:\n        pkg_list = ' '.join(extra_packages)\n        uv_path = os.path.expanduser(\"~/.local/bin/uv\")\n        if os.path.exists(uv_path):\n            result = subprocess.run(f\"{uv_path} pip install --python {venv_python} {pkg_list}\", shell=True, capture_output=True, text=True)\n        else:\n            result = subprocess.run(f\"{venv_python} -m pip install -q {pkg_list}\", shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            status(f\"Speech enhancer packages failed (continuing anyway)\", False)\n        else:\n            status(f\"Speech enhancer packages ({', '.join(extra_packages)})\")\n\nstatus(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# FIND MEDIA FILES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nsection(\"SCANNING FILES\")\nvideo_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\nvideos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n\nif not videos:\n    status(f\"No media files in {cfg['folder_name']}/\", False)\n    raise SystemExit(\"No media files found\")\n\nstatus(f\"Found {len(videos)} file(s)\")\nfor v in videos[:5]:\n    print(f\"  â€¢ {v.name}\")\nif len(videos) > 5:\n    print(f\"  ... and {len(videos)-5} more\")\n\nexisting_srts = set(folder_path.glob('*.srt'))\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TRANSCRIBE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nsection(\"TRANSCRIBING\")\n\ncmd = [cfg['whisperjav_cmd'], str(folder_path), '--output-dir', str(folder_path)]\n\nif cfg['use_two_step']:\n    cmd.extend(['--ensemble',\n        '--pass1-pipeline', cfg['pass1_pipeline'],\n        '--pass1-sensitivity', cfg['pass1_sensitivity'],\n        '--pass2-pipeline', cfg['pass2_pipeline'],\n        '--pass2-sensitivity', cfg['pass2_sensitivity'],\n        '--merge-strategy', cfg['merge_strategy']])\n\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--pass1-speech-segmenter', cfg['pass1_speech_segmenter']])\n    if cfg['pass2_speech_segmenter']:\n        cmd.extend(['--pass2-speech-segmenter', cfg['pass2_speech_segmenter']])\n    if cfg['pass1_model']:\n        cmd.extend(['--pass1-model', cfg['pass1_model']])\n    if cfg['pass2_model']:\n        cmd.extend(['--pass2-model', cfg['pass2_model']])\n\n    if expert:\n        if expert.get('pass1_scene_detector'):\n            cmd.extend(['--pass1-scene-detector', expert['pass1_scene_detector']])\n        if expert.get('pass1_speech_enhancer'):\n            if expert['pass1_speech_enhancer'] == 'ffmpeg-dsp':\n                effects = expert.get('pass1_ffmpeg_filters') or 'loudnorm'\n                cmd.extend(['--pass1-speech-enhancer', f'ffmpeg-dsp:{effects}'])\n            else:\n                cmd.extend(['--pass1-speech-enhancer', expert['pass1_speech_enhancer']])\n        if expert.get('pass2_scene_detector'):\n            cmd.extend(['--pass2-scene-detector', expert['pass2_scene_detector']])\n        if expert.get('pass2_speech_enhancer'):\n            if expert['pass2_speech_enhancer'] == 'ffmpeg-dsp':\n                effects = expert.get('pass2_ffmpeg_filters') or 'loudnorm'\n                cmd.extend(['--pass2-speech-enhancer', f'ffmpeg-dsp:{effects}'])\n            else:\n                cmd.extend(['--pass2-speech-enhancer', expert['pass2_speech_enhancer']])\n\n    p1_info = cfg['_quality']\n    if cfg['_speech_segmenter'] != 'automatic':\n        p1_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        p1_info += f\" ({cfg['_model']})\"\n    if expert:\n        if expert.get('_pass1_scene_detector') != 'automatic':\n            p1_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n        if expert.get('_pass1_speech_enhancer') != 'none':\n            p1_info += f\" [enh:{expert['_pass1_speech_enhancer']}]\"\n\n    p2_info = cfg['_secondpass_quality']\n    if cfg.get('_secondpass_model', 'automatic') != 'automatic':\n        p2_info += f\" ({cfg['_secondpass_model']})\"\n    if expert:\n        if expert.get('_pass2_scene_detector') != 'automatic':\n            p2_info += f\" [scene:{expert['_pass2_scene_detector']}]\"\n        if expert.get('_pass2_speech_enhancer') != 'none':\n            p2_info += f\" [enh:{expert['_pass2_speech_enhancer']}]\"\n\n    print(f\"Mode: Two-Step\")\n    print(f\"  Pass 1: {p1_info}\")\n    print(f\"  Pass 2: {p2_info}\")\n    print(f\"  Merge: {cfg['_merge_method']}\")\nelse:\n    cmd.extend(['--mode', cfg['pass1_pipeline'], '--sensitivity', cfg['pass1_sensitivity']])\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--speech-segmenter', cfg['pass1_speech_segmenter']])\n    if cfg['pass1_model']:\n        cmd.extend(['--model', cfg['pass1_model']])\n    if expert and expert.get('pass1_scene_detector'):\n        cmd.extend(['--scene-detection-method', expert['pass1_scene_detector']])\n\n    mode_info = f\"{cfg['_quality']}/{cfg['_speech_detection']}\"\n    if cfg['_speech_segmenter'] != 'automatic':\n        mode_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        mode_info += f\" ({cfg['_model']})\"\n    if expert and expert.get('_pass1_scene_detector') != 'automatic':\n        mode_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n    print(f\"Mode: Standard ({mode_info})\")\n\nif cfg['subtitle_language'] == 'direct-to-english':\n    cmd.extend(['--subs-language', 'direct-to-english'])\n    print(f\"Output: English (Whisper auto-translate)\")\nelse:\n    cmd.extend(['--subs-language', 'native'])\n    if cfg['subtitle_language'] == 'llm':\n        print(f\"Output: Japanese (AI translation will follow in Step 3)\")\n    else:\n        print(f\"Output: Japanese\")\n\nprint(f\"Input: {folder_path}\\n\")\n\nfull_cmd = shlex.join(cmd)\nprocess = subprocess.Popen(full_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, universal_newlines=True)\nfor line in process.stdout:\n    print(line, end='')\nprocess.wait()\n\nif process.returncode != 0:\n    status(\"Transcription failed\", False)\n    raise SystemExit(\"Transcription failed\")\n\nall_srts = set(folder_path.glob('*.srt'))\nnew_srts = list(all_srts - existing_srts)\nnew_srts.sort(key=lambda x: x.name)\n\nWHISPERJAV_NEW_SRTS = new_srts\nWHISPERJAV_FOLDER_PATH = folder_path\n\nstatus(f\"Created {len(new_srts)} new subtitle file(s)\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ADD CREDITS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nsection(\"ADDING CREDITS\")\n\nif cfg['opening_credit'] or cfg['closing_credit']:\n    credits_count = 0\n    for srt_file in new_srts:\n        try:\n            content = srt_file.read_text(encoding='utf-8')\n            if cfg['opening_credit']:\n                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n            if cfg['closing_credit']:\n                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n            srt_file.write_text(content, encoding='utf-8')\n            credits_count += 1\n        except Exception as e:\n            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n    status(f\"Credits added to {credits_count} file(s)\")\nelse:\n    status(\"No credits configured\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TRANSCRIPTION COMPLETE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nsection(\"TRANSCRIPTION COMPLETE\")\n\nif cfg['subtitle_language'] == 'llm' and (cfg['api_key'] or cfg['translation_service'] == 'local'):\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>âœ“ Transcription done!</b> {len(new_srts)} file(s). Run Step 3 next to start AI Translation.</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>âœ“ Done!</b> {len(new_srts)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n    if cfg['subtitle_language'] == 'llm' and not cfg['api_key'] and cfg['translation_service'] != 'local':\n        print(\"Note: AI translation skipped (no API key provided)\")\n\n    if cfg['auto_disconnect']:\n        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n        time.sleep(10)\n        try:\n            from google.colab import runtime\n            runtime.unassign()\n        except: pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 3: AI Translation { display-mode: \"form\" }\n#@markdown Translate the newly generated SRTs using `whisperjav-translate` (runs only if you selected **English (AI translate)** in Step 1).\n\nimport os, shlex, subprocess, time\nfrom pathlib import Path\nfrom IPython.display import display, HTML\n\ndef status(msg, ok=True):\n    icon = \"âœ“\" if ok else \"âœ—\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'â”€'*40}\\n{title}\\n{'â”€'*40}\")\n\n# Preconditions\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\nif 'WHISPERJAV_NEW_SRTS' not in dir() or 'WHISPERJAV_FOLDER_PATH' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n    raise SystemExit()\n\ncfg = WHISPERJAV_CONFIG\nnew_srts = WHISPERJAV_NEW_SRTS\nfolder_path = WHISPERJAV_FOLDER_PATH\n\n# Check if AI translation is needed\nif cfg['subtitle_language'] != 'llm':\n    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>â„¹ Skipped:</b> AI translation not selected</div>'))\n    raise SystemExit()\n\n# Check API key requirement (not needed for local provider)\nis_local = cfg['translation_service'] == 'local'\nif not is_local and not cfg['api_key']:\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for cloud translation. Use \"local\" provider for free GPU translation.</div>'))\n    raise SystemExit()\n\nif not new_srts:\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n    raise SystemExit()\n\n# Set up API key env var for provider (not needed for local)\nif not is_local:\n    env_map = {\n        \"deepseek\": \"DEEPSEEK_API_KEY\",\n        \"openrouter\": \"OPENROUTER_API_KEY\",\n        \"gemini\": \"GEMINI_API_KEY\",\n        \"claude\": \"ANTHROPIC_API_KEY\",\n        \"gpt\": \"OPENAI_API_KEY\",\n    }\n    os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n\nsection(\"AI TRANSLATION\")\nif is_local:\n    print(f\"Provider: local ({cfg.get('local_model', 'gemma-9b')})\")\n    print(\"Note: First run downloads model (~5GB)\")\nelse:\n    print(f\"Provider: {cfg['translation_service']}\")\nprint(f\"Style: {cfg['_translation_style']}\")\nprint(f\"Files to translate: {len(new_srts)}\\n\")\n\ntranslated_files = []\nfailed_files = []\n\nfor i, srt_file in enumerate(new_srts, 1):\n    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n\n    # Use venv path for translate command\n    translate_cmd = [\n        cfg['whisperjav_translate_cmd'],\n        '-i', str(srt_file),\n        '--provider', cfg['translation_service'],\n        '-t', 'english',\n        '--tone', cfg['translation_style'],\n        '--stream',\n    ]\n\n    if is_local:\n        translate_cmd.extend(['--model', cfg.get('local_model', 'gemma-9b')])\n\n    full_cmd = shlex.join(translate_cmd)\n\n    try:\n        process = subprocess.Popen(\n            full_cmd,\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            bufsize=1,\n            universal_newlines=True,\n        )\n\n        for line in process.stderr:\n            print(f\"    {line}\", end='')\n\n        stdout_output, _ = process.communicate()\n\n        if process.returncode == 0:\n            output_path = stdout_output.strip()\n            if output_path:\n                translated_files.append(Path(output_path))\n            status(f\"Completed: {srt_file.name}\")\n        else:\n            status(f\"Failed: {srt_file.name}\", False)\n            failed_files.append(srt_file)\n\n    except Exception as e:\n        status(f\"Error translating {srt_file.name}: {e}\", False)\n        failed_files.append(srt_file)\n\n    print()\n\nsection(\"COMPLETE\")\nif failed_files:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>âš  Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>âœ“ All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n\nif cfg.get('auto_disconnect'):\n    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n    time.sleep(10)\n    try:\n        from google.colab import runtime\n        runtime.unassign()\n    except: pass\nelse:\n    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}