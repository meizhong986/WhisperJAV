{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_edition_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# WhisperJAV Colab Edition v1.7.4 (Expert)\n",
    "\n",
    "| User Mode | What it does | Speed |\n",
    "|------|--------------|-------|\n",
    "| **Standard** | Processes your video once | Faster |\n",
    "| **Two-Step** | Processes twice and combines for better accuracy | Slower |\n",
    "\n",
    "| Option | What it controls |\n",
    "|--------|------------------|\n",
    "| **Scene Detection** | How to split audio into chunks (auditok, silero, semantic) |\n",
    "| **Speech Segmenter** | How to detect speech in audio (silero, ten) |\n",
    "| **Speech Enhancer** | Audio cleanup for noisy sources (ffmpeg-dsp, clearvoice, etc.) |\n",
    "| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n",
    "\n",
    "---\n",
    "<div style=\"font-size: 8px; line-height: 1.0;\">\n",
    "1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n",
    "2. Run <b>Step 1: Expert Configuration</b><br>\n",
    "3. Run <b>Step 2: Transcribe</b> and wait for completion<br>\n",
    "4. Run <b>Step 3: AI Translation</b> (if selected)\n",
    "</div>\n",
    "\n",
    "<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure"
   },
   "outputs": [],
   "source": [
    "#@title Step 1: Expert Configuration { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ## ğŸ“ Files & Output\n",
    "folder_name = \"WhisperJAV\" #@param {type:\"string\"}\n",
    "subtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## 1ï¸âƒ£ Pass 1 Settings\n",
    "pass1_pipeline = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "pass1_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "pass1_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown **Expert Audio Setup**\n",
    "pass1_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\n",
    "pass1_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "pass1_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n",
    "#@markdown <font size=\"1\">auditok=energy (fast), silero=VAD, semantic=texture (complex audio) | enhancer: ffmpeg-dsp(no GPU), clearvoice(48k), bs-roformer(vocal)</font>\n",
    "\n",
    "#@markdown **FFmpeg Filters** *(only if enhancer is ffmpeg-dsp)*\n",
    "pass1_ffmpeg_amplify = True #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_compress = False #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_highpass = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## ğŸ”„ Two-Step Processing\n",
    "use_two_step = False #@param {type:\"boolean\"}\n",
    "merge_method = \"prefer first step\" #@param [\"automatic\", \"keep all\", \"prefer first step\", \"prefer second step\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## 2ï¸âƒ£ Pass 2 Settings *(if enabled)*\n",
    "pass2_pipeline = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "pass2_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "pass2_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown **Expert Audio Setup**\n",
    "pass2_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\n",
    "pass2_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "pass2_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n",
    "\n",
    "#@markdown **FFmpeg Filters** *(only if enhancer is ffmpeg-dsp)*\n",
    "pass2_ffmpeg_amplify = True #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_compress = False #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_highpass = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## ğŸ¤– AI Translation *(if selected)*\n",
    "translation_service = \"deepseek\" #@param [\"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\n",
    "api_key = \"\" #@param {type:\"string\"}\n",
    "translation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## âš™ï¸ Session\n",
    "opening_credit = \"\" #@param {type:\"string\"}\n",
    "closing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n",
    "auto_disconnect = True #@param {type:\"boolean\"}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION LOGIC\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Mappings\n",
    "combine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n",
    "               \"prefer first step\": \"pass1_primary\", \"prefer second step\": \"pass2_primary\"}\n",
    "language_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n",
    "                \"English (AI translate)\": \"llm\"}\n",
    "tone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n",
    "\n",
    "model_map = {\n",
    "    \"automatic\": None,\n",
    "    \"large-v2\": \"large-v2\",\n",
    "    \"large-v3\": \"large-v3\",\n",
    "    \"turbo\": \"large-v3-turbo\",\n",
    "    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n",
    "    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n",
    "    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n",
    "    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n",
    "}\n",
    "\n",
    "# Auto-correction\n",
    "KOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\n",
    "LEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n",
    "warnings_list = []\n",
    "\n",
    "if pass1_model in KOTOBA_MODELS and pass1_pipeline in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 1: {pass1_model} requires 'transformers' pipeline. Auto-correcting.\")\n",
    "    pass1_pipeline = \"transformers\"\n",
    "\n",
    "if use_two_step and pass2_model in KOTOBA_MODELS and pass2_pipeline in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 2: {pass2_model} requires 'transformers' pipeline. Auto-correcting.\")\n",
    "    pass2_pipeline = \"transformers\"\n",
    "\n",
    "# Memory warning\n",
    "heavy_enhancers = {'clearvoice', 'bs-roformer', 'zipenhancer'}\n",
    "if pass1_speech_enhancer in heavy_enhancers and pass2_speech_enhancer in heavy_enhancers:\n",
    "    warnings_list.append(\"Using GPU enhancers on both passes may cause OOM on T4. Suggest using ffmpeg-dsp for one.\")\n",
    "\n",
    "# Helpers\n",
    "def build_ffmpeg_filters(amplify, loudnorm, compress, highpass):\n",
    "    filters = []\n",
    "    if amplify: filters.append(\"amplify\")\n",
    "    if loudnorm: filters.append(\"loudnorm\")\n",
    "    if compress: filters.append(\"compress\")\n",
    "    if highpass: filters.append(\"highpass\")\n",
    "    return \",\".join(filters) if filters else None\n",
    "\n",
    "def map_value(val):\n",
    "    return None if val == \"automatic\" else val\n",
    "\n",
    "def map_segmenter(val):\n",
    "    return \"none\" if val == \"none\" else map_value(val) # Explicit 'none' string is valid for segmenter\n",
    "\n",
    "# Build Configs (Both Standard and Expert for compatibility)\n",
    "WHISPERJAV_CONFIG = {\n",
    "    'use_two_step': use_two_step,\n",
    "    'pass1_pipeline': pass1_pipeline,\n",
    "    'pass1_sensitivity': pass1_sensitivity,\n",
    "    'pass1_speech_segmenter': map_segmenter(pass1_speech_segmenter),\n",
    "    'pass1_model': model_map[pass1_model],\n",
    "    'pass2_pipeline': pass2_pipeline,\n",
    "    'pass2_sensitivity': pass2_sensitivity,\n",
    "    'pass2_speech_segmenter': map_segmenter(pass2_speech_segmenter),\n",
    "    'pass2_model': model_map[pass2_model],\n",
    "    'merge_strategy': combine_map[merge_method],\n",
    "    'folder_name': folder_name,\n",
    "    'subtitle_language': language_map[subtitle_language],\n",
    "    'translation_service': translation_service,\n",
    "    'api_key': api_key,\n",
    "    'translation_style': tone_map[translation_style],\n",
    "    'opening_credit': opening_credit,\n",
    "    'closing_credit': closing_credit,\n",
    "    'auto_disconnect': auto_disconnect,\n",
    "    # Helpers for Step 2 display\n",
    "    '_quality': pass1_pipeline,\n",
    "    '_speech_detection': pass1_sensitivity,\n",
    "    '_speech_segmenter': pass1_speech_segmenter,\n",
    "    '_model': pass1_model,\n",
    "     # Pass 2 helpers\n",
    "    '_secondpass_quality': pass2_pipeline,\n",
    "    '_merge_method': merge_method,\n",
    "    '_secondpass_model': pass2_model,\n",
    "    '_translation_style': translation_style,\n",
    "}\n",
    "\n",
    "WHISPERJAV_EXPERT_CONFIG = {\n",
    "    'pass1_scene_detector': map_value(pass1_scene_detector),\n",
    "    'pass1_speech_segmenter': map_segmenter(pass1_speech_segmenter),\n",
    "    'pass1_speech_enhancer': None if pass1_speech_enhancer == \"none\" else pass1_speech_enhancer,\n",
    "    'pass1_ffmpeg_filters': build_ffmpeg_filters(pass1_ffmpeg_amplify, pass1_ffmpeg_loudnorm, pass1_ffmpeg_compress, pass1_ffmpeg_highpass) if pass1_speech_enhancer == \"ffmpeg-dsp\" else None,\n",
    "    'pass2_scene_detector': map_value(pass2_scene_detector),\n",
    "    'pass2_speech_segmenter': map_segmenter(pass2_speech_segmenter),\n",
    "    'pass2_speech_enhancer': None if pass2_speech_enhancer == \"none\" else pass2_speech_enhancer,\n",
    "    'pass2_ffmpeg_filters': build_ffmpeg_filters(pass2_ffmpeg_amplify, pass2_ffmpeg_loudnorm, pass2_ffmpeg_compress, pass2_ffmpeg_highpass) if pass2_speech_enhancer == \"ffmpeg-dsp\" else None,\n",
    "    # Display helpers\n",
    "    '_pass1_scene_detector': pass1_scene_detector,\n",
    "    '_pass1_speech_enhancer': pass1_speech_enhancer,\n",
    "    '_pass2_scene_detector': pass2_scene_detector,\n",
    "    '_pass2_speech_enhancer': pass2_speech_enhancer,\n",
    "}\n",
    "\n",
    "# Display Status\n",
    "from IPython.display import display, HTML\n",
    "for w in warnings_list:\n",
    "    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>âš ï¸ Warning:</b> {w}</div>'))\n",
    "\n",
    "display(HTML(f'<div style=\"padding:10px;background:#f0f9ff;border-radius:4px;font-size:11px\">'\n",
    "             f'<b>Configuration Loaded</b><br>'\n",
    "             f'Mode: {\"Two-Step\" if use_two_step else \"Standard\"} | '\n",
    "             f'Model: {pass1_model} ({pass1_pipeline}) | '\n",
    "             f'Output: {subtitle_language}'\n",
    "             f'</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transcribe"
   },
   "outputs": [],
   "source": [
    "#@title Step 2: Transcribe { display-mode: \"form\" }\n",
    "\n",
    "\n",
    "#@markdown Connect Drive â†’ Install â†’ Transcribe all media files â†’ Add credits\n",
    "\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"âœ“\" if ok else \"âœ—\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'â”€'*40}\\n{title}\\n{'â”€'*40}\")\n",
    "\n",
    "\n",
    "# Check config\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "\n",
    "\n",
    "# Check for expert config (always present now since Step 1 is unified)\n",
    "expert = WHISPERJAV_EXPERT_CONFIG if 'WHISPERJAV_EXPERT_CONFIG' in dir() else None\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONNECT GOOGLE DRIVE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"CONNECTING GOOGLE DRIVE\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    status(f\"Connected: {folder_path}\")\n",
    "except Exception as e:\n",
    "    status(f\"Failed to connect: {e}\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHECK GPU\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"CHECKING GPU\")\n",
    "gpu_check = subprocess.run(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\", shell=True, capture_output=True, text=True)\n",
    "if gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n",
    "    status(\"No GPU detected. Go to Runtime â†’ Change runtime type â†’ T4 GPU\", False)\n",
    "    raise SystemExit()\n",
    "status(f\"GPU: {gpu_check.stdout.strip()}\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INSTALL WHISPERJAV\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"INSTALLING (2-3 min)\")\n",
    "install_start = time.time()\n",
    "\n",
    "\n",
    "# Base installation steps\n",
    "steps = [\n",
    "    (\"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev libc++1 libc++abi1 > /dev/null 2>&1\", \"System tools\"),\n",
    "    (\"pip install -q tqdm numba>=0.58.0,<0.60.0 tiktoken soundfile auditok \\\"numpy<2.0\\\" scipy pysrt srt aiofiles jsonschema Pillow colorama librosa matplotlib pyloudnorm requests faster-whisper transformers optimum accelerate huggingface-hub pydantic ten-vad silero-vad pydub regex modelscope addict\", \"Python packages\"),\n",
    "    (\"pip install -q git+https://github.com/kkroening/ffmpeg-python.git\", \"FFmpeg-python\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n",
    "    (\"pip install -q git+https://github.com/meizhong986/WhisperJAV.git@main\", \"WhisperJAV\")\n",
    "]\n",
    "\n",
    "\n",
    "for cmd, name in steps:\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        status(f\"{name} failed\", False)\n",
    "        raise SystemExit()\n",
    "    status(name)\n",
    "\n",
    "\n",
    "# Conditional installation of speech enhancer dependencies\n",
    "if expert:\n",
    "    extra_packages = set()\n",
    "    for enhancer in [expert.get('pass1_speech_enhancer'), expert.get('pass2_speech_enhancer')]:\n",
    "        if enhancer == 'clearvoice':\n",
    "            extra_packages.add('clearvoice')\n",
    "        elif enhancer == 'zipenhancer':\n",
    "            # zipenhancer uses modelscope which is already installed above\n",
    "            pass\n",
    "        elif enhancer == 'bs-roformer':\n",
    "            extra_packages.add('bs-roformer-infer')\n",
    "    \n",
    "    if extra_packages:\n",
    "        pkg_list = ' '.join(extra_packages)\n",
    "        result = subprocess.run(f\"pip install -q {pkg_list}\", shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            status(f\"Speech enhancer packages failed (continuing anyway)\", False)\n",
    "        else:\n",
    "            status(f\"Speech enhancer packages ({', '.join(extra_packages)})\")\n",
    "\n",
    "\n",
    "status(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FIND MEDIA FILES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"SCANNING FILES\")\n",
    "video_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\n",
    "videos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n",
    "\n",
    "\n",
    "if not videos:\n",
    "    status(f\"No media files in {cfg['folder_name']}/\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "status(f\"Found {len(videos)} file(s)\")\n",
    "for v in videos[:5]:\n",
    "    print(f\"  â€¢ {v.name}\")\n",
    "if len(videos) > 5:\n",
    "    print(f\"  ... and {len(videos)-5} more\")\n",
    "\n",
    "\n",
    "# Record existing SRT files before transcription\n",
    "existing_srts = set(folder_path.glob('*.srt'))\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TRANSCRIBE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"TRANSCRIBING\")\n",
    "\n",
    "\n",
    "# Build command - always transcribe to native (Japanese) first\n",
    "# For \"direct-to-english\", use Whisper's built-in translation\n",
    "# For \"llm\", transcribe to Japanese and translate in Step 3\n",
    "cmd = ['whisperjav', str(folder_path), '--output-dir', str(folder_path)]\n",
    "\n",
    "\n",
    "if cfg['use_two_step']:\n",
    "    # Two-step ensemble mode\n",
    "    cmd.extend(['--ensemble',\n",
    "        '--pass1-pipeline', cfg['pass1_pipeline'],\n",
    "        '--pass1-sensitivity', cfg['pass1_sensitivity'],\n",
    "        '--pass2-pipeline', cfg['pass2_pipeline'],\n",
    "        '--pass2-sensitivity', cfg['pass2_sensitivity'],\n",
    "        '--merge-strategy', cfg['merge_strategy']])\n",
    "\n",
    "\n",
    "    # Add speech segmenters from basic config if not automatic\n",
    "    if cfg['pass1_speech_segmenter']:\n",
    "        cmd.extend(['--pass1-speech-segmenter', cfg['pass1_speech_segmenter']])\n",
    "    if cfg['pass2_speech_segmenter']:\n",
    "        cmd.extend(['--pass2-speech-segmenter', cfg['pass2_speech_segmenter']])\n",
    "\n",
    "\n",
    "    # Add models if not automatic\n",
    "    if cfg['pass1_model']:\n",
    "        cmd.extend(['--pass1-model', cfg['pass1_model']])\n",
    "    if cfg['pass2_model']:\n",
    "        cmd.extend(['--pass2-model', cfg['pass2_model']])\n",
    "\n",
    "\n",
    "    # Add expert options if provided\n",
    "    if expert:\n",
    "        # Pass 1 expert settings\n",
    "        if expert.get('pass1_scene_detector'):\n",
    "            cmd.extend(['--pass1-scene-detector', expert['pass1_scene_detector']])\n",
    "        # Speech segmenter already handled in basic config (now merged)\n",
    "        if expert.get('pass1_speech_enhancer'):\n",
    "            if expert['pass1_speech_enhancer'] == 'ffmpeg-dsp':\n",
    "                effects = expert.get('pass1_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'loudnorm'\n",
    "                cmd.extend(['--pass1-speech-enhancer', f'ffmpeg-dsp:{effects_str}'])\n",
    "            else:\n",
    "                cmd.extend(['--pass1-speech-enhancer', expert['pass1_speech_enhancer']])\n",
    "        \n",
    "        # Pass 2 expert settings\n",
    "        if expert.get('pass2_scene_detector'):\n",
    "            cmd.extend(['--pass2-scene-detector', expert['pass2_scene_detector']])\n",
    "        # Speech segmenter already handled in basic config (now merged)\n",
    "        if expert.get('pass2_speech_enhancer'):\n",
    "            if expert['pass2_speech_enhancer'] == 'ffmpeg-dsp':\n",
    "                effects = expert.get('pass2_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'loudnorm'\n",
    "                cmd.extend(['--pass2-speech-enhancer', f'ffmpeg-dsp:{effects_str}'])\n",
    "            else:\n",
    "                cmd.extend(['--pass2-speech-enhancer', expert['pass2_speech_enhancer']])\n",
    "\n",
    "\n",
    "    # Display mode info\n",
    "    p1_info = cfg['_quality']\n",
    "    if cfg['_speech_segmenter'] != 'automatic':\n",
    "        p1_info += f\" + {cfg['_speech_segmenter']}\"\n",
    "    if cfg['_model'] != 'automatic':\n",
    "        p1_info += f\" ({cfg['_model']})\"\n",
    "    if expert:\n",
    "        if expert.get('_pass1_scene_detector') != 'automatic':\n",
    "            p1_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n",
    "        if expert.get('_pass1_speech_enhancer') != 'none':\n",
    "            p1_info += f\" [enh:{expert['_pass1_speech_enhancer']}]\"\n",
    "\n",
    "\n",
    "    p2_info = cfg['_secondpass_quality']\n",
    "    # Fix: use correct key for merge display\n",
    "    if cfg.get('_secondpass_speech_segmenter', 'automatic') != 'automatic': \n",
    "        p2_info += f\" + {cfg['_secondpass_speech_segmenter']}\"\n",
    "    if cfg.get('_secondpass_model', 'automatic') != 'automatic':\n",
    "        p2_info += f\" ({cfg['_secondpass_model']})\"\n",
    "    if expert:\n",
    "        if expert.get('_pass2_scene_detector') != 'automatic':\n",
    "            p2_info += f\" [scene:{expert['_pass2_scene_detector']}]\"\n",
    "        if expert.get('_pass2_speech_enhancer') != 'none':\n",
    "            p2_info += f\" [enh:{expert['_pass2_speech_enhancer']}]\"\n",
    "\n",
    "\n",
    "    print(f\"Mode: Two-Step\")\n",
    "    print(f\"  Pass 1: {p1_info}\")\n",
    "    print(f\"  Pass 2: {p2_info}\")\n",
    "    print(f\"  Merge: {cfg['_merge_method']}\")\n",
    "else:\n",
    "    # Single-pass mode\n",
    "    cmd.extend(['--mode', cfg['pass1_pipeline'], '--sensitivity', cfg['pass1_sensitivity']])\n",
    "\n",
    "\n",
    "    # Add speech segmenter from basic config if not automatic\n",
    "    if cfg['pass1_speech_segmenter']:\n",
    "        cmd.extend(['--speech-segmenter', cfg['pass1_speech_segmenter']])\n",
    "\n",
    "\n",
    "    # Add model if specified\n",
    "    if cfg['pass1_model']:\n",
    "        cmd.extend(['--model', cfg['pass1_model']])\n",
    "\n",
    "\n",
    "    # Add expert options if provided (single-pass uses pass1 settings)\n",
    "    if expert:\n",
    "        if expert.get('pass1_scene_detector'):\n",
    "            cmd.extend(['--scene-detection-method', expert['pass1_scene_detector']])\n",
    "        # Speech segmenter already handled\n",
    "        # Note: Single-pass speech enhancer support depends on CLI implementation\n",
    "        if expert.get('pass1_speech_enhancer'):\n",
    "            if expert['pass1_speech_enhancer'] == 'ffmpeg-dsp':\n",
    "                effects = expert.get('pass1_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'loudnorm'\n",
    "                # Single pass speech enhancer flag might differ or not exist in CLI?\n",
    "                # Looking at CLI args, --pass1-speech-enhancer is for ensemble.\n",
    "                # Does main.py have --speech-enhancer for single pass?\n",
    "                # Assuming no (based on original file structure), so skipping here or handled by CLI if supported.\n",
    "                # Actually, main.py usually mirrors structure. Let's assume it's NOT supported in single pass for now as original NB didn't have it.\n",
    "                pass \n",
    "\n",
    "\n",
    "    mode_info = f\"{cfg['_quality']}/{cfg['_speech_detection']}\"\n",
    "    if cfg['_speech_segmenter'] != 'automatic':\n",
    "        mode_info += f\" + {cfg['_speech_segmenter']}\"\n",
    "    if cfg['_model'] != 'automatic':\n",
    "        mode_info += f\" ({cfg['_model']})\"\n",
    "    if expert:\n",
    "        if expert.get('_pass1_scene_detector') != 'automatic':\n",
    "            mode_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n",
    "        \n",
    "    print(f\"Mode: Standard ({mode_info})\")\n",
    "\n",
    "\n",
    "# Set subtitle language for transcription\n",
    "if cfg['subtitle_language'] == 'direct-to-english':\n",
    "    cmd.extend(['--subs-language', 'direct-to-english'])\n",
    "    print(f\"Output: English (Whisper auto-translate)\")\n",
    "else:\n",
    "    # For both 'native' and 'llm', transcribe to Japanese first\n",
    "    cmd.extend(['--subs-language', 'native'])\n",
    "    if cfg['subtitle_language'] == 'llm':\n",
    "        print(f\"Output: Japanese (AI translation will follow in Step 3)\")\n",
    "    else:\n",
    "        print(f\"Output: Japanese\")\n",
    "\n",
    "\n",
    "print(f\"Input: {folder_path}\\n\")\n",
    "\n",
    "\n",
    "full_cmd = shlex.join(cmd)\n",
    "process = subprocess.Popen(full_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, universal_newlines=True)\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "process.wait()\n",
    "\n",
    "\n",
    "if process.returncode != 0:\n",
    "    status(\"Transcription failed\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# IDENTIFY NEW SRT FILES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "all_srts = set(folder_path.glob('*.srt'))\n",
    "new_srts = list(all_srts - existing_srts)\n",
    "new_srts.sort(key=lambda x: x.name)\n",
    "\n",
    "\n",
    "# Store for Step 3\n",
    "WHISPERJAV_NEW_SRTS = new_srts\n",
    "WHISPERJAV_FOLDER_PATH = folder_path\n",
    "\n",
    "\n",
    "status(f\"Created {len(new_srts)} new subtitle file(s)\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ADD CREDITS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"ADDING CREDITS\")\n",
    "\n",
    "\n",
    "if cfg['opening_credit'] or cfg['closing_credit']:\n",
    "    credits_count = 0\n",
    "    for srt_file in new_srts:\n",
    "        try:\n",
    "            content = srt_file.read_text(encoding='utf-8')\n",
    "            if cfg['opening_credit']:\n",
    "                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n",
    "            if cfg['closing_credit']:\n",
    "                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n",
    "            srt_file.write_text(content, encoding='utf-8')\n",
    "            credits_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n",
    "    status(f\"Credits added to {credits_count} file(s)\")\n",
    "else:\n",
    "    status(\"No credits configured\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TRANSCRIPTION COMPLETE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"TRANSCRIPTION COMPLETE\")\n",
    "\n",
    "\n",
    "if cfg['subtitle_language'] == 'llm' and cfg['api_key']:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>âœ“ Transcription done!</b> {len(new_srts)} file(s). Run Step 3 next to start AI Translation.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>âœ“ Done!</b> {len(new_srts)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "    if cfg['subtitle_language'] == 'llm' and not cfg['api_key']:\n",
    "        print(\"Note: AI translation skipped (no API key provided)\")\n",
    "\n",
    "\n",
    "    # Auto-disconnect if no AI translation needed\n",
    "    if cfg['auto_disconnect']:\n",
    "        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            from google.colab import runtime\n",
    "            runtime.unassign()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation { display-mode: \"form\" }\n",
    "\n",
    "\n",
    "#@markdown Translate the newly generated SRTs using `whisperjav-translate` (runs only if you selected **English (AI translate)** in Step 1).\n",
    "\n",
    "\n",
    "import os, shlex, subprocess, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"âœ“\" if ok else \"âœ—\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'â”€'*40}\\n{title}\\n{'â”€'*40}\")\n",
    "\n",
    "\n",
    "# Preconditions\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir() or 'WHISPERJAV_FOLDER_PATH' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "folder_path = WHISPERJAV_FOLDER_PATH\n",
    "\n",
    "\n",
    "# Check if AI translation is needed\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>â„¹ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "if not cfg['api_key']:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for AI translation</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "# Set up API key env var for provider\n",
    "env_map = {\n",
    "    \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "    \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "    \"gemini\": \"GEMINI_API_KEY\",\n",
    "    \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "    \"gpt\": \"OPENAI_API_KEY\",\n",
    "}\n",
    "os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n",
    "\n",
    "\n",
    "section(\"AI TRANSLATION\")\n",
    "print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['_translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "\n",
    "    translate_cmd = [\n",
    "        'whisperjav-translate',\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream',\n",
    "    ]\n",
    "\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Stream progress from stderr\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "section(\"COMPLETE\")\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>âš  Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>âœ“ All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "\n",
    "\n",
    "# Auto-disconnect\n",
    "if cfg.get('auto_disconnect'):\n",
    "    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
