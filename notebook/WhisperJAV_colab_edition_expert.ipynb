{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_edition_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# WhisperJAV Colab Edition v1.7.4 (Expert)\n\n| User Mode | What it does | Speed |\n|------|--------------|-------|\n| **Standard** | Processes your video once | Faster |\n| **Two-Step** | Processes twice and combines for better accuracy | Slower |\n\n| Option | What it controls |\n|--------|------------------|\n| **Scene Detection** | How to split audio into chunks (auditok, silero, semantic) |\n| **Speech Segmenter** | How to detect speech in audio (silero, ten) |\n| **Speech Enhancer** | Audio cleanup for noisy sources (ffmpeg-dsp, clearvoice, etc.) |\n| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n\n---\n<div style=\"font-size: 8px; line-height: 1.0;\">\n1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n2. Run <b>Step 1: Settings</b> (required)<br>\n3. Run <b>Step 1.5: Expert Options</b> (optional - skip if unsure)<br>\n4. Run <b>Step 2: Transcribe</b> and wait for completion<br>\n5. Run <b>Step 3: AI Translation</b> (if selected)\n</div>\n\n<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure"
   },
   "outputs": [],
   "source": "#@title Step 1: Settings { display-mode: \"form\" }\n\n#@markdown **Transcription**\nquality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\nspeech_detection = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\nspeech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\nmodel = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n\n#@markdown ---\n#@markdown **Two-Step Processing** *(slower but more accurate)*\nuse_two_step = False #@param {type:\"boolean\"}\nsecondpass_quality = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\nsecondpass_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\nsecondpass_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\nsecondpass_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\nmerge_method = \"prefer first step\" #@param [\"automatic\", \"keep all\", \"prefer first step\", \"prefer second step\"]\n\n#@markdown ---\n#@markdown **Files & Output**\nfolder_name = \"WhisperJAV\" #@param {type:\"string\"}\nsubtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n\n#@markdown ---\n#@markdown **AI Translation** *(if selected \"English (AI translate)\")*\ntranslation_service = \"deepseek\" #@param [\"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\napi_key = \"\" #@param {type:\"string\"}\ntranslation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n\n#@markdown ---\n#@markdown **Credits**\nopening_credit = \"\" #@param {type:\"string\"}\nclosing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown **Session**\nauto_disconnect = True #@param {type:\"boolean\"}\n#@markdown ☝️ Auto-disconnect when done (saves GPU credits)\n\n# Mapping dictionaries\ncombine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n               \"prefer first step\": \"pass1_primary\", \"prefer second step\": \"pass2_primary\"}\nlanguage_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n                \"English (AI translate)\": \"llm\"}\ntone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n\n# Speech segmenter mapping (None = use pipeline default)\nsegmenter_map = {\"automatic\": None, \"silero\": \"silero\", \"ten\": \"ten\", \"none\": \"none\"}\n\n# Model mapping (None = use pipeline default)\nmodel_map = {\n    \"automatic\": None,\n    \"large-v2\": \"large-v2\",\n    \"large-v3\": \"large-v3\",\n    \"turbo\": \"large-v3-turbo\",\n    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n}\n\n# Define model compatibility:\n# - Kotoba models (HuggingFace) ONLY work with \"transformers\" pipeline\n# - Legacy models (large-v2/v3/turbo) work with ALL pipelines (faster, fast, balanced, fidelity, transformers)\nKOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\nLEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n\n# Auto-correct incompatible model-pipeline combinations\nwarnings_list = []\n\n# Check Pass 1 compatibility\nif model in KOTOBA_MODELS and quality in LEGACY_PIPELINES:\n    warnings_list.append(f\"Pass 1: {model} requires 'transformers' pipeline. Auto-correcting from '{quality}' to 'transformers'.\")\n    quality = \"transformers\"\n\n# Check Pass 2 compatibility (only relevant if two-step is enabled)\nif use_two_step and secondpass_model in KOTOBA_MODELS and secondpass_quality in LEGACY_PIPELINES:\n    warnings_list.append(f\"Pass 2: {secondpass_model} requires 'transformers' pipeline. Auto-correcting from '{secondpass_quality}' to 'transformers'.\")\n    secondpass_quality = \"transformers\"\n\nWHISPERJAV_CONFIG = {\n    'use_two_step': use_two_step,\n    'pass1_pipeline': quality,\n    'pass1_sensitivity': speech_detection,\n    'pass1_speech_segmenter': segmenter_map[speech_segmenter],\n    'pass1_model': model_map[model],\n    'pass2_pipeline': secondpass_quality,\n    'pass2_sensitivity': secondpass_sensitivity,\n    'pass2_speech_segmenter': segmenter_map[secondpass_speech_segmenter],\n    'pass2_model': model_map[secondpass_model],\n    'merge_strategy': combine_map[merge_method],\n    'folder_name': folder_name,\n    'subtitle_language': language_map[subtitle_language],\n    'translation_service': translation_service,\n    'api_key': api_key,\n    'translation_style': tone_map[translation_style],\n    'opening_credit': opening_credit,\n    'closing_credit': closing_credit,\n    'auto_disconnect': auto_disconnect,\n    # Display values (for status messages)\n    '_quality': quality,\n    '_speech_detection': speech_detection,\n    '_speech_segmenter': speech_segmenter,\n    '_model': model,\n    '_secondpass_quality': secondpass_quality,\n    '_secondpass_sensitivity': secondpass_sensitivity,\n    '_secondpass_speech_segmenter': secondpass_speech_segmenter,\n    '_secondpass_model': secondpass_model,\n    '_merge_method': merge_method,\n    '_subtitle_language': subtitle_language,\n    '_translation_style': translation_style,\n}\n\nfrom IPython.display import display, HTML\n\n# Display any auto-correction warnings\nfor warning in warnings_list:\n    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>⚠ Auto-corrected:</b> {warning}</div>'))\n\n# Build status display\nif use_two_step:\n    mode_text = \"Two-Step\"\n    p1_info = f\"{quality}\"\n    if speech_segmenter != \"automatic\":\n        p1_info += f\"/{speech_segmenter}\"\n    if model != \"automatic\":\n        p1_info += f\"/{model}\"\n    p2_info = f\"{secondpass_quality}\"\n    if secondpass_speech_segmenter != \"automatic\":\n        p2_info += f\"/{secondpass_speech_segmenter}\"\n    if secondpass_model != \"automatic\":\n        p2_info += f\"/{secondpass_model}\"\n    details = f\"{p1_info} → {p2_info}\"\nelse:\n    mode_text = \"Standard\"\n    details = f\"{quality}/{speech_detection}\"\n    if speech_segmenter != \"automatic\":\n        details += f\"/{speech_segmenter}\"\n    if model != \"automatic\":\n        details += f\"/{model}\"\n\ndisplay(HTML(f'<div style=\"padding:6px 10px;background:#f0f9ff;border-radius:4px;font-size:10px\"><b>Settings:</b> {mode_text} ({details}) | Folder: {folder_name} | Output: {subtitle_language}</div>'))"
  },
  {
   "cell_type": "code",
   "source": "#@title Step 1.5: Expert Options (Optional) { display-mode: \"form\" }\n#@markdown <font color=\"gray\">*Skip this cell if unsure. Default settings work well for most videos.*</font>\n#@markdown\n#@markdown <font color=\"orange\">⚠️ **Memory Notice:** Speech enhancement uses additional GPU memory. If you encounter OOM errors, use `none` or `ffmpeg-dsp`.</font>\n\n#@markdown ---\n#@markdown ## Pass 1 Settings\n\n#@markdown **Scene Detection** *(how to split audio into chunks)*\npass1_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\", \"none\"]\n#@markdown <font size=\"1\">auditok=energy-based (fast), silero=VAD-based, semantic=texture clustering (best for complex audio)</font>\n\n#@markdown **Speech Segmenter** *(how to detect speech within chunks)*\npass1_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n\n#@markdown **Speech Enhancer** *(audio cleanup for noisy sources)*\npass1_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n#@markdown <font size=\"1\">none=skip, ffmpeg-dsp=filters (no GPU), clearvoice=denoise (48kHz), zipenhancer=lightweight, bs-roformer=vocal isolation</font>\n\n#@markdown **FFmpeg DSP Filters** *(only applies when ffmpeg-dsp selected above)*\npass1_ffmpeg_amplify = True #@param {type:\"boolean\"}\n#@markdown ↳ Amplify quiet audio (recommended)\npass1_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n#@markdown ↳ Loudness normalization\npass1_ffmpeg_compress = False #@param {type:\"boolean\"}\n#@markdown ↳ Dynamic range compression\npass1_ffmpeg_highpass = False #@param {type:\"boolean\"}\n#@markdown ↳ Remove low rumble (<80Hz)\n\n#@markdown ---\n#@markdown ## Pass 2 Settings *(only applies when Two-Step is enabled in Step 1)*\n\n#@markdown **Scene Detection**\npass2_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\", \"none\"]\n\n#@markdown **Speech Segmenter**\npass2_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n\n#@markdown **Speech Enhancer**\npass2_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n\n#@markdown **FFmpeg DSP Filters** *(only applies when ffmpeg-dsp selected above)*\npass2_ffmpeg_amplify = True #@param {type:\"boolean\"}\n#@markdown ↳ Amplify quiet audio (recommended)\npass2_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n#@markdown ↳ Loudness normalization\npass2_ffmpeg_compress = False #@param {type:\"boolean\"}\n#@markdown ↳ Dynamic range compression\npass2_ffmpeg_highpass = False #@param {type:\"boolean\"}\n#@markdown ↳ Remove low rumble (<80Hz)\n\n# ═══════════════════════════════════════════\n# BUILD EXPERT CONFIG\n# ═══════════════════════════════════════════\n\ndef build_ffmpeg_filters(amplify, loudnorm, compress, highpass):\n    \"\"\"Combine selected FFmpeg filters into comma-separated string.\"\"\"\n    filters = []\n    if amplify:\n        filters.append(\"amplify\")\n    if loudnorm:\n        filters.append(\"loudnorm\")\n    if compress:\n        filters.append(\"compress\")\n    if highpass:\n        filters.append(\"highpass\")\n    return \",\".join(filters) if filters else None\n\n# Map \"automatic\" to None (let CLI use pipeline defaults)\ndef map_value(val):\n    return None if val == \"automatic\" else val\n\nWHISPERJAV_EXPERT_CONFIG = {\n    # Pass 1\n    'pass1_scene_detector': map_value(pass1_scene_detector),\n    'pass1_speech_segmenter': map_value(pass1_speech_segmenter),\n    'pass1_speech_enhancer': None if pass1_speech_enhancer == \"none\" else pass1_speech_enhancer,\n    'pass1_ffmpeg_filters': build_ffmpeg_filters(pass1_ffmpeg_amplify, pass1_ffmpeg_loudnorm, pass1_ffmpeg_compress, pass1_ffmpeg_highpass) if pass1_speech_enhancer == \"ffmpeg-dsp\" else None,\n    # Pass 2\n    'pass2_scene_detector': map_value(pass2_scene_detector),\n    'pass2_speech_segmenter': map_value(pass2_speech_segmenter),\n    'pass2_speech_enhancer': None if pass2_speech_enhancer == \"none\" else pass2_speech_enhancer,\n    'pass2_ffmpeg_filters': build_ffmpeg_filters(pass2_ffmpeg_amplify, pass2_ffmpeg_loudnorm, pass2_ffmpeg_compress, pass2_ffmpeg_highpass) if pass2_speech_enhancer == \"ffmpeg-dsp\" else None,\n    # Display values\n    '_pass1_scene_detector': pass1_scene_detector,\n    '_pass1_speech_segmenter': pass1_speech_segmenter,\n    '_pass1_speech_enhancer': pass1_speech_enhancer,\n    '_pass2_scene_detector': pass2_scene_detector,\n    '_pass2_speech_segmenter': pass2_speech_segmenter,\n    '_pass2_speech_enhancer': pass2_speech_enhancer,\n}\n\nfrom IPython.display import display, HTML\n\n# Validation warnings\nwarnings = []\n\n# Check for memory-intensive combinations\nheavy_enhancers = {'clearvoice', 'bs-roformer', 'zipenhancer'}\nif pass1_speech_enhancer in heavy_enhancers and pass2_speech_enhancer in heavy_enhancers:\n    warnings.append(\"Using GPU-based enhancement on both passes may cause OOM on T4 GPU. Consider using 'none' or 'ffmpeg-dsp' for one pass.\")\n\n# Display warnings\nfor w in warnings:\n    display(HTML(f'<div style=\"padding:4px 8px;background:#fef9c3;border-radius:4px;font-size:9px;margin:2px 0\"><b>⚠️</b> {w}</div>'))\n\n# Build summary\ndef summarize_pass(scene, seg, enh, ffmpeg_filters):\n    parts = []\n    if scene != \"automatic\":\n        parts.append(f\"scene:{scene}\")\n    if seg != \"automatic\":\n        parts.append(f\"seg:{seg}\")\n    if enh != \"none\":\n        if enh == \"ffmpeg-dsp\" and ffmpeg_filters:\n            parts.append(f\"enh:{enh}({ffmpeg_filters})\")\n        else:\n            parts.append(f\"enh:{enh}\")\n    return \", \".join(parts) if parts else \"defaults\"\n\np1_summary = summarize_pass(pass1_scene_detector, pass1_speech_segmenter, pass1_speech_enhancer,\n                            WHISPERJAV_EXPERT_CONFIG['pass1_ffmpeg_filters'])\np2_summary = summarize_pass(pass2_scene_detector, pass2_speech_segmenter, pass2_speech_enhancer,\n                            WHISPERJAV_EXPERT_CONFIG['pass2_ffmpeg_filters'])\n\ndisplay(HTML(f'<div style=\"padding:6px 10px;background:#f5f3ff;border-radius:4px;font-size:10px\"><b>Expert Settings:</b> Pass 1: {p1_summary} | Pass 2: {p2_summary}</div>'))\nprint(\"\\n✓ Expert options configured. Run Step 2 to transcribe.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transcribe"
   },
   "outputs": [],
   "source": "#@title Step 2: Transcribe { display-mode: \"form\" }\n#@markdown Connect Drive → Install → Transcribe all media files → Add credits\n\nimport os, sys, subprocess, shlex, time\nfrom pathlib import Path\nfrom IPython.display import display, HTML, clear_output\n\ndef status(msg, ok=True):\n    icon = \"✓\" if ok else \"✗\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n\n# Check config\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\ncfg = WHISPERJAV_CONFIG\n\n# Check for expert config (optional - defaults if not run)\nexpert = WHISPERJAV_EXPERT_CONFIG if 'WHISPERJAV_EXPERT_CONFIG' in dir() else None\n\n# ═══════════════════════════════════════════\n# CONNECT GOOGLE DRIVE\n# ═══════════════════════════════════════════\nsection(\"CONNECTING GOOGLE DRIVE\")\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n    folder_path.mkdir(parents=True, exist_ok=True)\n    status(f\"Connected: {folder_path}\")\nexcept Exception as e:\n    status(f\"Failed to connect: {e}\", False)\n    raise SystemExit()\n\n# ═══════════════════════════════════════════\n# CHECK GPU\n# ═══════════════════════════════════════════\nsection(\"CHECKING GPU\")\ngpu_check = subprocess.run(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\", shell=True, capture_output=True, text=True)\nif gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n    status(\"No GPU detected. Go to Runtime → Change runtime type → T4 GPU\", False)\n    raise SystemExit()\nstatus(f\"GPU: {gpu_check.stdout.strip()}\")\n\n# ═══════════════════════════════════════════\n# INSTALL WHISPERJAV\n# ═══════════════════════════════════════════\nsection(\"INSTALLING (2-3 min)\")\ninstall_start = time.time()\n\n# Base installation steps\nsteps = [\n    (\"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev libc++1 libc++abi1 > /dev/null 2>&1\", \"System tools\"),\n    (\"pip install -q tqdm numba tiktoken ffmpeg-python soundfile auditok numpy scipy pysrt srt aiofiles jsonschema Pillow colorama librosa matplotlib pyloudnorm requests faster-whisper transformers optimum accelerate huggingface-hub pydantic ten-vad silero-vad pydub regex modelscope addict\", \"Python packages\"),\n    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n    (\"pip install -q git+https://github.com/meizhong986/WhisperJAV.git@main\", \"WhisperJAV\")\n]\n\nfor cmd, name in steps:\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        status(f\"{name} failed\", False)\n        raise SystemExit()\n    status(name)\n\n# Conditional installation of speech enhancer dependencies\nif expert:\n    extra_packages = set()\n    for enhancer in [expert.get('pass1_speech_enhancer'), expert.get('pass2_speech_enhancer')]:\n        if enhancer == 'clearvoice':\n            extra_packages.add('clearvoice')\n        elif enhancer == 'zipenhancer':\n            # zipenhancer uses modelscope which is already installed above\n            pass\n        elif enhancer == 'bs-roformer':\n            extra_packages.add('bs-roformer-infer')\n    \n    if extra_packages:\n        pkg_list = ' '.join(extra_packages)\n        result = subprocess.run(f\"pip install -q {pkg_list}\", shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            status(f\"Speech enhancer packages failed (continuing anyway)\", False)\n        else:\n            status(f\"Speech enhancer packages ({', '.join(extra_packages)})\")\n\nstatus(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n\n# ═══════════════════════════════════════════\n# FIND MEDIA FILES\n# ═══════════════════════════════════════════\nsection(\"SCANNING FILES\")\nvideo_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\nvideos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n\nif not videos:\n    status(f\"No media files in {cfg['folder_name']}/\", False)\n    raise SystemExit()\n\nstatus(f\"Found {len(videos)} file(s)\")\nfor v in videos[:5]:\n    print(f\"  • {v.name}\")\nif len(videos) > 5:\n    print(f\"  ... and {len(videos)-5} more\")\n\n# Record existing SRT files before transcription\nexisting_srts = set(folder_path.glob('*.srt'))\n\n# ═══════════════════════════════════════════\n# TRANSCRIBE\n# ═══════════════════════════════════════════\nsection(\"TRANSCRIBING\")\n\n# Build command - always transcribe to native (Japanese) first\n# For \"direct-to-english\", use Whisper's built-in translation\n# For \"llm\", transcribe to Japanese and translate in Step 3\ncmd = ['whisperjav', str(folder_path), '--output-dir', str(folder_path)]\n\nif cfg['use_two_step']:\n    # Two-step ensemble mode\n    cmd.extend(['--ensemble',\n        '--pass1-pipeline', cfg['pass1_pipeline'],\n        '--pass1-sensitivity', cfg['pass1_sensitivity'],\n        '--pass2-pipeline', cfg['pass2_pipeline'],\n        '--pass2-sensitivity', cfg['pass2_sensitivity'],\n        '--merge-strategy', cfg['merge_strategy']])\n\n    # Add speech segmenters from basic config if not automatic\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--pass1-speech-segmenter', cfg['pass1_speech_segmenter']])\n    if cfg['pass2_speech_segmenter']:\n        cmd.extend(['--pass2-speech-segmenter', cfg['pass2_speech_segmenter']])\n\n    # Add models if not automatic\n    if cfg['pass1_model']:\n        cmd.extend(['--pass1-model', cfg['pass1_model']])\n    if cfg['pass2_model']:\n        cmd.extend(['--pass2-model', cfg['pass2_model']])\n\n    # Add expert options if provided\n    if expert:\n        # Pass 1 expert settings\n        if expert.get('pass1_scene_detector'):\n            cmd.extend(['--pass1-scene-detector', expert['pass1_scene_detector']])\n        if expert.get('pass1_speech_segmenter'):\n            # Expert speech segmenter overrides basic config\n            # Remove existing --pass1-speech-segmenter if present and add new one\n            if '--pass1-speech-segmenter' in cmd:\n                idx = cmd.index('--pass1-speech-segmenter')\n                cmd.pop(idx)  # Remove flag\n                cmd.pop(idx)  # Remove value\n            cmd.extend(['--pass1-speech-segmenter', expert['pass1_speech_segmenter']])\n        if expert.get('pass1_speech_enhancer'):\n            cmd.extend(['--pass1-speech-enhancer', expert['pass1_speech_enhancer']])\n        \n        # Pass 2 expert settings\n        if expert.get('pass2_scene_detector'):\n            cmd.extend(['--pass2-scene-detector', expert['pass2_scene_detector']])\n        if expert.get('pass2_speech_segmenter'):\n            # Expert speech segmenter overrides basic config\n            if '--pass2-speech-segmenter' in cmd:\n                idx = cmd.index('--pass2-speech-segmenter')\n                cmd.pop(idx)\n                cmd.pop(idx)\n            cmd.extend(['--pass2-speech-segmenter', expert['pass2_speech_segmenter']])\n        if expert.get('pass2_speech_enhancer'):\n            cmd.extend(['--pass2-speech-enhancer', expert['pass2_speech_enhancer']])\n\n    # Display mode info\n    p1_info = cfg['_quality']\n    if cfg['_speech_segmenter'] != 'automatic':\n        p1_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        p1_info += f\" ({cfg['_model']})\"\n    if expert:\n        if expert.get('_pass1_scene_detector') != 'automatic':\n            p1_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n        if expert.get('_pass1_speech_enhancer') != 'none':\n            p1_info += f\" [enh:{expert['_pass1_speech_enhancer']}]\"\n\n    p2_info = cfg['_secondpass_quality']\n    if cfg['_secondpass_speech_segmenter'] != 'automatic':\n        p2_info += f\" + {cfg['_secondpass_speech_segmenter']}\"\n    if cfg['_secondpass_model'] != 'automatic':\n        p2_info += f\" ({cfg['_secondpass_model']})\"\n    if expert:\n        if expert.get('_pass2_scene_detector') != 'automatic':\n            p2_info += f\" [scene:{expert['_pass2_scene_detector']}]\"\n        if expert.get('_pass2_speech_enhancer') != 'none':\n            p2_info += f\" [enh:{expert['_pass2_speech_enhancer']}]\"\n\n    print(f\"Mode: Two-Step\")\n    print(f\"  Pass 1: {p1_info}\")\n    print(f\"  Pass 2: {p2_info}\")\n    print(f\"  Merge: {cfg['_merge_method']}\")\nelse:\n    # Single-pass mode\n    cmd.extend(['--mode', cfg['pass1_pipeline'], '--sensitivity', cfg['pass1_sensitivity']])\n\n    # Add speech segmenter from basic config if not automatic\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--speech-segmenter', cfg['pass1_speech_segmenter']])\n\n    # Add model if specified\n    if cfg['pass1_model']:\n        cmd.extend(['--model', cfg['pass1_model']])\n\n    # Add expert options if provided (single-pass uses pass1 settings)\n    if expert:\n        if expert.get('pass1_scene_detector'):\n            cmd.extend(['--scene-detection-method', expert['pass1_scene_detector']])\n        if expert.get('pass1_speech_segmenter'):\n            # Expert speech segmenter overrides basic config\n            if '--speech-segmenter' in cmd:\n                idx = cmd.index('--speech-segmenter')\n                cmd.pop(idx)\n                cmd.pop(idx)\n            cmd.extend(['--speech-segmenter', expert['pass1_speech_segmenter']])\n        # Note: Single-pass speech enhancer support depends on CLI implementation\n\n    mode_info = f\"{cfg['_quality']}/{cfg['_speech_detection']}\"\n    if cfg['_speech_segmenter'] != 'automatic':\n        mode_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        mode_info += f\" ({cfg['_model']})\"\n    if expert:\n        if expert.get('_pass1_scene_detector') != 'automatic':\n            mode_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n        if expert.get('_pass1_speech_enhancer') != 'none':\n            mode_info += f\" [enh:{expert['_pass1_speech_enhancer']}]\"\n    print(f\"Mode: Standard ({mode_info})\")\n\n# Set subtitle language for transcription\nif cfg['subtitle_language'] == 'direct-to-english':\n    cmd.extend(['--subs-language', 'direct-to-english'])\n    print(f\"Output: English (Whisper auto-translate)\")\nelse:\n    # For both 'native' and 'llm', transcribe to Japanese first\n    cmd.extend(['--subs-language', 'native'])\n    if cfg['subtitle_language'] == 'llm':\n        print(f\"Output: Japanese (AI translation will follow in Step 3)\")\n    else:\n        print(f\"Output: Japanese\")\n\nprint(f\"Input: {folder_path}\\n\")\n\nfull_cmd = shlex.join(cmd)\nprocess = subprocess.Popen(full_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, universal_newlines=True)\nfor line in process.stdout:\n    print(line, end='')\nprocess.wait()\n\nif process.returncode != 0:\n    status(\"Transcription failed\", False)\n    raise SystemExit()\n\n# ═══════════════════════════════════════════\n# IDENTIFY NEW SRT FILES\n# ═══════════════════════════════════════════\nall_srts = set(folder_path.glob('*.srt'))\nnew_srts = list(all_srts - existing_srts)\nnew_srts.sort(key=lambda x: x.name)\n\n# Store for Step 3\nWHISPERJAV_NEW_SRTS = new_srts\nWHISPERJAV_FOLDER_PATH = folder_path\n\nstatus(f\"Created {len(new_srts)} new subtitle file(s)\")\n\n# ═══════════════════════════════════════════\n# ADD CREDITS\n# ═══════════════════════════════════════════\nsection(\"ADDING CREDITS\")\n\nif cfg['opening_credit'] or cfg['closing_credit']:\n    credits_count = 0\n    for srt_file in new_srts:\n        try:\n            content = srt_file.read_text(encoding='utf-8')\n            if cfg['opening_credit']:\n                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n            if cfg['closing_credit']:\n                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n            srt_file.write_text(content, encoding='utf-8')\n            credits_count += 1\n        except Exception as e:\n            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n    status(f\"Credits added to {credits_count} file(s)\")\nelse:\n    status(\"No credits configured\")\n\n# ═══════════════════════════════════════════\n# TRANSCRIPTION COMPLETE\n# ═══════════════════════════════════════════\nsection(\"TRANSCRIPTION COMPLETE\")\n\nif cfg['subtitle_language'] == 'llm' and cfg['api_key']:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>✓ Transcription done!</b> {len(new_srts)} file(s). AI Translation will start next...</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ Done!</b> {len(new_srts)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n    if cfg['subtitle_language'] == 'llm' and not cfg['api_key']:\n        print(\"Note: AI translation skipped (no API key provided)\")\n\n    # Auto-disconnect if no AI translation needed\n    if cfg['auto_disconnect']:\n        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n        time.sleep(10)\n        try:\n            from google.colab import runtime\n            runtime.unassign()\n        except: pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "translate"
   },
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation (if selected) { display-mode: \"form\" }\n",
    "#@markdown Translate each subtitle file using AI (only runs if \"English (AI translate)\" selected)\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"✓\" if ok else \"✗\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK PREREQUISITES\n",
    "# ═══════════════════════════════════════════\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "folder_path = WHISPERJAV_FOLDER_PATH\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK IF AI TRANSLATION IS NEEDED\n",
    "# ═══════════════════════════════════════════\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>ℹ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not cfg['api_key']:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for AI translation</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# SET UP API KEY\n",
    "# ═══════════════════════════════════════════\n",
    "env_map = {\n",
    "    \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "    \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "    \"gemini\": \"GEMINI_API_KEY\",\n",
    "    \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "    \"gpt\": \"OPENAI_API_KEY\"\n",
    "}\n",
    "os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSLATE EACH SRT FILE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"AI TRANSLATION\")\n",
    "print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['_translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "    # Build whisperjav-translate command\n",
    "    translate_cmd = [\n",
    "        'whisperjav-translate',\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream'\n",
    "    ]\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture stdout (output path) and stderr (progress)\n",
    "        stdout_lines = []\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "    print()  # Blank line between files\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSLATION COMPLETE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"COMPLETE\")\n",
    "\n",
    "total_srts = len(new_srts) + len(translated_files)\n",
    "\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>⚠ Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# AUTO-DISCONNECT\n",
    "# ═══════════════════════════════════════════\n",
    "if cfg['auto_disconnect']:\n",
    "    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "    except: pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}