{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# WhisperJAV Colab Edition\n",
    "\n",
    "| User Mode | What it does | Speed |\n",
    "|------|--------------|-------|\n",
    "| **Standard** | Processes your video once | Faster |\n",
    "| **Two-Step** | Processes twice and combines for better accuracy | Slower |\n",
    "\n",
    "| Option | What it controls |\n",
    "|--------|------------------|\n",
    "| **Speech Segmenter** | How to detect speech in audio (silero, ten, none) |\n",
    "| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n",
    "\n",
    "---\n",
    "<div style=\"font-size: 8px; line-height: 1.0;\">\n",
    "1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n",
    "2. Select settings and Click <b>Runtime → Run all</b> in the menu<br>\n",
    "3. <b>Connect Google Drive</b> when prompted<br>\n",
    "4. Wait for your subtitles!\n",
    "</div>\n",
    "\n",
    "<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1: Settings { display-mode: \"form\" }\n",
    "\n",
    "#@markdown **Transcription**\n",
    "quality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "speech_detection = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Two-Step Processing** *(slower but more accurate)*\n",
    "use_two_step = False #@param {type:\"boolean\"}\n",
    "secondpass_quality = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "secondpass_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "secondpass_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "secondpass_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "merge_method = \"prefer first step\" #@param [\"automatic\", \"keep all\", \"prefer first step\", \"prefer second step\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Files & Output**\n",
    "folder_name = \"WhisperJAV\" #@param {type:\"string\"}\n",
    "subtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **AI Translation** *(if selected \"English (AI translate)\")*\n",
    "translation_service = \"local\" #@param [\"local\", \"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\n",
    "local_model = \"gemma-9b\" #@param [\"gemma-9b\", \"llama-8b\", \"llama-3b\", \"auto\"]\n",
    "#@markdown <font size=\"1\">local: Free, runs on GPU. gemma-9b (8GB+ VRAM), llama-8b (6GB+), llama-3b (3GB+). Cloud providers require API key.</font>\n",
    "api_key = \"\" #@param {type:\"string\"}\n",
    "translation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Credits**\n",
    "opening_credit = \"\" #@param {type:\"string\"}\n",
    "closing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Session**\n",
    "auto_disconnect = True #@param {type:\"boolean\"}\n",
    "#@markdown ☝️ Auto-disconnect when done (saves GPU credits)\n",
    "\n",
    "# Mapping dictionaries\n",
    "combine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n",
    "               \"prefer first step\": \"pass1_primary\", \"prefer second step\": \"pass2_primary\"}\n",
    "language_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n",
    "                \"English (AI translate)\": \"llm\"}\n",
    "tone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n",
    "\n",
    "# Speech segmenter mapping (None = use pipeline default)\n",
    "segmenter_map = {\"automatic\": None, \"silero\": \"silero\", \"ten\": \"ten\", \"none\": \"none\"}\n",
    "\n",
    "# Model mapping (None = use pipeline default)\n",
    "model_map = {\n",
    "    \"automatic\": None,\n",
    "    \"large-v2\": \"large-v2\",\n",
    "    \"large-v3\": \"large-v3\",\n",
    "    \"turbo\": \"large-v3-turbo\",\n",
    "    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n",
    "    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n",
    "    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n",
    "    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n",
    "}\n",
    "\n",
    "# Define model compatibility:\n",
    "# - Kotoba models (HuggingFace) ONLY work with \"transformers\" pipeline\n",
    "# - Legacy models (large-v2/v3/turbo) work with ALL pipelines (faster, fast, balanced, fidelity, transformers)\n",
    "KOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\n",
    "LEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n",
    "\n",
    "# Auto-correct incompatible model-pipeline combinations\n",
    "warnings_list = []\n",
    "\n",
    "# Check Pass 1 compatibility\n",
    "if model in KOTOBA_MODELS and quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 1: {model} requires 'transformers' pipeline. Auto-correcting from '{quality}' to 'transformers'.\")\n",
    "    quality = \"transformers\"\n",
    "\n",
    "# Check Pass 2 compatibility (only relevant if two-step is enabled)\n",
    "if use_two_step and secondpass_model in KOTOBA_MODELS and secondpass_quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 2: {secondpass_model} requires 'transformers' pipeline. Auto-correcting from '{secondpass_quality}' to 'transformers'.\")\n",
    "    secondpass_quality = \"transformers\"\n",
    "\n",
    "# Venv path for WhisperJAV commands\n",
    "VENV_PATH = \"/content/whisperjav_env\"\n",
    "WHISPERJAV_CMD = f\"{VENV_PATH}/bin/whisperjav\"\n",
    "WHISPERJAV_TRANSLATE_CMD = f\"{VENV_PATH}/bin/whisperjav-translate\"\n",
    "\n",
    "WHISPERJAV_CONFIG = {\n",
    "    'use_two_step': use_two_step,\n",
    "    'pass1_pipeline': quality,\n",
    "    'pass1_sensitivity': speech_detection,\n",
    "    'pass1_speech_segmenter': segmenter_map[speech_segmenter],\n",
    "    'pass1_model': model_map[model],\n",
    "    'pass2_pipeline': secondpass_quality,\n",
    "    'pass2_sensitivity': secondpass_sensitivity,\n",
    "    'pass2_speech_segmenter': segmenter_map[secondpass_speech_segmenter],\n",
    "    'pass2_model': model_map[secondpass_model],\n",
    "    'merge_strategy': combine_map[merge_method],\n",
    "    'folder_name': folder_name,\n",
    "    'subtitle_language': language_map[subtitle_language],\n",
    "    'translation_service': translation_service,\n",
    "    'local_model': local_model,\n",
    "    'api_key': api_key,\n",
    "    'translation_style': tone_map[translation_style],\n",
    "    'opening_credit': opening_credit,\n",
    "    'closing_credit': closing_credit,\n",
    "    'auto_disconnect': auto_disconnect,\n",
    "    'venv_path': VENV_PATH,\n",
    "    'whisperjav_cmd': WHISPERJAV_CMD,\n",
    "    'whisperjav_translate_cmd': WHISPERJAV_TRANSLATE_CMD,\n",
    "    # Display values (for status messages)\n",
    "    '_quality': quality,\n",
    "    '_speech_detection': speech_detection,\n",
    "    '_speech_segmenter': speech_segmenter,\n",
    "    '_model': model,\n",
    "    '_secondpass_quality': secondpass_quality,\n",
    "    '_secondpass_sensitivity': secondpass_sensitivity,\n",
    "    '_secondpass_speech_segmenter': secondpass_speech_segmenter,\n",
    "    '_secondpass_model': secondpass_model,\n",
    "    '_merge_method': merge_method,\n",
    "    '_subtitle_language': subtitle_language,\n",
    "    '_translation_style': translation_style,\n",
    "}\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display any auto-correction warnings\n",
    "for warning in warnings_list:\n",
    "    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>⚠ Auto-corrected:</b> {warning}</div>'))\n",
    "\n",
    "# Build status display\n",
    "if use_two_step:\n",
    "    mode_text = \"Two-Step\"\n",
    "    p1_info = f\"{quality}\"\n",
    "    if speech_segmenter != \"automatic\":\n",
    "        p1_info += f\"/{speech_segmenter}\"\n",
    "    if model != \"automatic\":\n",
    "        p1_info += f\"/{model}\"\n",
    "    p2_info = f\"{secondpass_quality}\"\n",
    "    if secondpass_speech_segmenter != \"automatic\":\n",
    "        p2_info += f\"/{secondpass_speech_segmenter}\"\n",
    "    if secondpass_model != \"automatic\":\n",
    "        p2_info += f\"/{secondpass_model}\"\n",
    "    details = f\"{p1_info} → {p2_info}\"\n",
    "else:\n",
    "    mode_text = \"Standard\"\n",
    "    details = f\"{quality}/{speech_detection}\"\n",
    "    if speech_segmenter != \"automatic\":\n",
    "        details += f\"/{speech_segmenter}\"\n",
    "    if model != \"automatic\":\n",
    "        details += f\"/{model}\"\n",
    "\n",
    "display(HTML(f'<div style=\"padding:6px 10px;background:#f0f9ff;border-radius:4px;font-size:10px\"><b>Settings:</b> {mode_text} ({details}) | Folder: {folder_name} | Output: {subtitle_language}</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 2: Transcribe { display-mode: \"form\" }\n#@markdown Connect Drive → Install → Transcribe all media files → Add credits\n\nimport os, sys, subprocess, shlex, time\nfrom pathlib import Path\nfrom IPython.display import display, HTML\n\ndef status(msg, ok=True):\n    icon = \"✓\" if ok else \"✗\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n\n# Check config\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\ncfg = WHISPERJAV_CONFIG\n\n# ═══════════════════════════════════════════\n# CONNECT GOOGLE DRIVE\n# ═══════════════════════════════════════════\nsection(\"CONNECTING GOOGLE DRIVE\")\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n    folder_path.mkdir(parents=True, exist_ok=True)\n    status(f\"Connected: {folder_path}\")\nexcept Exception as e:\n    status(f\"Failed to connect: {e}\", False)\n    raise SystemExit(\"Google Drive connection failed\")\n\n# ═══════════════════════════════════════════\n# CHECK GPU\n# ═══════════════════════════════════════════\nsection(\"CHECKING GPU\")\ngpu_check = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"], capture_output=True, text=True)\nif gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n    status(\"No GPU detected. Go to Runtime → Change runtime type → T4 GPU\", False)\n    raise SystemExit(\"No GPU detected\")\nstatus(f\"GPU: {gpu_check.stdout.strip()}\")\n\n# ═══════════════════════════════════════════\n# INSTALL WHISPERJAV\n# ═══════════════════════════════════════════\nsection(\"INSTALLING WHISPERJAV\")\ninstall_start = time.time()\n\nREPO_URL = \"https://github.com/meizhong986/WhisperJAV.git\"\nREPO_PATH = \"/content/WhisperJAV\"\nSCRIPT_PATH = f\"{REPO_PATH}/installer/install_colab.sh\"\n\ndef run_installer():\n    \"\"\"Run install script with real-time output streaming.\"\"\"\n    env = {**os.environ, \"PATH\": f\"{os.environ.get('PATH', '')}:{os.path.expanduser('~/.local/bin')}\"}\n    process = subprocess.Popen(\n        [\"bash\", SCRIPT_PATH],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        bufsize=1,\n        text=True,\n        env=env\n    )\n    for line in iter(process.stdout.readline, ''):\n        print(line, end='', flush=True)\n    process.wait()\n    return process.returncode\n\n# Check if already installed\nvenv_python = f\"{cfg['venv_path']}/bin/python\"\nif os.path.exists(venv_python):\n    check = subprocess.run([venv_python, \"-c\", \"import whisperjav\"], capture_output=True)\n    if check.returncode == 0:\n        status(\"WhisperJAV already installed (skipping)\")\n    else:\n        status(\"Existing venv corrupt, reinstalling...\")\n        subprocess.run([\"rm\", \"-rf\", cfg['venv_path']], capture_output=True)\n        # Clone and install\n        if not os.path.exists(REPO_PATH):\n            subprocess.run([\"git\", \"clone\", REPO_URL, REPO_PATH], capture_output=True)\n        returncode = run_installer()\n        if returncode != 0:\n            status(\"Installation failed\", False)\n            raise SystemExit(\"Installation failed\")\nelse:\n    # Fresh install: clone repo and run installer\n    print(\"Installing WhisperJAV (uv-accelerated)...\\n\")\n    sys.stdout.flush()\n    if not os.path.exists(REPO_PATH):\n        print(f\"Cloning {REPO_URL}...\")\n        result = subprocess.run([\"git\", \"clone\", REPO_URL, REPO_PATH], capture_output=True, text=True)\n        if result.returncode != 0:\n            status(f\"Failed to clone: {result.stderr}\", False)\n            raise SystemExit(\"Clone failed\")\n\n    if not os.path.exists(SCRIPT_PATH):\n        status(f\"Install script not found at {SCRIPT_PATH}\", False)\n        raise SystemExit(\"Install script missing from repo\")\n\n    returncode = run_installer()\n    if returncode != 0:\n        status(\"Installation failed\", False)\n        raise SystemExit(\"Installation failed\")\n\nstatus(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n\n# ═══════════════════════════════════════════\n# FIND MEDIA FILES\n# ═══════════════════════════════════════════\nsection(\"SCANNING FILES\")\nvideo_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\nvideos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n\nif not videos:\n    status(f\"No media files in {cfg['folder_name']}/\", False)\n    raise SystemExit(\"No media files found\")\n\nstatus(f\"Found {len(videos)} file(s)\")\nfor v in videos[:5]:\n    print(f\"  • {v.name}\")\nif len(videos) > 5:\n    print(f\"  ... and {len(videos)-5} more\")\n\nexisting_srts = set(folder_path.glob('*.srt'))\n\n# ═══════════════════════════════════════════\n# TRANSCRIBE\n# ═══════════════════════════════════════════\nsection(\"TRANSCRIBING\")\n\ncmd = [cfg['whisperjav_cmd'], str(folder_path), '--output-dir', str(folder_path)]\n\nif cfg['use_two_step']:\n    cmd.extend(['--ensemble',\n        '--pass1-pipeline', cfg['pass1_pipeline'],\n        '--pass1-sensitivity', cfg['pass1_sensitivity'],\n        '--pass2-pipeline', cfg['pass2_pipeline'],\n        '--pass2-sensitivity', cfg['pass2_sensitivity'],\n        '--merge-strategy', cfg['merge_strategy']])\n\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--pass1-speech-segmenter', cfg['pass1_speech_segmenter']])\n    if cfg['pass2_speech_segmenter']:\n        cmd.extend(['--pass2-speech-segmenter', cfg['pass2_speech_segmenter']])\n    if cfg['pass1_model']:\n        cmd.extend(['--pass1-model', cfg['pass1_model']])\n    if cfg['pass2_model']:\n        cmd.extend(['--pass2-model', cfg['pass2_model']])\n\n    p1_info = cfg['_quality']\n    if cfg['_speech_segmenter'] != 'automatic':\n        p1_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        p1_info += f\" ({cfg['_model']})\"\n\n    p2_info = cfg['_secondpass_quality']\n    if cfg['_secondpass_speech_segmenter'] != 'automatic':\n        p2_info += f\" + {cfg['_secondpass_speech_segmenter']}\"\n    if cfg['_secondpass_model'] != 'automatic':\n        p2_info += f\" ({cfg['_secondpass_model']})\"\n\n    print(f\"Mode: Two-Step\")\n    print(f\"  Pass 1: {p1_info}\")\n    print(f\"  Pass 2: {p2_info}\")\n    print(f\"  Merge: {cfg['_merge_method']}\")\nelse:\n    cmd.extend(['--mode', cfg['pass1_pipeline'], '--sensitivity', cfg['pass1_sensitivity']])\n\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--speech-segmenter', cfg['pass1_speech_segmenter']])\n\n    mode_info = f\"{cfg['_quality']}/{cfg['_speech_detection']}\"\n    if cfg['_speech_segmenter'] != 'automatic':\n        mode_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        mode_info += f\" ({cfg['_model']})\"\n    print(f\"Mode: Standard ({mode_info})\")\n\nif cfg['subtitle_language'] == 'direct-to-english':\n    cmd.extend(['--subs-language', 'direct-to-english'])\n    print(f\"Output: English (Whisper auto-translate)\")\nelse:\n    cmd.extend(['--subs-language', 'native'])\n    if cfg['subtitle_language'] == 'llm':\n        print(f\"Output: Japanese (AI translation will follow in Step 3)\")\n    else:\n        print(f\"Output: Japanese\")\n\nprint(f\"Input: {folder_path}\\n\")\n\nfull_cmd = shlex.join(cmd)\nprocess = subprocess.Popen(full_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, universal_newlines=True)\nfor line in process.stdout:\n    print(line, end='')\nprocess.wait()\n\nif process.returncode != 0:\n    status(\"Transcription failed\", False)\n    raise SystemExit(\"Transcription failed\")\n\n# ═══════════════════════════════════════════\n# IDENTIFY NEW SRT FILES\n# ═══════════════════════════════════════════\nall_srts = set(folder_path.glob('*.srt'))\nnew_srts = list(all_srts - existing_srts)\nnew_srts.sort(key=lambda x: x.name)\n\nWHISPERJAV_NEW_SRTS = new_srts\nWHISPERJAV_FOLDER_PATH = folder_path\n\nstatus(f\"Created {len(new_srts)} new subtitle file(s)\")\n\n# ═══════════════════════════════════════════\n# ADD CREDITS\n# ═══════════════════════════════════════════\nsection(\"ADDING CREDITS\")\n\nif cfg['opening_credit'] or cfg['closing_credit']:\n    credits_count = 0\n    for srt_file in new_srts:\n        try:\n            content = srt_file.read_text(encoding='utf-8')\n            if cfg['opening_credit']:\n                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n            if cfg['closing_credit']:\n                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n            srt_file.write_text(content, encoding='utf-8')\n            credits_count += 1\n        except Exception as e:\n            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n    status(f\"Credits added to {credits_count} file(s)\")\nelse:\n    status(\"No credits configured\")\n\n# ═══════════════════════════════════════════\n# TRANSCRIPTION COMPLETE\n# ═══════════════════════════════════════════\nsection(\"TRANSCRIPTION COMPLETE\")\n\nif cfg['subtitle_language'] == 'llm' and (cfg['api_key'] or cfg['translation_service'] == 'local'):\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>✓ Transcription done!</b> {len(new_srts)} file(s). AI Translation will start next...</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ Done!</b> {len(new_srts)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n    if cfg['subtitle_language'] == 'llm' and not cfg['api_key'] and cfg['translation_service'] != 'local':\n        print(\"Note: AI translation skipped (no API key provided)\")\n\n    if cfg['auto_disconnect']:\n        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n        time.sleep(10)\n        try:\n            from google.colab import runtime\n            runtime.unassign()\n        except: pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation (if selected) { display-mode: \"form\" }\n",
    "#@markdown Translate each subtitle file using AI (only runs if \"English (AI translate)\" selected)\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"✓\" if ok else \"✗\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK PREREQUISITES\n",
    "# ═══════════════════════════════════════════\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "folder_path = WHISPERJAV_FOLDER_PATH\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK IF AI TRANSLATION IS NEEDED\n",
    "# ═══════════════════════════════════════════\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>ℹ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# Check API key requirement (not needed for local provider)\n",
    "is_local = cfg['translation_service'] == 'local'\n",
    "if not is_local and not cfg['api_key']:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for cloud translation. Use \"local\" provider for free GPU translation.</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# SET UP API KEY (for cloud providers)\n",
    "# ═══════════════════════════════════════════\n",
    "if not is_local:\n",
    "    env_map = {\n",
    "        \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "        \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "        \"gemini\": \"GEMINI_API_KEY\",\n",
    "        \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "        \"gpt\": \"OPENAI_API_KEY\"\n",
    "    }\n",
    "    os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSLATE EACH SRT FILE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"AI TRANSLATION\")\n",
    "if is_local:\n",
    "    print(f\"Provider: local ({cfg.get('local_model', 'gemma-9b')})\")\n",
    "    print(\"Note: First run downloads model (~5GB)\")\n",
    "else:\n",
    "    print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['_translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "    # Build whisperjav-translate command using venv path\n",
    "    translate_cmd = [\n",
    "        cfg['whisperjav_translate_cmd'],\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream'\n",
    "    ]\n",
    "\n",
    "    # Add model for local provider\n",
    "    if is_local:\n",
    "        translate_cmd.extend(['--model', cfg.get('local_model', 'gemma-9b')])\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture stdout (output path) and stderr (progress)\n",
    "        stdout_lines = []\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "    print()  # Blank line between files\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSLATION COMPLETE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"COMPLETE\")\n",
    "\n",
    "total_srts = len(new_srts) + len(translated_files)\n",
    "\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>⚠ Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# AUTO-DISCONNECT\n",
    "# ═══════════════════════════════════════════\n",
    "if cfg['auto_disconnect']:\n",
    "    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "    except: pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}