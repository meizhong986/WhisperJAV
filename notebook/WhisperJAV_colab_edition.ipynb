{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# WhisperJAV Colab Edition\n\n| User Mode | What it does | Speed |\n|------|--------------|-------|\n| **Standard** | Processes your video once | Faster |\n| **Two-Step** | Processes twice and combines for better accuracy | Slower |\n\n| Option | What it controls |\n|--------|------------------|\n| **Speech Segmenter** | How to detect speech in audio (silero, ten, none) |\n| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n\n---\n<div style=\"font-size: 8px; line-height: 1.0;\">\n1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n2. Select settings and Click <b>Runtime → Run all</b> in the menu<br>\n3. <b>Connect Google Drive</b> when prompted<br>\n4. Wait for your subtitles!\n</div>\n\n<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure"
   },
   "outputs": [],
   "source": "#@title Step 1: Settings { display-mode: \"form\" }\n\n#@markdown **Transcription**\nquality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\nspeech_detection = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\nspeech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\nmodel = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n\n#@markdown ---\n#@markdown **Two-Step Processing** *(slower but more accurate)*\nuse_two_step = False #@param {type:\"boolean\"}\nsecondpass_quality = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\nsecondpass_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\nsecondpass_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\nsecondpass_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\nmerge_method = \"prefer first step\" #@param [\"automatic\", \"keep all\", \"prefer first step\", \"prefer second step\"]\n\n#@markdown ---\n#@markdown **Files & Output**\nfolder_name = \"WhisperJAV\" #@param {type:\"string\"}\nsubtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n\n#@markdown ---\n#@markdown **AI Translation** *(if selected \"English (AI translate)\")*\ntranslation_service = \"deepseek\" #@param [\"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\napi_key = \"\" #@param {type:\"string\"}\ntranslation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n\n#@markdown ---\n#@markdown **Credits**\nopening_credit = \"\" #@param {type:\"string\"}\nclosing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown **Session**\nauto_disconnect = True #@param {type:\"boolean\"}\n#@markdown ☝️ Auto-disconnect when done (saves GPU credits)\n\n# Mapping dictionaries\ncombine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n               \"prefer first step\": \"pass1_primary\", \"prefer second step\": \"pass2_primary\"}\nlanguage_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n                \"English (AI translate)\": \"llm\"}\ntone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n\n# Speech segmenter mapping (None = use pipeline default)\nsegmenter_map = {\"automatic\": None, \"silero\": \"silero\", \"ten\": \"ten\", \"none\": \"none\"}\n\n# Model mapping (None = use pipeline default)\nmodel_map = {\n    \"automatic\": None,\n    \"large-v2\": \"large-v2\",\n    \"large-v3\": \"large-v3\",\n    \"turbo\": \"large-v3-turbo\",\n    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n}\n\nWHISPERJAV_CONFIG = {\n    'use_two_step': use_two_step,\n    'pass1_pipeline': quality,\n    'pass1_sensitivity': speech_detection,\n    'pass1_speech_segmenter': segmenter_map[speech_segmenter],\n    'pass1_model': model_map[model],\n    'pass2_pipeline': secondpass_quality,\n    'pass2_sensitivity': secondpass_sensitivity,\n    'pass2_speech_segmenter': segmenter_map[secondpass_speech_segmenter],\n    'pass2_model': model_map[secondpass_model],\n    'merge_strategy': combine_map[merge_method],\n    'folder_name': folder_name,\n    'subtitle_language': language_map[subtitle_language],\n    'translation_service': translation_service,\n    'api_key': api_key,\n    'translation_style': tone_map[translation_style],\n    'opening_credit': opening_credit,\n    'closing_credit': closing_credit,\n    'auto_disconnect': auto_disconnect,\n    # Display values (for status messages)\n    '_quality': quality,\n    '_speech_detection': speech_detection,\n    '_speech_segmenter': speech_segmenter,\n    '_model': model,\n    '_secondpass_quality': secondpass_quality,\n    '_secondpass_sensitivity': secondpass_sensitivity,\n    '_secondpass_speech_segmenter': secondpass_speech_segmenter,\n    '_secondpass_model': secondpass_model,\n    '_merge_method': merge_method,\n    '_subtitle_language': subtitle_language,\n    '_translation_style': translation_style,\n}\n\nfrom IPython.display import display, HTML\n\n# Build status display\nif use_two_step:\n    mode_text = \"Two-Step\"\n    p1_info = f\"{quality}\"\n    if speech_segmenter != \"automatic\":\n        p1_info += f\"/{speech_segmenter}\"\n    if model != \"automatic\":\n        p1_info += f\"/{model}\"\n    p2_info = f\"{secondpass_quality}\"\n    if secondpass_speech_segmenter != \"automatic\":\n        p2_info += f\"/{secondpass_speech_segmenter}\"\n    if secondpass_model != \"automatic\":\n        p2_info += f\"/{secondpass_model}\"\n    details = f\"{p1_info} → {p2_info}\"\nelse:\n    mode_text = \"Standard\"\n    details = f\"{quality}/{speech_detection}\"\n    if speech_segmenter != \"automatic\":\n        details += f\"/{speech_segmenter}\"\n    if model != \"automatic\":\n        details += f\"/{model}\"\n\ndisplay(HTML(f'<div style=\"padding:6px 10px;background:#f0f9ff;border-radius:4px;font-size:10px\"><b>Settings:</b> {mode_text} ({details}) | Folder: {folder_name} | Output: {subtitle_language}</div>'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transcribe"
   },
   "outputs": [],
   "source": "#@title Step 2: Transcribe { display-mode: \"form\" }\n#@markdown Connect Drive → Install → Transcribe all media files → Add credits\n\nimport os, sys, subprocess, shlex, time\nfrom pathlib import Path\nfrom IPython.display import display, HTML, clear_output\n\ndef status(msg, ok=True):\n    icon = \"✓\" if ok else \"✗\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n\n# Check config\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\ncfg = WHISPERJAV_CONFIG\n\n# ═══════════════════════════════════════════\n# CONNECT GOOGLE DRIVE\n# ═══════════════════════════════════════════\nsection(\"CONNECTING GOOGLE DRIVE\")\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n    folder_path.mkdir(parents=True, exist_ok=True)\n    status(f\"Connected: {folder_path}\")\nexcept Exception as e:\n    status(f\"Failed to connect: {e}\", False)\n    raise SystemExit()\n\n# ═══════════════════════════════════════════\n# CHECK GPU\n# ═══════════════════════════════════════════\nsection(\"CHECKING GPU\")\ngpu_check = subprocess.run(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\", shell=True, capture_output=True, text=True)\nif gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n    status(\"No GPU detected. Go to Runtime → Change runtime type → T4 GPU\", False)\n    raise SystemExit()\nstatus(f\"GPU: {gpu_check.stdout.strip()}\")\n\n# ═══════════════════════════════════════════\n# INSTALL WHISPERJAV\n# ═══════════════════════════════════════════\nsection(\"INSTALLING (2-3 min)\")\ninstall_start = time.time()\n\nsteps = [\n    (\"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev > /dev/null 2>&1\", \"System tools\"),\n    (\"pip install -q tqdm numba tiktoken ffmpeg-python soundfile auditok numpy scipy pysrt srt aiofiles jsonschema Pillow colorama librosa matplotlib pyloudnorm requests faster-whisper transformers optimum accelerate huggingface-hub pydantic ten-vad silero-vad pydub regex modelscope addict\", \"Python packages\"),\n    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n    (\"pip install -q git+https://github.com/meizhong986/WhisperJAV.git@main\", \"WhisperJAV\")\n]\n\nfor cmd, name in steps:\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        status(f\"{name} failed\", False)\n        raise SystemExit()\n    status(name)\n\nstatus(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n\n# ═══════════════════════════════════════════\n# FIND MEDIA FILES\n# ═══════════════════════════════════════════\nsection(\"SCANNING FILES\")\nvideo_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\nvideos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n\nif not videos:\n    status(f\"No media files in {cfg['folder_name']}/\", False)\n    raise SystemExit()\n\nstatus(f\"Found {len(videos)} file(s)\")\nfor v in videos[:5]:\n    print(f\"  • {v.name}\")\nif len(videos) > 5:\n    print(f\"  ... and {len(videos)-5} more\")\n\n# Record existing SRT files before transcription\nexisting_srts = set(folder_path.glob('*.srt'))\n\n# ═══════════════════════════════════════════\n# TRANSCRIBE\n# ═══════════════════════════════════════════\nsection(\"TRANSCRIBING\")\n\n# Build command - always transcribe to native (Japanese) first\n# For \"direct-to-english\", use Whisper's built-in translation\n# For \"llm\", transcribe to Japanese and translate in Step 3\ncmd = ['whisperjav', str(folder_path), '--output-dir', str(folder_path)]\n\nif cfg['use_two_step']:\n    # Two-step ensemble mode\n    cmd.extend(['--ensemble',\n        '--pass1-pipeline', cfg['pass1_pipeline'],\n        '--pass1-sensitivity', cfg['pass1_sensitivity'],\n        '--pass2-pipeline', cfg['pass2_pipeline'],\n        '--pass2-sensitivity', cfg['pass2_sensitivity'],\n        '--merge-strategy', cfg['merge_strategy']])\n\n    # Add speech segmenters if not automatic\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--pass1-speech-segmenter', cfg['pass1_speech_segmenter']])\n    if cfg['pass2_speech_segmenter']:\n        cmd.extend(['--pass2-speech-segmenter', cfg['pass2_speech_segmenter']])\n\n    # Add models if not automatic\n    if cfg['pass1_model']:\n        cmd.extend(['--pass1-model', cfg['pass1_model']])\n    if cfg['pass2_model']:\n        cmd.extend(['--pass2-model', cfg['pass2_model']])\n\n    # Display mode info\n    p1_info = cfg['_quality']\n    if cfg['_speech_segmenter'] != 'automatic':\n        p1_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        p1_info += f\" ({cfg['_model']})\"\n\n    p2_info = cfg['_secondpass_quality']\n    if cfg['_secondpass_speech_segmenter'] != 'automatic':\n        p2_info += f\" + {cfg['_secondpass_speech_segmenter']}\"\n    if cfg['_secondpass_model'] != 'automatic':\n        p2_info += f\" ({cfg['_secondpass_model']})\"\n\n    print(f\"Mode: Two-Step\")\n    print(f\"  Pass 1: {p1_info}\")\n    print(f\"  Pass 2: {p2_info}\")\n    print(f\"  Merge: {cfg['_merge_method']}\")\nelse:\n    # Single-pass mode\n    cmd.extend(['--mode', cfg['pass1_pipeline'], '--sensitivity', cfg['pass1_sensitivity']])\n\n    # Add speech segmenter if not automatic\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--speech-segmenter', cfg['pass1_speech_segmenter']])\n\n    # Add model if not automatic (for single-pass, use --model flag if available)\n    # Note: --model flag may not be available for all pipelines\n    # The pipeline will use its default model if not specified\n\n    mode_info = f\"{cfg['_quality']}/{cfg['_speech_detection']}\"\n    if cfg['_speech_segmenter'] != 'automatic':\n        mode_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        mode_info += f\" ({cfg['_model']})\"\n    print(f\"Mode: Standard ({mode_info})\")\n\n# Set subtitle language for transcription\nif cfg['subtitle_language'] == 'direct-to-english':\n    cmd.extend(['--subs-language', 'direct-to-english'])\n    print(f\"Output: English (Whisper auto-translate)\")\nelse:\n    # For both 'native' and 'llm', transcribe to Japanese first\n    cmd.extend(['--subs-language', 'native'])\n    if cfg['subtitle_language'] == 'llm':\n        print(f\"Output: Japanese (AI translation will follow in Step 3)\")\n    else:\n        print(f\"Output: Japanese\")\n\nprint(f\"Input: {folder_path}\\n\")\n\nfull_cmd = shlex.join(cmd)\nprocess = subprocess.Popen(full_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, universal_newlines=True)\nfor line in process.stdout:\n    print(line, end='')\nprocess.wait()\n\nif process.returncode != 0:\n    status(\"Transcription failed\", False)\n    raise SystemExit()\n\n# ═══════════════════════════════════════════\n# IDENTIFY NEW SRT FILES\n# ═══════════════════════════════════════════\nall_srts = set(folder_path.glob('*.srt'))\nnew_srts = list(all_srts - existing_srts)\nnew_srts.sort(key=lambda x: x.name)\n\n# Store for Step 3\nWHISPERJAV_NEW_SRTS = new_srts\nWHISPERJAV_FOLDER_PATH = folder_path\n\nstatus(f\"Created {len(new_srts)} new subtitle file(s)\")\n\n# ═══════════════════════════════════════════\n# ADD CREDITS\n# ═══════════════════════════════════════════\nsection(\"ADDING CREDITS\")\n\nif cfg['opening_credit'] or cfg['closing_credit']:\n    credits_count = 0\n    for srt_file in new_srts:\n        try:\n            content = srt_file.read_text(encoding='utf-8')\n            if cfg['opening_credit']:\n                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n            if cfg['closing_credit']:\n                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n            srt_file.write_text(content, encoding='utf-8')\n            credits_count += 1\n        except Exception as e:\n            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n    status(f\"Credits added to {credits_count} file(s)\")\nelse:\n    status(\"No credits configured\")\n\n# ═══════════════════════════════════════════\n# TRANSCRIPTION COMPLETE\n# ═══════════════════════════════════════════\nsection(\"TRANSCRIPTION COMPLETE\")\n\nif cfg['subtitle_language'] == 'llm' and cfg['api_key']:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>✓ Transcription done!</b> {len(new_srts)} file(s). AI Translation will start next...</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ Done!</b> {len(new_srts)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n    if cfg['subtitle_language'] == 'llm' and not cfg['api_key']:\n        print(\"Note: AI translation skipped (no API key provided)\")\n\n    # Auto-disconnect if no AI translation needed\n    if cfg['auto_disconnect']:\n        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n        time.sleep(10)\n        try:\n            from google.colab import runtime\n            runtime.unassign()\n        except: pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "translate"
   },
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation (if selected) { display-mode: \"form\" }\n",
    "#@markdown Translate each subtitle file using AI (only runs if \"English (AI translate)\" selected)\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"✓\" if ok else \"✗\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK PREREQUISITES\n",
    "# ═══════════════════════════════════════════\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "folder_path = WHISPERJAV_FOLDER_PATH\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK IF AI TRANSLATION IS NEEDED\n",
    "# ═══════════════════════════════════════════\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>ℹ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not cfg['api_key']:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for AI translation</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# SET UP API KEY\n",
    "# ═══════════════════════════════════════════\n",
    "env_map = {\n",
    "    \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "    \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "    \"gemini\": \"GEMINI_API_KEY\",\n",
    "    \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "    \"gpt\": \"OPENAI_API_KEY\"\n",
    "}\n",
    "os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSLATE EACH SRT FILE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"AI TRANSLATION\")\n",
    "print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['_translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "    # Build whisperjav-translate command\n",
    "    translate_cmd = [\n",
    "        'whisperjav-translate',\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream'\n",
    "    ]\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture stdout (output path) and stderr (progress)\n",
    "        stdout_lines = []\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "    print()  # Blank line between files\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSLATION COMPLETE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"COMPLETE\")\n",
    "\n",
    "total_srts = len(new_srts) + len(translated_files)\n",
    "\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>⚠ Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# AUTO-DISCONNECT\n",
    "# ═══════════════════════════════════════════\n",
    "if cfg['auto_disconnect']:\n",
    "    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "    except: pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}