{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# WhisperJAV Colab Edition\n",
    "\n",
    "| User Mode | What it does | Speed |\n",
    "|------|--------------|-------|\n",
    "| **Standard** | Processes your video once | Faster |\n",
    "| **Two-Step** | Processes twice and combines for better accuracy | Slower |\n",
    "\n",
    "| Option | What it controls |\n",
    "|--------|------------------|\n",
    "| **Speech Segmenter** | How to detect speech in audio (silero, ten, none) |\n",
    "| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n",
    "\n",
    "---\n",
    "<div style=\"font-size: 8px; line-height: 1.0;\">\n",
    "1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n",
    "2. Select settings and Click <b>Runtime → Run all</b> in the menu<br>\n",
    "3. <b>Connect Google Drive</b> when prompted<br>\n",
    "4. Wait for your subtitles!\n",
    "</div>\n",
    "\n",
    "<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure"
   },
   "outputs": [],
   "source": [
    "#@title Step 1: Settings { display-mode: \"form\" }\n",
    "\n",
    "#@markdown **Transcription**\n",
    "quality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "speech_detection = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Two-Step Processing** *(slower but more accurate)*\n",
    "use_two_step = False #@param {type:\"boolean\"}\n",
    "secondpass_quality = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "secondpass_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "secondpass_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "secondpass_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "merge_method = \"prefer first step\" #@param [\"automatic\", \"keep all\", \"prefer first step\", \"prefer second step\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Files & Output**\n",
    "folder_name = \"WhisperJAV\" #@param {type:\"string\"}\n",
    "subtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **AI Translation** *(if selected \"English (AI translate)\")*\n",
    "translation_service = \"local\" #@param [\"local\", \"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\n",
    "#@markdown ☝️ **local** = Free, runs on Colab GPU. Others need API key.\n",
    "api_key = \"\" #@param {type:\"string\"}\n",
    "#@markdown ☝️ Leave empty for **local** provider\n",
    "translation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Credits**\n",
    "opening_credit = \"\" #@param {type:\"string\"}\n",
    "closing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Session**\n",
    "auto_disconnect = True #@param {type:\"boolean\"}\n",
    "#@markdown ☝️ Auto-disconnect when done (saves GPU credits)\n",
    "\n",
    "# Mapping dictionaries\n",
    "combine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n",
    "               \"prefer first step\": \"pass1_primary\", \"prefer second step\": \"pass2_primary\"}\n",
    "language_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n",
    "                \"English (AI translate)\": \"llm\"}\n",
    "tone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n",
    "\n",
    "# Speech segmenter mapping (None = use pipeline default)\n",
    "segmenter_map = {\"automatic\": None, \"silero\": \"silero\", \"ten\": \"ten\", \"none\": \"none\"}\n",
    "\n",
    "# Model mapping (None = use pipeline default)\n",
    "model_map = {\n",
    "    \"automatic\": None,\n",
    "    \"large-v2\": \"large-v2\",\n",
    "    \"large-v3\": \"large-v3\",\n",
    "    \"turbo\": \"large-v3-turbo\",\n",
    "    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n",
    "    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n",
    "    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n",
    "    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n",
    "}\n",
    "\n",
    "# Define model compatibility:\n",
    "# - Kotoba models (HuggingFace) ONLY work with \"transformers\" pipeline\n",
    "# - Legacy models (large-v2/v3/turbo) work with ALL pipelines (faster, fast, balanced, fidelity, transformers)\n",
    "KOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\n",
    "LEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n",
    "\n",
    "# Auto-correct incompatible model-pipeline combinations\n",
    "warnings_list = []\n",
    "\n",
    "# Check Pass 1 compatibility\n",
    "if model in KOTOBA_MODELS and quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 1: {model} requires 'transformers' pipeline. Auto-correcting from '{quality}' to 'transformers'.\")\n",
    "    quality = \"transformers\"\n",
    "\n",
    "# Check Pass 2 compatibility (only relevant if two-step is enabled)\n",
    "if use_two_step and secondpass_model in KOTOBA_MODELS and secondpass_quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 2: {secondpass_model} requires 'transformers' pipeline. Auto-correcting from '{secondpass_quality}' to 'transformers'.\")\n",
    "    secondpass_quality = \"transformers\"\n",
    "\n",
    "WHISPERJAV_CONFIG = {\n",
    "    'use_two_step': use_two_step,\n",
    "    'pass1_pipeline': quality,\n",
    "    'pass1_sensitivity': speech_detection,\n",
    "    'pass1_speech_segmenter': segmenter_map[speech_segmenter],\n",
    "    'pass1_model': model_map[model],\n",
    "    'pass2_pipeline': secondpass_quality,\n",
    "    'pass2_sensitivity': secondpass_sensitivity,\n",
    "    'pass2_speech_segmenter': segmenter_map[secondpass_speech_segmenter],\n",
    "    'pass2_model': model_map[secondpass_model],\n",
    "    'merge_strategy': combine_map[merge_method],\n",
    "    'folder_name': folder_name,\n",
    "    'subtitle_language': language_map[subtitle_language],\n",
    "    'translation_service': translation_service,\n",
    "    'api_key': api_key,\n",
    "    'translation_style': tone_map[translation_style],\n",
    "    'opening_credit': opening_credit,\n",
    "    'closing_credit': closing_credit,\n",
    "    'auto_disconnect': auto_disconnect,\n",
    "    # Display values (for status messages)\n",
    "    '_quality': quality,\n",
    "    '_speech_detection': speech_detection,\n",
    "    '_speech_segmenter': speech_segmenter,\n",
    "    '_model': model,\n",
    "    '_secondpass_quality': secondpass_quality,\n",
    "    '_secondpass_sensitivity': secondpass_sensitivity,\n",
    "    '_secondpass_speech_segmenter': secondpass_speech_segmenter,\n",
    "    '_secondpass_model': secondpass_model,\n",
    "    '_merge_method': merge_method,\n",
    "    '_subtitle_language': subtitle_language,\n",
    "    '_translation_style': translation_style,\n",
    "}\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display any auto-correction warnings\n",
    "for warning in warnings_list:\n",
    "    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>⚠ Auto-corrected:</b> {warning}</div>'))\n",
    "\n",
    "# Build status display\n",
    "if use_two_step:\n",
    "    mode_text = \"Two-Step\"\n",
    "    p1_info = f\"{quality}\"\n",
    "    if speech_segmenter != \"automatic\":\n",
    "        p1_info += f\"/{speech_segmenter}\"\n",
    "    if model != \"automatic\":\n",
    "        p1_info += f\"/{model}\"\n",
    "    p2_info = f\"{secondpass_quality}\"\n",
    "    if secondpass_speech_segmenter != \"automatic\":\n",
    "        p2_info += f\"/{secondpass_speech_segmenter}\"\n",
    "    if secondpass_model != \"automatic\":\n",
    "        p2_info += f\"/{secondpass_model}\"\n",
    "    details = f\"{p1_info} → {p2_info}\"\n",
    "else:\n",
    "    mode_text = \"Standard\"\n",
    "    details = f\"{quality}/{speech_detection}\"\n",
    "    if speech_segmenter != \"automatic\":\n",
    "        details += f\"/{speech_segmenter}\"\n",
    "    if model != \"automatic\":\n",
    "        details += f\"/{model}\"\n",
    "\n",
    "display(HTML(f'<div style=\"padding:6px 10px;background:#f0f9ff;border-radius:4px;font-size:10px\"><b>Settings:</b> {mode_text} ({details}) | Folder: {folder_name} | Output: {subtitle_language}</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transcribe"
   },
   "outputs": [],
   "source": "#@title Step 2: Transcribe { display-mode: \"form\" }\n#@markdown Connect Drive → Install → Transcribe all media files → Add credits\n\nimport os, sys, subprocess, shlex, time\nfrom pathlib import Path\nfrom IPython.display import display, HTML, clear_output\n\ndef status(msg, ok=True):\n    icon = \"✓\" if ok else \"✗\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n\n# Check config\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\ncfg = WHISPERJAV_CONFIG\n\n\n# ═══════════════════════════════════════════\n# LLAMA-CPP-PYTHON CUDA INSTALLER (INLINE)\n# ═══════════════════════════════════════════\n\ndef install_llama_cpp_cuda():\n    \"\"\"Install llama-cpp-python with CUDA support. Tries prebuilt wheel first, then source.\"\"\"\n    import json\n    import tempfile\n    import urllib.request\n    import urllib.error\n    import multiprocessing\n\n    # --- Detect CUDA version ---\n    def detect_cuda_version():\n        try:\n            import torch\n            if torch.cuda.is_available():\n                parts = torch.version.cuda.split(\".\")\n                if len(parts) >= 2:\n                    return f\"cu{parts[0]}{parts[1]}\"\n        except: pass\n        try:\n            r = subprocess.run([\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n                            capture_output=True, text=True, timeout=10)\n            if r.returncode == 0:\n                major = int(r.stdout.strip().split(\".\")[0])\n                if major >= 570:\n                    return \"cu128\"\n                elif major >= 560:\n                    return \"cu126\"\n                elif major >= 550:\n                    return \"cu124\"\n                elif major >= 520:\n                    return \"cu118\"\n        except: pass\n        return None\n\n    # --- Detect GPU architecture ---\n    def get_cuda_arch():\n        try:\n            r = subprocess.run([\"nvidia-smi\", \"--query-gpu=compute_cap\", \"--format=csv,noheader\"],\n                            capture_output=True, text=True, timeout=10)\n            if r.returncode == 0:\n                cap = r.stdout.strip().split('\\n')[0].strip()\n                if '.' in cap:\n                    return cap.replace('.', '')\n        except: pass\n        try:\n            r = subprocess.run([\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"],\n                            capture_output=True, text=True, timeout=10)\n            if r.returncode == 0:\n                gpu = r.stdout.strip().lower()\n                if \"t4\" in gpu: return \"75\"\n                elif any(x in gpu for x in [\"a100\", \"a10\", \"rtx 30\"]): return \"86\"\n                elif \"v100\" in gpu: return \"70\"\n                elif any(x in gpu for x in [\"rtx 40\", \"l40\"]): return \"89\"\n        except: pass\n        return None\n\n    # --- Try prebuilt wheel first ---\n    def try_prebuilt_wheel():\n        cuda_ver = detect_cuda_version()\n        if not cuda_ver:\n            return False, \"No CUDA detected\"\n\n        py_ver = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n        target_cudas = []\n        if cuda_ver and sys.platform in (\"win32\", \"linux\"):\n            if cuda_ver >= \"cu128\":\n                target_cudas = [\"cu128\", \"cu126\", \"cu124\", \"cu118\"]\n            elif cuda_ver >= \"cu126\":\n                target_cudas = [\"cu126\", \"cu124\", \"cu118\"]\n            elif cuda_ver >= \"cu124\":\n                target_cudas = [\"cu124\", \"cu118\"]\n            elif cuda_ver >= \"cu118\":\n                target_cudas = [\"cu118\"]\n\n        if not target_cudas:\n            return False, \"Unsupported CUDA version\"\n\n        try:\n            api_url = \"https://api.github.com/repos/JamePeng/llama-cpp-python/releases?per_page=50\"\n            req = urllib.request.Request(api_url, headers={\"Accept\": \"application/vnd.github.v3+json\"})\n            with urllib.request.urlopen(req, timeout=15) as response:\n                releases = json.loads(response.read().decode())\n        except Exception as e:\n            return False, f\"GitHub API failed: {e}\"\n\n        # Search for matching wheel\n        for cuda_tag in target_cudas:\n            for release in releases:\n                tag = release.get(\"tag_name\", \"\")\n                if f\"-{cuda_tag}-\" not in tag or \"-linux-\" not in tag:\n                    continue\n                for asset in release.get(\"assets\", []):\n                    name = asset.get(\"name\", \"\")\n                    if name.endswith(\".whl\") and py_ver in name and \"linux_x86_64\" in name:\n                        wheel_url = asset.get(\"browser_download_url\")\n                        # Download and install\n                        print(f\"  Found prebuilt wheel: {cuda_tag}\")\n                        temp_dir = Path(tempfile.gettempdir()) / \"whisperjav_wheels\"\n                        temp_dir.mkdir(exist_ok=True)\n                        dest = temp_dir / name\n                        if not dest.exists():\n                            print(f\"  Downloading...\")\n                            urllib.request.urlretrieve(wheel_url, dest)\n                        print(f\"  Installing...\")\n                        r = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", str(dest)],\n                                        capture_output=True, text=True, timeout=300)\n                        if r.returncode == 0:\n                            # Install server extras\n                            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"llama-cpp-python[server]\"],\n                                        capture_output=True, timeout=60)\n                            return True, f\"CUDA ({cuda_tag} prebuilt)\"\n                        return False, f\"Wheel install failed: {r.stderr[:200]}\"\n\n        return False, \"No matching wheel found\"\n\n    # --- Build from source fallback ---\n    def build_from_source():\n        cuda_arch = get_cuda_arch()\n        if not cuda_arch:\n            return False, \"No CUDA GPU detected\"\n\n        parallel = max(2, min(16, int(multiprocessing.cpu_count() * 0.75)))\n        os.environ[\"CMAKE_BUILD_PARALLEL_LEVEL\"] = str(parallel)\n        os.environ[\"CMAKE_ARGS\"] = f\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES={cuda_arch}\"\n\n        print(f\"  Building from source (sm_{cuda_arch}, {parallel} jobs)...\")\n        print(f\"  This may take ~10 minutes...\")\n\n        r = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--force-reinstall\", \"--no-cache-dir\",\n            \"llama-cpp-python[server] @ git+https://github.com/JamePeng/llama-cpp-python.git\"],\n            capture_output=True, text=True, timeout=900\n        )\n\n        if r.returncode == 0:\n            return True, f\"CUDA (sm_{cuda_arch} source build)\"\n        return False, r.stderr[-300:] if r.stderr else \"Unknown error\"\n\n    # --- Main flow ---\n    print(\"  Trying prebuilt wheel...\")\n    success, info = try_prebuilt_wheel()\n    if success:\n        return True, info\n\n    print(f\"  Prebuilt not available ({info}), building from source...\")\n    return build_from_source()\n\n\n# ═══════════════════════════════════════════\n# CONNECT GOOGLE DRIVE\n# ═══════════════════════════════════════════\nsection(\"CONNECTING GOOGLE DRIVE\")\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n    folder_path.mkdir(parents=True, exist_ok=True)\n    status(f\"Connected: {folder_path}\")\nexcept Exception as e:\n    status(f\"Failed to connect: {e}\", False)\n    raise SystemExit()\n\n# ═══════════════════════════════════════════\n# CHECK GPU\n# ═══════════════════════════════════════════\nsection(\"CHECKING GPU\")\ngpu_check = subprocess.run(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\", shell=True, capture_output=True, text=True)\nif gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n    status(\"No GPU detected. Go to Runtime → Change runtime type → T4 GPU\", False)\n    raise SystemExit()\nstatus(f\"GPU: {gpu_check.stdout.strip()}\")\n\n# ═══════════════════════════════════════════\n# INSTALL WHISPERJAV\n# ═══════════════════════════════════════════\nsection(\"INSTALLING (2-3 min)\")\ninstall_start = time.time()\n\nsteps = [\n    (\"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev libc++1 libc++abi1 > /dev/null 2>&1\", \"System tools\"),\n    (\"pip install -q tqdm numba tiktoken ffmpeg-python soundfile auditok numpy scipy pysrt srt aiofiles jsonschema Pillow colorama librosa matplotlib pyloudnorm requests faster-whisper transformers optimum accelerate huggingface-hub pydantic ten-vad silero-vad pydub regex modelscope addict\", \"Python packages\"),\n    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n]\n\nfor cmd, name in steps:\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        status(f\"{name} failed\", False)\n        raise SystemExit()\n    status(name)\n\n# Install CUDA llama-cpp-python BEFORE WhisperJAV (so pip doesn't reinstall CPU version)\nif cfg.get('translation_service') == 'local':\n    print(\"... installing llama-cpp-python (CUDA)\")\n    success, info = install_llama_cpp_cuda()\n    if success:\n        status(f\"llama-cpp-python ({info})\")\n    else:\n        status(f\"llama-cpp-python (CUDA failed)\", False)\n\n# Install WhisperJAV last (will skip llama-cpp-python if already installed)\nresult = subprocess.run(\"pip install -q git+https://github.com/meizhong986/WhisperJAV.git@main\",\n                        shell=True, capture_output=True, text=True)\nif result.returncode != 0:\n    status(\"WhisperJAV failed\", False)\n    raise SystemExit()\nstatus(\"WhisperJAV\")\n\nstatus(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n\n# ═══════════════════════════════════════════\n# FIND MEDIA FILES\n# ═══════════════════════════════════════════\nsection(\"SCANNING FILES\")\nvideo_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\nvideos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n\nif not videos:\n    status(f\"No media files in {cfg['folder_name']}/\", False)\n    raise SystemExit()\n\nstatus(f\"Found {len(videos)} file(s)\")\nfor v in videos[:5]:\n    print(f\"  • {v.name}\")\nif len(videos) > 5:\n    print(f\"  ... and {len(videos)-5} more\")\n\n# Record existing SRT files before transcription\nexisting_srts = set(folder_path.glob('*.srt'))\n\n# ═══════════════════════════════════════════\n# TRANSCRIBE\n# ═══════════════════════════════════════════\nsection(\"TRANSCRIBING\")\n\n# Build command - always transcribe to native (Japanese) first\n# For \"direct-to-english\", use Whisper's built-in translation\n# For \"llm\", transcribe to Japanese and translate in Step 3\ncmd = ['whisperjav', str(folder_path), '--output-dir', str(folder_path)]\n\nif cfg['use_two_step']:\n    # Two-step ensemble mode\n    cmd.extend(['--ensemble',\n        '--pass1-pipeline', cfg['pass1_pipeline'],\n        '--pass1-sensitivity', cfg['pass1_sensitivity'],\n        '--pass2-pipeline', cfg['pass2_pipeline'],\n        '--pass2-sensitivity', cfg['pass2_sensitivity'],\n        '--merge-strategy', cfg['merge_strategy']])\n\n    # Add speech segmenters if not automatic\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--pass1-speech-segmenter', cfg['pass1_speech_segmenter']])\n    if cfg['pass2_speech_segmenter']:\n        cmd.extend(['--pass2-speech-segmenter', cfg['pass2_speech_segmenter']])\n\n    # Add models if not automatic\n    if cfg['pass1_model']:\n        cmd.extend(['--pass1-model', cfg['pass1_model']])\n    if cfg['pass2_model']:\n        cmd.extend(['--pass2-model', cfg['pass2_model']])\n\n    # Display mode info\n    p1_info = cfg['_quality']\n    if cfg['_speech_segmenter'] != 'automatic':\n        p1_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        p1_info += f\" ({cfg['_model']})\"\n\n    p2_info = cfg['_secondpass_quality']\n    if cfg['_secondpass_speech_segmenter'] != 'automatic':\n        p2_info += f\" + {cfg['_secondpass_speech_segmenter']}\"\n    if cfg['_secondpass_model'] != 'automatic':\n        p2_info += f\" ({cfg['_secondpass_model']})\"\n\n    print(f\"Mode: Two-Step\")\n    print(f\"  Pass 1: {p1_info}\")\n    print(f\"  Pass 2: {p2_info}\")\n    print(f\"  Merge: {cfg['_merge_method']}\")\nelse:\n    # Single-pass mode\n    cmd.extend(['--mode', cfg['pass1_pipeline'], '--sensitivity', cfg['pass1_sensitivity']])\n\n    # Add speech segmenter if not automatic\n    if cfg['pass1_speech_segmenter']:\n        cmd.extend(['--speech-segmenter', cfg['pass1_speech_segmenter']])\n\n    # Add model if not automatic (for single-pass, use --model flag if available)\n    # Note: --model flag may not be available for all pipelines\n    # The pipeline will use its default model if not specified\n\n    mode_info = f\"{cfg['_quality']}/{cfg['_speech_detection']}\"\n    if cfg['_speech_segmenter'] != 'automatic':\n        mode_info += f\" + {cfg['_speech_segmenter']}\"\n    if cfg['_model'] != 'automatic':\n        mode_info += f\" ({cfg['_model']})\"\n    print(f\"Mode: Standard ({mode_info})\")\n\n# Set subtitle language for transcription\nif cfg['subtitle_language'] == 'direct-to-english':\n    cmd.extend(['--subs-language', 'direct-to-english'])\n    print(f\"Output: English (Whisper auto-translate)\")\nelse:\n    # For both 'native' and 'llm', transcribe to Japanese first\n    cmd.extend(['--subs-language', 'native'])\n    if cfg['subtitle_language'] == 'llm':\n        print(f\"Output: Japanese (AI translation will follow in Step 3)\")\n    else:\n        print(f\"Output: Japanese\")\n\nprint(f\"Input: {folder_path}\\n\")\n\nfull_cmd = shlex.join(cmd)\nprocess = subprocess.Popen(full_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, universal_newlines=True)\nfor line in process.stdout:\n    print(line, end='')\nprocess.wait()\n\nif process.returncode != 0:\n    status(\"Transcription failed\", False)\n    raise SystemExit()\n\n# ═══════════════════════════════════════════\n# IDENTIFY NEW SRT FILES\n# ═══════════════════════════════════════════\nall_srts = set(folder_path.glob('*.srt'))\nnew_srts = list(all_srts - existing_srts)\nnew_srts.sort(key=lambda x: x.name)\n\n# Store for Step 3\nWHISPERJAV_NEW_SRTS = new_srts\nWHISPERJAV_FOLDER_PATH = folder_path\n\nstatus(f\"Created {len(new_srts)} new subtitle file(s)\")\n\n# ═══════════════════════════════════════════\n# ADD CREDITS\n# ═══════════════════════════════════════════\nsection(\"ADDING CREDITS\")\n\nif cfg['opening_credit'] or cfg['closing_credit']:\n    credits_count = 0\n    for srt_file in new_srts:\n        try:\n            content = srt_file.read_text(encoding='utf-8')\n            if cfg['opening_credit']:\n                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n            if cfg['closing_credit']:\n                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n            srt_file.write_text(content, encoding='utf-8')\n            credits_count += 1\n        except Exception as e:\n            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n    status(f\"Credits added to {credits_count} file(s)\")\nelse:\n    status(\"No credits configured\")\n\n# ═══════════════════════════════════════════\n# TRANSCRIPTION COMPLETE\n# ═══════════════════════════════════════════\nsection(\"TRANSCRIPTION COMPLETE\")\nis_local = cfg['translation_service'] == 'local'\nif cfg['subtitle_language'] == 'llm' and (cfg['api_key'] or is_local):\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>✓ Transcription done!</b> {len(new_srts)} file(s). AI Translation will start next...</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ Done!</b> {len(new_srts)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n    if cfg['subtitle_language'] == 'llm' and not cfg['api_key'] and not is_local:\n        print(\"Note: AI translation skipped (no API key provided)\")\n\n    # Auto-disconnect if no AI translation needed\n    if cfg['auto_disconnect']:\n        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n        time.sleep(10)\n        try:\n            from google.colab import runtime\n            runtime.unassign()\n        except: pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "translate"
   },
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation (if selected) { display-mode: \"form\" }\n",
    "#@markdown Translate each subtitle file using AI (only runs if \"English (AI translate)\" selected)\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"✓\" if ok else \"✗\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'─'*40}\\n{title}\\n{'─'*40}\")\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK PREREQUISITES\n",
    "# ═══════════════════════════════════════════\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "folder_path = WHISPERJAV_FOLDER_PATH\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# CHECK IF AI TRANSLATION IS NEEDED\n",
    "# ═══════════════════════════════════════════\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>ℹ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# Check API key (not needed for local provider)\n",
    "is_local = cfg['translation_service'] == 'local'\n",
    "if not is_local and not cfg['api_key']:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for AI translation</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# SET UP API KEY (for cloud providers)\n",
    "# ═══════════════════════════════════════════\n",
    "if not is_local:\n",
    "    env_map = {\n",
    "        \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "        \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "        \"gemini\": \"GEMINI_API_KEY\",\n",
    "        \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "        \"gpt\": \"OPENAI_API_KEY\"\n",
    "    }\n",
    "    os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSLATE EACH SRT FILE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"AI TRANSLATION\")\n",
    "if is_local:\n",
    "    print(f\"Provider: local (Colab GPU)\")\n",
    "else:\n",
    "    print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "    # Build whisperjav-translate command\n",
    "    translate_cmd = [\n",
    "        'whisperjav-translate',\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream'\n",
    "    ]\n",
    "\n",
    "    # Add --expose-server for local provider (required for Colab)\n",
    "    if is_local:\n",
    "        translate_cmd.append('--expose-server')\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture stdout (output path) and stderr (progress)\n",
    "        stdout_lines = []\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "    print()  # Blank line between files\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TRANSLATION COMPLETE\n",
    "# ═══════════════════════════════════════════\n",
    "section(\"COMPLETE\")\n",
    "\n",
    "total_srts = len(new_srts) + len(translated_files)\n",
    "\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>⚠ Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>✓ All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# AUTO-DISCONNECT\n",
    "# ═══════════════════════════════════════════\n",
    "if cfg['auto_disconnect']:\n",
    "    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "    except: pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}