{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-0",
   "source": "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_parallel_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# WhisperJAV Two-Pass Edition v1.7.4 (Expert)\n\n**Adaptive Two-Pass Processing** - Automatically optimizes for your platform.\n\n| Platform | GPUs | How it works |\n|----------|------|---------------|\n| **Kaggle** | 2x T4 (8GB each) | **Parallel** - Pass 1 on GPU 0, Pass 2 on GPU 1 simultaneously |\n| **Colab L4/A100** | 1x GPU (16-24GB) | **Sequential** - Pass 1 first, then Pass 2 (avoids memory issues) |\n\n| Option | What it controls |\n|--------|------------------|\n| **Scene Detection** | How to split audio into chunks (auditok, silero, semantic) |\n| **Speech Segmenter** | How to detect speech in audio (silero, ten) |\n| **Speech Enhancer** | Audio cleanup for noisy sources (ffmpeg-dsp, clearvoice, etc.) |\n| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n\n---\n<div style=\"font-size: 8px; line-height: 1.0;\">\n1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n2. Run <b>Step 1: Settings</b> (required)<br>\n3. Run <b>Step 1.5: Expert Options</b> (optional - skip if unsure)<br>\n4. Run <b>Step 2: Two-Pass Transcribe</b> and wait for completion<br>\n5. Run <b>Step 3: AI Translation</b> (if selected)\n</div>\n\n<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-1",
   "outputs": [],
   "source": "#@title Step 1: Settings { display-mode: \"form\" }\n\n#@markdown **Pass 1 Configuration**\npass1_quality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\npass1_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\npass1_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\npass1_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n\n#@markdown ---\n#@markdown **Pass 2 Configuration**\npass2_quality = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\npass2_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\npass2_speech_segmenter = \"ten\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\npass2_model = \"kotoba-bilingual\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n\n#@markdown ---\n#@markdown **Merge Strategy**\nmerge_method = \"prefer first pass\" #@param [\"automatic\", \"keep all\", \"prefer first pass\", \"prefer second pass\"]\n\n#@markdown ---\n#@markdown **Files & Output**\nfolder_name = \"WhisperJAV\" #@param {type:\"string\"}\nsubtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n\n#@markdown ---\n#@markdown **AI Translation** *(if selected \"English (AI translate)\")*\ntranslation_service = \"deepseek\" #@param [\"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\napi_key = \"\" #@param {type:\"string\"}\ntranslation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n\n#@markdown ---\n#@markdown **Credits**\nopening_credit = \"\" #@param {type:\"string\"}\nclosing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown **Session**\nauto_disconnect = True #@param {type:\"boolean\"}\n#@markdown ‚òùÔ∏è Auto-disconnect when done (saves GPU credits)\n\n# Mapping dictionaries\ncombine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n               \"prefer first pass\": \"pass1_primary\", \"prefer second pass\": \"pass2_primary\"}\nlanguage_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n                \"English (AI translate)\": \"llm\"}\ntone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n\n# Speech segmenter mapping (None = use pipeline default)\nsegmenter_map = {\"automatic\": None, \"silero\": \"silero\", \"ten\": \"ten\", \"none\": \"none\"}\n\n# Model mapping (None = use pipeline default)\nmodel_map = {\n    \"automatic\": None,\n    \"large-v2\": \"large-v2\",\n    \"large-v3\": \"large-v3\",\n    \"turbo\": \"large-v3-turbo\",\n    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n}\n\n# Define model compatibility:\n# - Kotoba models (HuggingFace) ONLY work with \"transformers\" pipeline\n# - Legacy models (large-v2/v3/turbo) work with ALL pipelines (faster, fast, balanced, fidelity, transformers)\nKOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\nLEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n\n# Auto-correct incompatible model-pipeline combinations\nwarnings_list = []\n\n# Check Pass 1 compatibility\nif pass1_model in KOTOBA_MODELS and pass1_quality in LEGACY_PIPELINES:\n    warnings_list.append(f\"Pass 1: {pass1_model} requires 'transformers' pipeline. Auto-correcting from '{pass1_quality}' to 'transformers'.\")\n    pass1_quality = \"transformers\"\n\n# Check Pass 2 compatibility\nif pass2_model in KOTOBA_MODELS and pass2_quality in LEGACY_PIPELINES:\n    warnings_list.append(f\"Pass 2: {pass2_model} requires 'transformers' pipeline. Auto-correcting from '{pass2_quality}' to 'transformers'.\")\n    pass2_quality = \"transformers\"\n\nWHISPERJAV_CONFIG = {\n    'pass1_pipeline': pass1_quality,\n    'pass1_sensitivity': pass1_sensitivity,\n    'pass1_speech_segmenter': segmenter_map[pass1_speech_segmenter],\n    'pass1_model': model_map[pass1_model],\n    'pass2_pipeline': pass2_quality,\n    'pass2_sensitivity': pass2_sensitivity,\n    'pass2_speech_segmenter': segmenter_map[pass2_speech_segmenter],\n    'pass2_model': model_map[pass2_model],\n    'merge_strategy': combine_map[merge_method],\n    'folder_name': folder_name,\n    'subtitle_language': language_map[subtitle_language],\n    'translation_service': translation_service,\n    'api_key': api_key,\n    'translation_style': tone_map[translation_style],\n    'opening_credit': opening_credit,\n    'closing_credit': closing_credit,\n    'auto_disconnect': auto_disconnect,\n    # Display values\n    '_pass1_quality': pass1_quality,\n    '_pass1_sensitivity': pass1_sensitivity,\n    '_pass1_speech_segmenter': pass1_speech_segmenter,\n    '_pass1_model': pass1_model,\n    '_pass2_quality': pass2_quality,\n    '_pass2_sensitivity': pass2_sensitivity,\n    '_pass2_speech_segmenter': pass2_speech_segmenter,\n    '_pass2_model': pass2_model,\n    '_merge_method': merge_method,\n    '_subtitle_language': subtitle_language,\n    '_translation_style': translation_style,\n}\n\nfrom IPython.display import display, HTML\n\n# Display any auto-correction warnings\nfor warning in warnings_list:\n    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>‚ö† Auto-corrected:</b> {warning}</div>'))\n\n# Build status display\np1_info = f\"{pass1_quality}\"\nif pass1_speech_segmenter != \"automatic\":\n    p1_info += f\"/{pass1_speech_segmenter}\"\nif pass1_model != \"automatic\":\n    p1_info += f\"/{pass1_model}\"\n\np2_info = f\"{pass2_quality}\"\nif pass2_speech_segmenter != \"automatic\":\n    p2_info += f\"/{pass2_speech_segmenter}\"\nif pass2_model != \"automatic\":\n    p2_info += f\"/{pass2_model}\"\n\ndisplay(HTML(f'<div style=\"padding:6px 10px;background:#e0f2fe;border-radius:4px;font-size:10px\"><b>Parallel Mode:</b> Pass1({p1_info}) ‚áÜ Pass2({p2_info}) | Merge: {merge_method} | Folder: {folder_name}</div>'))"
  },
  {
   "cell_type": "code",
   "id": "r0rbtdrto8f",
   "source": "#@title Step 1.5: Expert Options (Optional) { display-mode: \"form\" }\n#@markdown <font color=\"gray\">*Skip this cell if unsure. Default settings work well for most videos.*</font>\n#@markdown\n#@markdown <font color=\"orange\">‚ö†Ô∏è **Memory Notice:** Speech enhancement uses additional GPU memory. If you encounter OOM errors, use `none` or `ffmpeg-dsp`.</font>\n\n#@markdown ---\n#@markdown ## Pass 1 Settings\n\n#@markdown **Scene Detection** *(how to split audio into chunks)*\npass1_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\", \"none\"]\n#@markdown <font size=\"1\">auditok=energy-based (fast), silero=VAD-based, semantic=texture clustering (best for complex audio)</font>\n\n#@markdown **Speech Segmenter** *(how to detect speech within chunks)*\npass1_speech_segmenter_expert = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n\n#@markdown **Speech Enhancer** *(audio cleanup for noisy sources)*\npass1_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n#@markdown <font size=\"1\">none=skip, ffmpeg-dsp=filters (no GPU), clearvoice=denoise (48kHz), zipenhancer=lightweight, bs-roformer=vocal isolation</font>\n\n#@markdown **FFmpeg DSP Filters** *(only applies when ffmpeg-dsp selected above)*\npass1_ffmpeg_amplify = True #@param {type:\"boolean\"}\n#@markdown ‚Ü≥ Amplify quiet audio (recommended)\npass1_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n#@markdown ‚Ü≥ Loudness normalization\npass1_ffmpeg_compress = False #@param {type:\"boolean\"}\n#@markdown ‚Ü≥ Dynamic range compression\npass1_ffmpeg_highpass = False #@param {type:\"boolean\"}\n#@markdown ‚Ü≥ Remove low rumble (<80Hz)\n\n#@markdown ---\n#@markdown ## Pass 2 Settings\n\n#@markdown **Scene Detection**\npass2_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\", \"none\"]\n\n#@markdown **Speech Segmenter**\npass2_speech_segmenter_expert = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n\n#@markdown **Speech Enhancer**\npass2_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n\n#@markdown **FFmpeg DSP Filters** *(only applies when ffmpeg-dsp selected above)*\npass2_ffmpeg_amplify = True #@param {type:\"boolean\"}\n#@markdown ‚Ü≥ Amplify quiet audio (recommended)\npass2_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n#@markdown ‚Ü≥ Loudness normalization\npass2_ffmpeg_compress = False #@param {type:\"boolean\"}\n#@markdown ‚Ü≥ Dynamic range compression\npass2_ffmpeg_highpass = False #@param {type:\"boolean\"}\n#@markdown ‚Ü≥ Remove low rumble (<80Hz)\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# BUILD EXPERT CONFIG\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef build_ffmpeg_filters(amplify, loudnorm, compress, highpass):\n    \"\"\"Combine selected FFmpeg filters into comma-separated string.\"\"\"\n    filters = []\n    if amplify:\n        filters.append(\"amplify\")\n    if loudnorm:\n        filters.append(\"loudnorm\")\n    if compress:\n        filters.append(\"compress\")\n    if highpass:\n        filters.append(\"highpass\")\n    return \",\".join(filters) if filters else None\n\n# Map \"automatic\" to None (let CLI use pipeline defaults)\ndef map_value(val):\n    return None if val == \"automatic\" else val\n\nWHISPERJAV_EXPERT_CONFIG = {\n    # Pass 1\n    'pass1_scene_detector': map_value(pass1_scene_detector),\n    'pass1_speech_segmenter': map_value(pass1_speech_segmenter_expert),\n    'pass1_speech_enhancer': None if pass1_speech_enhancer == \"none\" else pass1_speech_enhancer,\n    'pass1_ffmpeg_filters': build_ffmpeg_filters(pass1_ffmpeg_amplify, pass1_ffmpeg_loudnorm, pass1_ffmpeg_compress, pass1_ffmpeg_highpass) if pass1_speech_enhancer == \"ffmpeg-dsp\" else None,\n    # Pass 2\n    'pass2_scene_detector': map_value(pass2_scene_detector),\n    'pass2_speech_segmenter': map_value(pass2_speech_segmenter_expert),\n    'pass2_speech_enhancer': None if pass2_speech_enhancer == \"none\" else pass2_speech_enhancer,\n    'pass2_ffmpeg_filters': build_ffmpeg_filters(pass2_ffmpeg_amplify, pass2_ffmpeg_loudnorm, pass2_ffmpeg_compress, pass2_ffmpeg_highpass) if pass2_speech_enhancer == \"ffmpeg-dsp\" else None,\n    # Display values\n    '_pass1_scene_detector': pass1_scene_detector,\n    '_pass1_speech_segmenter': pass1_speech_segmenter_expert,\n    '_pass1_speech_enhancer': pass1_speech_enhancer,\n    '_pass2_scene_detector': pass2_scene_detector,\n    '_pass2_speech_segmenter': pass2_speech_segmenter_expert,\n    '_pass2_speech_enhancer': pass2_speech_enhancer,\n}\n\nfrom IPython.display import display, HTML\n\n# Validation warnings\nwarnings = []\n\n# Check for memory-intensive combinations\nheavy_enhancers = {'clearvoice', 'bs-roformer', 'zipenhancer'}\nif pass1_speech_enhancer in heavy_enhancers and pass2_speech_enhancer in heavy_enhancers:\n    warnings.append(\"Using GPU-based enhancement on both passes may cause OOM on T4 GPU. Consider using 'none' or 'ffmpeg-dsp' for one pass.\")\n\n# Display warnings\nfor w in warnings:\n    display(HTML(f'<div style=\"padding:4px 8px;background:#fef9c3;border-radius:4px;font-size:9px;margin:2px 0\"><b>‚ö†Ô∏è</b> {w}</div>'))\n\n# Build summary\ndef summarize_pass(scene, seg, enh, ffmpeg_filters):\n    parts = []\n    if scene != \"automatic\":\n        parts.append(f\"scene:{scene}\")\n    if seg != \"automatic\":\n        parts.append(f\"seg:{seg}\")\n    if enh != \"none\":\n        if enh == \"ffmpeg-dsp\" and ffmpeg_filters:\n            parts.append(f\"enh:{enh}({ffmpeg_filters})\")\n        else:\n            parts.append(f\"enh:{enh}\")\n    return \", \".join(parts) if parts else \"defaults\"\n\np1_summary = summarize_pass(pass1_scene_detector, pass1_speech_segmenter_expert, pass1_speech_enhancer,\n                            WHISPERJAV_EXPERT_CONFIG['pass1_ffmpeg_filters'])\np2_summary = summarize_pass(pass2_scene_detector, pass2_speech_segmenter_expert, pass2_speech_enhancer,\n                            WHISPERJAV_EXPERT_CONFIG['pass2_ffmpeg_filters'])\n\ndisplay(HTML(f'<div style=\"padding:6px 10px;background:#f5f3ff;border-radius:4px;font-size:10px\"><b>Expert Settings:</b> Pass 1: {p1_summary} | Pass 2: {p2_summary}</div>'))\nprint(\"\\n‚úì Expert options configured. Run Step 2 to transcribe.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-2",
   "outputs": [],
   "source": "#@title Step 2: Two-Pass Transcribe { display-mode: \"form\" }\n#@markdown Connect Drive ‚Üí Install ‚Üí Run passes (parallel on Kaggle, sequential on Colab) ‚Üí Merge results\n\nimport os, sys, subprocess, shlex, time, re\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom IPython.display import display, HTML, clear_output\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Tuple, Optional\n\ndef status(msg, ok=True):\n    icon = \"‚úì\" if ok else \"‚úó\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'‚îÄ'*50}\\n{title}\\n{'‚îÄ'*50}\")\n\n# Check config\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\ncfg = WHISPERJAV_CONFIG\n\n# Check for expert config (optional - defaults if not run)\nexpert = WHISPERJAV_EXPERT_CONFIG if 'WHISPERJAV_EXPERT_CONFIG' in dir() else None\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# CONNECT GOOGLE DRIVE\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"CONNECTING GOOGLE DRIVE\")\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n    folder_path.mkdir(parents=True, exist_ok=True)\n    status(f\"Connected: {folder_path}\")\nexcept Exception as e:\n    status(f\"Failed to connect: {e}\", False)\n    raise SystemExit()\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# CHECK GPUs AND DETERMINE MODE\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"DETECTING PLATFORM\")\ngpu_check = subprocess.run(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\", shell=True, capture_output=True, text=True)\nif gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n    status(\"No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\", False)\n    raise SystemExit()\n\ngpu_lines = [line.strip() for line in gpu_check.stdout.strip().split('\\n') if line.strip()]\nnum_gpus = len(gpu_lines)\n\nfor i, gpu_info in enumerate(gpu_lines):\n    status(f\"GPU {i}: {gpu_info}\")\n\n# Adaptive mode selection\nif num_gpus >= 2:\n    PARALLEL_MODE = True\n    gpu_assignment = {1: \"0\", 2: \"1\"}\n    print(f\"\\n  ‚ö° Kaggle Mode: PARALLEL (Pass 1 ‚Üí GPU 0, Pass 2 ‚Üí GPU 1)\")\nelse:\n    PARALLEL_MODE = False\n    gpu_assignment = {1: \"0\", 2: \"0\"}\n    print(f\"\\n  üìù Colab Mode: SEQUENTIAL (avoids memory contention)\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# INSTALL WHISPERJAV\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"INSTALLING (2-3 min)\")\ninstall_start = time.time()\n\nsteps = [\n    (\"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev libc++1 libc++abi1 > /dev/null 2>&1\", \"System tools\"),\n    (\"pip install -q tqdm numba tiktoken ffmpeg-python soundfile auditok numpy scipy pysrt srt aiofiles jsonschema Pillow colorama librosa matplotlib pyloudnorm requests faster-whisper transformers optimum accelerate huggingface-hub pydantic ten-vad silero-vad pydub regex modelscope addict\", \"Python packages\"),\n    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n    (\"pip install -q git+https://github.com/meizhong986/WhisperJAV.git@main\", \"WhisperJAV\")\n]\n\nfor cmd, name in steps:\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        status(f\"{name} failed\", False)\n        raise SystemExit()\n    status(name)\n\n# Conditional installation of speech enhancer dependencies\nif expert:\n    extra_packages = set()\n    for enhancer in [expert.get('pass1_speech_enhancer'), expert.get('pass2_speech_enhancer')]:\n        if enhancer == 'clearvoice':\n            extra_packages.add('clearvoice')\n        elif enhancer == 'zipenhancer':\n            # zipenhancer uses modelscope which is already installed above\n            pass\n        elif enhancer == 'bs-roformer':\n            extra_packages.add('bs-roformer-infer')\n    \n    if extra_packages:\n        pkg_list = ' '.join(extra_packages)\n        result = subprocess.run(f\"pip install -q {pkg_list}\", shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            status(f\"Speech enhancer packages failed (continuing anyway)\", False)\n        else:\n            status(f\"Speech enhancer packages ({', '.join(extra_packages)})\")\n\nstatus(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# FIND MEDIA FILES\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"SCANNING FILES\")\nvideo_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\nvideos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n\nif not videos:\n    status(f\"No media files in {cfg['folder_name']}/\", False)\n    raise SystemExit()\n\nstatus(f\"Found {len(videos)} file(s)\")\nfor v in videos[:5]:\n    print(f\"  ‚Ä¢ {v.name}\")\nif len(videos) > 5:\n    print(f\"  ... and {len(videos)-5} more\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# MERGE FUNCTIONS (from whisperjav/ensemble/merge.py)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n@dataclass\nclass Subtitle:\n    index: int\n    start_time: float\n    end_time: float\n    text: str\n\n    @property\n    def duration(self) -> float:\n        return self.end_time - self.start_time\n\ndef parse_srt(path: Path) -> List[Subtitle]:\n    if not path.exists():\n        return []\n    subtitles = []\n    content = path.read_text(encoding='utf-8')\n    blocks = re.split(r'\\n\\s*\\n', content.strip())\n    for block in blocks:\n        if not block.strip():\n            continue\n        lines = block.strip().split('\\n')\n        if len(lines) < 3:\n            continue\n        try:\n            index = int(lines[0].strip())\n            ts_match = re.match(r'(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})\\s*-->\\s*(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})', lines[1].strip())\n            if not ts_match:\n                continue\n            g = ts_match.groups()\n            start = int(g[0])*3600 + int(g[1])*60 + int(g[2]) + int(g[3])/1000\n            end = int(g[4])*3600 + int(g[5])*60 + int(g[6]) + int(g[7])/1000\n            text = '\\n'.join(lines[2:]).strip()\n            subtitles.append(Subtitle(index, start, end, text))\n        except:\n            continue\n    return subtitles\n\ndef write_srt(subtitles: List[Subtitle], path: Path):\n    def ts(seconds):\n        h, m = int(seconds // 3600), int((seconds % 3600) // 60)\n        s, ms = int(seconds % 60), int((seconds % 1) * 1000)\n        return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n    lines = []\n    for i, sub in enumerate(subtitles, 1):\n        lines.extend([str(i), f\"{ts(sub.start_time)} --> {ts(sub.end_time)}\", sub.text, ''])\n    path.write_text('\\n'.join(lines), encoding='utf-8')\n\ndef merge_srt(srt1: Path, srt2: Path, output: Path, strategy: str) -> Dict[str, Any]:\n    subs1, subs2 = parse_srt(srt1), parse_srt(srt2)\n    \n    if strategy == 'full_merge':\n        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs1 + subs2]\n    elif strategy == 'pass1_primary':\n        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs1]\n        for s2 in subs2:\n            if not any(max(s1.start_time, s2.start_time) < min(s1.end_time, s2.end_time) for s1 in subs1):\n                merged.append(Subtitle(0, s2.start_time, s2.end_time, s2.text))\n    elif strategy == 'pass2_primary':\n        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs2]\n        for s1 in subs1:\n            if not any(max(s1.start_time, s2.start_time) < min(s1.end_time, s2.end_time) for s2 in subs2):\n                merged.append(Subtitle(0, s1.start_time, s1.end_time, s1.text))\n    else:  # smart_merge\n        merged, used = [], set()\n        for s1 in subs1:\n            best_i, best_overlap = None, 0\n            for i, s2 in enumerate(subs2):\n                if i in used: continue\n                overlap = max(0, min(s1.end_time, s2.end_time) - max(s1.start_time, s2.start_time))\n                if overlap > best_overlap:\n                    best_overlap, best_i = overlap, i\n            if best_i is not None and best_overlap > 0.3 * min(s1.duration, subs2[best_i].duration):\n                used.add(best_i)\n                chosen = s1 if s1.duration <= subs2[best_i].duration else subs2[best_i]\n                merged.append(Subtitle(0, chosen.start_time, chosen.end_time, chosen.text))\n            else:\n                merged.append(Subtitle(0, s1.start_time, s1.end_time, s1.text))\n        for i, s2 in enumerate(subs2):\n            if i not in used:\n                merged.append(Subtitle(0, s2.start_time, s2.end_time, s2.text))\n    \n    merged.sort(key=lambda s: s.start_time)\n    write_srt(merged, output)\n    return {'pass1_count': len(subs1), 'pass2_count': len(subs2), 'merged_count': len(merged)}\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# TWO-PASS TRANSCRIPTION (ADAPTIVE)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"TWO-PASS TRANSCRIPTION\" + (\" (PARALLEL)\" if PARALLEL_MODE else \" (SEQUENTIAL)\"))\n\ndef build_pass_command(pass_num: int, video_path: Path, output_dir: Path, cfg: dict, expert: Optional[dict] = None) -> Tuple[List[str], Path]:\n    \"\"\"Build whisperjav command for a single pass.\n\n    Note: WhisperJAV doesn't have --output-name, so we use separate directories\n    for each pass to avoid conflicts when running in parallel.\n    Output naming is automatic: {basename}.{lang_code}.whisperjav.srt\n    \"\"\"\n    # Use separate directory for each pass to avoid conflicts\n    pass_output_dir = output_dir / f\"pass{pass_num}\"\n    pass_output_dir.mkdir(parents=True, exist_ok=True)\n\n    pipeline = cfg[f'pass{pass_num}_pipeline']\n    sensitivity = cfg[f'pass{pass_num}_sensitivity']\n    segmenter = cfg[f'pass{pass_num}_speech_segmenter']\n    model = cfg[f'pass{pass_num}_model']\n\n    cmd = ['whisperjav', str(video_path), '--output-dir', str(pass_output_dir),\n           '--mode', pipeline, '--sensitivity', sensitivity]\n\n    # Add speech segmenter from basic config\n    if segmenter:\n        cmd.extend(['--speech-segmenter', segmenter])\n\n    # Add model if specified\n    if model:\n        cmd.extend(['--model', model])\n\n    # Add expert options if provided\n    if expert:\n        # Scene detector\n        scene_detector = expert.get(f'pass{pass_num}_scene_detector')\n        if scene_detector:\n            cmd.extend(['--scene-detection-method', scene_detector])\n        \n        # Speech segmenter (expert overrides basic)\n        expert_segmenter = expert.get(f'pass{pass_num}_speech_segmenter')\n        if expert_segmenter:\n            # Remove existing --speech-segmenter if present\n            if '--speech-segmenter' in cmd:\n                idx = cmd.index('--speech-segmenter')\n                cmd.pop(idx)\n                cmd.pop(idx)\n            cmd.extend(['--speech-segmenter', expert_segmenter])\n        \n        # Speech enhancer\n        speech_enhancer = expert.get(f'pass{pass_num}_speech_enhancer')\n        if speech_enhancer:\n            # Note: CLI support for speech enhancer in single-pass mode\n            # may need verification - adding the argument anyway\n            pass  # Speech enhancer CLI flag TBD based on CLI support\n\n    # Set subtitle language\n    if cfg['subtitle_language'] == 'direct-to-english':\n        cmd.extend(['--subs-language', 'direct-to-english'])\n    else:\n        cmd.extend(['--subs-language', 'native'])\n\n    # Return the pass output directory - we'll find the SRT file after processing\n    return cmd, pass_output_dir\n\ndef find_output_srt(pass_output_dir: Path, video_name: str) -> Path:\n    \"\"\"Find the generated SRT file in the pass output directory.\n\n    WhisperJAV auto-generates: {basename}.{lang}.whisperjav.srt\n    e.g., video.ja.whisperjav.srt or video.en.whisperjav.srt\n    \"\"\"\n    base_name = Path(video_name).stem\n    # Look for any SRT file matching the video name\n    patterns = [\n        f\"{base_name}.*.whisperjav.srt\",  # Standard format\n        f\"{base_name}.srt\",                # Fallback\n        f\"{base_name}*.srt\",               # Any SRT with base name\n    ]\n    for pattern in patterns:\n        matches = list(pass_output_dir.glob(pattern))\n        if matches:\n            return matches[0]\n    # Last resort: any SRT in directory\n    all_srts = list(pass_output_dir.glob(\"*.srt\"))\n    return all_srts[0] if all_srts else None\n\ndef run_pass(pass_num: int, video: Path, output_dir: Path, cfg: dict, expert: Optional[dict], gpu_id: str) -> Dict:\n    \"\"\"Run a single pass on a specific GPU.\"\"\"\n    cmd, pass_output_dir = build_pass_command(pass_num, video, output_dir, cfg, expert)\n\n    env = os.environ.copy()\n    env['CUDA_VISIBLE_DEVICES'] = gpu_id\n\n    start_time = time.time()\n    result = subprocess.run(shlex.join(cmd), shell=True, capture_output=True, text=True, env=env)\n    elapsed = time.time() - start_time\n\n    # Find the output SRT file\n    actual_output = find_output_srt(pass_output_dir, video.name)\n\n    return {\n        'pass': pass_num,\n        'video': video.name,\n        'success': result.returncode == 0 and actual_output and actual_output.exists(),\n        'output': actual_output,\n        'output_dir': pass_output_dir,\n        'elapsed': elapsed,\n        'gpu': gpu_id,\n        'stderr': result.stderr[-500:] if result.stderr else ''  # Last 500 chars for debugging\n    }\n\n# Display mode info\np1_info = cfg['_pass1_quality']\nif cfg['_pass1_speech_segmenter'] != 'automatic':\n    p1_info += f\"/{cfg['_pass1_speech_segmenter']}\"\nif cfg['_pass1_model'] != 'automatic':\n    p1_info += f\"/{cfg['_pass1_model']}\"\nif expert:\n    if expert.get('_pass1_scene_detector') != 'automatic':\n        p1_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n    if expert.get('_pass1_speech_enhancer') != 'none':\n        p1_info += f\" [enh:{expert['_pass1_speech_enhancer']}]\"\n\np2_info = cfg['_pass2_quality']\nif cfg['_pass2_speech_segmenter'] != 'automatic':\n    p2_info += f\"/{cfg['_pass2_speech_segmenter']}\"\nif cfg['_pass2_model'] != 'automatic':\n    p2_info += f\"/{cfg['_pass2_model']}\"\nif expert:\n    if expert.get('_pass2_scene_detector') != 'automatic':\n        p2_info += f\" [scene:{expert['_pass2_scene_detector']}]\"\n    if expert.get('_pass2_speech_enhancer') != 'none':\n        p2_info += f\" [enh:{expert['_pass2_speech_enhancer']}]\"\n\nprint(f\"Pass 1: {p1_info}\")\nprint(f\"Pass 2: {p2_info}\")\nprint(f\"Merge: {cfg['_merge_method']}\\n\")\n\n# Process each video\nall_results = []\nmerged_outputs = []\n\nfor video_idx, video in enumerate(videos, 1):\n    print(f\"\\n[{video_idx}/{len(videos)}] Processing: {video.name}\")\n\n    results = {}\n\n    if PARALLEL_MODE:\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # KAGGLE: Run both passes in parallel on separate GPUs\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            futures = {\n                executor.submit(run_pass, 1, video, folder_path, cfg, expert, gpu_assignment[1]): 1,\n                executor.submit(run_pass, 2, video, folder_path, cfg, expert, gpu_assignment[2]): 2\n            }\n\n            for future in as_completed(futures):\n                pass_num = futures[future]\n                result = future.result()\n                results[pass_num] = result\n                status_icon = \"‚úì\" if result['success'] else \"‚úó\"\n                print(f\"    {status_icon} Pass {pass_num} (GPU {result['gpu']}): {result['elapsed']:.1f}s\")\n                if not result['success'] and result['stderr']:\n                    print(f\"        Error: {result['stderr'][:200]}\")\n    else:\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # COLAB: Run passes sequentially on same GPU\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        for pass_num in [1, 2]:\n            result = run_pass(pass_num, video, folder_path, cfg, expert, gpu_assignment[pass_num])\n            results[pass_num] = result\n            status_icon = \"‚úì\" if result['success'] else \"‚úó\"\n            print(f\"    {status_icon} Pass {pass_num}: {result['elapsed']:.1f}s\")\n            if not result['success'] and result['stderr']:\n                print(f\"        Error: {result['stderr'][:200]}\")\n\n    # Merge results if both passes succeeded\n    if results[1]['success'] and results[2]['success']:\n        merged_output = folder_path / f\"{video.stem}.merged.whisperjav.srt\"\n        stats = merge_srt(results[1]['output'], results[2]['output'], merged_output, cfg['merge_strategy'])\n        print(f\"    ‚úì Merged: {stats['pass1_count']} + {stats['pass2_count']} ‚Üí {stats['merged_count']} subtitles\")\n        merged_outputs.append(merged_output)\n    else:\n        # Use whichever pass succeeded\n        for p in [1, 2]:\n            if results[p]['success']:\n                # Copy to main folder with consistent naming\n                final_output = folder_path / f\"{video.stem}.whisperjav.srt\"\n                import shutil\n                shutil.copy2(results[p]['output'], final_output)\n                merged_outputs.append(final_output)\n                print(f\"    ‚ö† Using Pass {p} only (other pass failed)\")\n                break\n        else:\n            print(f\"    ‚úó Both passes failed!\")\n\n    all_results.append(results)\n\n# Store for Step 3\nWHISPERJAV_NEW_SRTS = merged_outputs\nWHISPERJAV_FOLDER_PATH = folder_path\n\nstatus(f\"\\nCreated {len(merged_outputs)} merged subtitle file(s)\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ADD CREDITS\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"ADDING CREDITS\")\n\nif cfg['opening_credit'] or cfg['closing_credit']:\n    credits_count = 0\n    for srt_file in merged_outputs:\n        try:\n            content = srt_file.read_text(encoding='utf-8')\n            if cfg['opening_credit']:\n                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n            if cfg['closing_credit']:\n                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n            srt_file.write_text(content, encoding='utf-8')\n            credits_count += 1\n        except Exception as e:\n            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n    status(f\"Credits added to {credits_count} file(s)\")\nelse:\n    status(\"No credits configured\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# COMPLETE\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"TWO-PASS TRANSCRIPTION COMPLETE\")\n\nmode_text = \"parallel\" if PARALLEL_MODE else \"sequential\"\nif cfg['subtitle_language'] == 'llm' and cfg['api_key']:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>‚úì Transcription done ({mode_text})!</b> {len(merged_outputs)} file(s). AI Translation will start next...</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>‚úì Done ({mode_text})!</b> {len(merged_outputs)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n    if cfg['subtitle_language'] == 'llm' and not cfg['api_key']:\n        print(\"Note: AI translation skipped (no API key provided)\")\n\n    if cfg['auto_disconnect']:\n        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n        time.sleep(10)\n        try:\n            from google.colab import runtime\n            runtime.unassign()\n        except: pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-3",
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation (if selected) { display-mode: \"form\" }\n",
    "#@markdown Translate each subtitle file using AI (only runs if \"English (AI translate)\" selected)\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"‚úì\" if ok else \"‚úó\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'‚îÄ'*40}\\n{title}\\n{'‚îÄ'*40}\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "folder_path = WHISPERJAV_FOLDER_PATH\n",
    "\n",
    "# Check if AI translation is needed\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>‚Ñπ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not cfg['api_key']:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for AI translation</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# Set up API key\n",
    "env_map = {\n",
    "    \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "    \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "    \"gemini\": \"GEMINI_API_KEY\",\n",
    "    \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "    \"gpt\": \"OPENAI_API_KEY\"\n",
    "}\n",
    "os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n",
    "\n",
    "# Translate each SRT file\n",
    "section(\"AI TRANSLATION\")\n",
    "print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['_translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "    translate_cmd = [\n",
    "        'whisperjav-translate',\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream'\n",
    "    ]\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "    print()\n",
    "\n",
    "# Complete\n",
    "section(\"COMPLETE\")\n",
    "\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>‚ö† Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>‚úì All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "\n",
    "# Auto-disconnect\n",
    "if cfg['auto_disconnect']:\n",
    "    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "    except: pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}