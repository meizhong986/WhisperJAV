{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_parallel_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<a href=\"https://kaggle.com/kernels/welcome?src=https://github.com/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_parallel_expert.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"/></a>\n",
    "\n",
    "# WhisperJAV Ensemble Dual-GPU Edition (v1.7.5)\n",
    "\n",
    "**Universal** - Auto-detects Kaggle T4x2 for parallel processing, falls back to single-GPU on Colab\n",
    "\n",
    "| Platform | Logic | Storage |\n",
    "|----------|-------|---------|\n",
    "| **Kaggle** | **Parallel** (2x T4) | **Input**: `/kaggle/input` (Dataset) <br> **Output**: `/kaggle/working` (Artifacts) |\n",
    "| **Colab** | **Sequential** (1x GPU) | **Input/Output**: Google Drive |\n",
    "\n",
    "| Option | What it controls |\n",
    "|--------|------------------|\n",
    "| **Scene Detection** | How to split audio into chunks (auditok, silero, semantic) |\n",
    "| **Speech Segmenter** | How to detect speech in audio (silero, ten) |\n",
    "| **Speech Enhancer** | Audio cleanup for noisy sources (ffmpeg-dsp, clearvoice, etc.) |\n",
    "| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n",
    "\n",
    "---\n",
    "<div style=\"font-size: 11px; line-height: 1.4;\">\n",
    "1. <b>Setup Files:</b><br>\n",
    "&nbsp;&nbsp;&nbsp;â€¢ <b>Colab:</b> Upload videos to <code>Google Drive/WhisperJAV/</code><br>\n",
    "&nbsp;&nbsp;&nbsp;â€¢ <b>Kaggle:</b> Add your videos as a Dataset (recommended) or upload to Input.<br>\n",
    "2. Run <b>Step 1: Expert Configuration</b> (required)<br>\n",
    "3. Run <b>Step 2: Two-Pass Transcribe</b> (auto-detects platform)<br>\n",
    "4. Run <b>Step 3: AI Translation</b> (supports Kaggle Secrets/Env/Keys)\n",
    "</div>\n",
    "\n",
    "<small>The notebook will automatically disconnect (Colab) or finish session (Kaggle) when done.</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "_kg_hide-input": true,
    "cellView": "form",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title Step 1: Expert Configuration { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ## ğŸ“ Files & Output\n",
    "folder_name = \"WhisperJAV\" #@param {type:\"string\"}\n",
    "subtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n",
    "spoken_language = \"Japanese\" #@param [\"Japanese\", \"Chinese\", \"English\", \"Korean\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## 1ï¸âƒ£ Pass 1 Configuration (GPU 0)\n",
    "pass1_quality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "pass1_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "pass1_model = \"large-v2\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown **Expert Audio Setup (Pass 1)**\n",
    "pass1_scene_detector = \"semantic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\n",
    "pass1_speech_segmenter = \"ten\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "pass1_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n",
    "#@markdown <font size=\"1\">auditok=energy (fast), silero=VAD, semantic=texture (complex audio) | enhancer: ffmpeg-dsp(no GPU), clearvoice(48k), bs-roformer(vocal)</font>\n",
    "\n",
    "#@markdown **FFmpeg Filters (Pass 1)** *(only if enhancer is ffmpeg-dsp)*\n",
    "pass1_ffmpeg_amplify = True #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_compress = False #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_highpass = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## 2ï¸âƒ£ Pass 2 Configuration (GPU 1)\n",
    "pass2_quality = \"fidelity\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "pass2_sensitivity = \"balanced\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "pass2_model = \"turbo\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown **Expert Audio Setup (Pass 2)**\n",
    "pass2_scene_detector = \"semantic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\n",
    "pass2_speech_segmenter = \"ten\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "pass2_speech_enhancer = \"ffmpeg-dsp\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n",
    "\n",
    "#@markdown **FFmpeg Filters (Pass 2)** *(only if enhancer is ffmpeg-dsp)*\n",
    "pass2_ffmpeg_amplify = True #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_compress = False #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_highpass = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## ğŸ”— Merge Strategy\n",
    "merge_method = \"prefer first pass\" #@param [\"automatic\", \"keep all\", \"prefer first pass\", \"prefer second pass\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## ğŸ¤– AI Translation *(if selected)*\n",
    "translation_service = \"deepseek\" #@param [\"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\n",
    "api_key = \"\" #@param {type:\"string\"}\n",
    "translation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## âš™ï¸ Session\n",
    "opening_credit = \"\" #@param {type:\"string\"}\n",
    "closing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n",
    "auto_disconnect = True #@param {type:\"boolean\"}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION LOGIC\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Mapping dictionaries\n",
    "combine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n",
    "               \"prefer first pass\": \"pass1_primary\", \"prefer second pass\": \"pass2_primary\"}\n",
    "language_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n",
    "                \"English (AI translate)\": \"llm\"}\n",
    "tone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n",
    "spoken_language_map = {\"Japanese\": \"japanese\", \"Chinese\": \"chinese\", \"English\": \"english\", \"Korean\": \"korean\"}\n",
    "\n",
    "# Model mapping (None = use pipeline default)\n",
    "model_map = {\n",
    "    \"automatic\": None,\n",
    "    \"large-v2\": \"large-v2\",\n",
    "    \"large-v3\": \"large-v3\",\n",
    "    \"turbo\": \"turbo\",\n",
    "    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n",
    "    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n",
    "    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n",
    "    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n",
    "}\n",
    "\n",
    "# Define model compatibility:\n",
    "KOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\n",
    "LEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n",
    "\n",
    "# Auto-correct incompatible model-pipeline combinations\n",
    "warnings_list = []\n",
    "\n",
    "# Check Pass 1 compatibility\n",
    "if pass1_model in KOTOBA_MODELS and pass1_quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 1: {pass1_model} requires 'transformers' pipeline. Auto-correcting from '{pass1_quality}' to 'transformers'.\")\n",
    "    pass1_quality = \"transformers\"\n",
    "\n",
    "# Check Pass 2 compatibility\n",
    "if pass2_model in KOTOBA_MODELS and pass2_quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 2: {pass2_model} requires 'transformers' pipeline. Auto-correcting from '{pass2_quality}' to 'transformers'.\")\n",
    "    pass2_quality = \"transformers\"\n",
    "\n",
    "# Memory warning\n",
    "heavy_enhancers = {'clearvoice', 'bs-roformer', 'zipenhancer'}\n",
    "if pass1_speech_enhancer in heavy_enhancers and pass2_speech_enhancer in heavy_enhancers:\n",
    "    warnings_list.append(\"Using GPU-based enhancement on both passes may cause OOM on T4 GPU (Sequential Mode). Suggest using ffmpeg-dsp for one pass.\")\n",
    "\n",
    "# Helpers\n",
    "def build_ffmpeg_filters(amplify, loudnorm, compress, highpass):\n",
    "    \"\"\"Combine selected FFmpeg filters into comma-separated string.\"\"\"\n",
    "    filters = []\n",
    "    if amplify: filters.append(\"amplify\")\n",
    "    if loudnorm: filters.append(\"loudnorm\")\n",
    "    if compress: filters.append(\"compress\")\n",
    "    if highpass: filters.append(\"highpass\")\n",
    "    return \",\".join(filters) if filters else None\n",
    "\n",
    "def map_value(val):\n",
    "    return None if val == \"automatic\" else val\n",
    "\n",
    "def map_segmenter(val):\n",
    "    return \"none\" if val == \"none\" else map_value(val)\n",
    "\n",
    "# Unified Config Construction\n",
    "WHISPERJAV_CONFIG = {\n",
    "    'pass1_pipeline': pass1_quality,\n",
    "    'pass1_sensitivity': pass1_sensitivity,\n",
    "    'pass1_speech_segmenter': map_segmenter(pass1_speech_segmenter),\n",
    "    'pass1_model': model_map[pass1_model],\n",
    "    'pass2_pipeline': pass2_quality,\n",
    "    'pass2_sensitivity': pass2_sensitivity,\n",
    "    'pass2_speech_segmenter': map_segmenter(pass2_speech_segmenter),\n",
    "    'pass2_model': model_map[pass2_model],\n",
    "    'merge_strategy': combine_map[merge_method],\n",
    "    'folder_name': folder_name,\n",
    "    'subtitle_language': language_map[subtitle_language],\n",
    "    'language': spoken_language_map[spoken_language],\n",
    "    'translation_service': translation_service,\n",
    "    'api_key': api_key,\n",
    "    'translation_style': tone_map[translation_style],\n",
    "    'opening_credit': opening_credit,\n",
    "    'closing_credit': closing_credit,\n",
    "    'auto_disconnect': auto_disconnect,\n",
    "    # Compatibility checks for Step 2\n",
    "    '_pass1_quality': pass1_quality,\n",
    "    '_pass1_sensitivity': pass1_sensitivity,\n",
    "    '_pass1_speech_segmenter': pass1_speech_segmenter,\n",
    "    '_pass1_model': pass1_model,\n",
    "    '_pass2_quality': pass2_quality,\n",
    "    '_pass2_sensitivity': pass2_sensitivity,\n",
    "    '_pass2_speech_segmenter': pass2_speech_segmenter,\n",
    "    '_pass2_model': pass2_model,\n",
    "    '_merge_method': merge_method,\n",
    "    '_subtitle_language': subtitle_language,\n",
    "    '_translation_style': translation_style,\n",
    "}\n",
    "\n",
    "WHISPERJAV_EXPERT_CONFIG = {\n",
    "    # Pass 1 Expert\n",
    "    'pass1_scene_detector': map_value(pass1_scene_detector),\n",
    "    'pass1_speech_segmenter': map_segmenter(pass1_speech_segmenter), # Now unified\n",
    "    'pass1_speech_enhancer': None if pass1_speech_enhancer == \"none\" else pass1_speech_enhancer,\n",
    "    'pass1_ffmpeg_filters': build_ffmpeg_filters(pass1_ffmpeg_amplify, pass1_ffmpeg_loudnorm, pass1_ffmpeg_compress, pass1_ffmpeg_highpass) if pass1_speech_enhancer == \"ffmpeg-dsp\" else None,\n",
    "    # Pass 2 Expert\n",
    "    'pass2_scene_detector': map_value(pass2_scene_detector),\n",
    "    'pass2_speech_segmenter': map_segmenter(pass2_speech_segmenter), # Now unified\n",
    "    'pass2_speech_enhancer': None if pass2_speech_enhancer == \"none\" else pass2_speech_enhancer,\n",
    "    'pass2_ffmpeg_filters': build_ffmpeg_filters(pass2_ffmpeg_amplify, pass2_ffmpeg_loudnorm, pass2_ffmpeg_compress, pass2_ffmpeg_highpass) if pass2_speech_enhancer == \"ffmpeg-dsp\" else None,\n",
    "    # Display helpers\n",
    "    '_pass1_scene_detector': pass1_scene_detector,\n",
    "    '_pass1_speech_enhancer': pass1_speech_enhancer,\n",
    "    '_pass2_scene_detector': pass2_scene_detector,\n",
    "    '_pass2_speech_enhancer': pass2_speech_enhancer,\n",
    "}\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display warnings\n",
    "for warning in warnings_list:\n",
    "    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>âš ï¸ Auto-corrected:</b> {warning}</div>'))\n",
    "\n",
    "# Build status display\n",
    "p1_info = f\"{pass1_quality}\"\n",
    "if pass1_speech_segmenter != \"automatic\":\n",
    "    p1_info += f\"/{pass1_speech_segmenter}\"\n",
    "if pass1_model != \"automatic\":\n",
    "    p1_info += f\"/{pass1_model}\"\n",
    "\n",
    "p2_info = f\"{pass2_quality}\"\n",
    "if pass2_speech_segmenter != \"automatic\":\n",
    "    p2_info += f\"/{pass2_speech_segmenter}\"\n",
    "if pass2_model != \"automatic\":\n",
    "    p2_info += f\"/{pass2_model}\"\n",
    "\n",
    "display(HTML(f'<div style=\"padding:10px;background:#e0f2fe;border-radius:4px;font-size:11px\">'\n",
    "             f'<b>Parallel Configuration Loaded</b><br>'\n",
    "             f'Pass 1: {p1_info} | Pass 2: {p2_info}<br>'\n",
    "             f'Merge: {merge_method} | Folder: {folder_name} | Language: {spoken_language}'\n",
    "             f'</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {
    "_kg_hide-input": true,
    "cellView": "form",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title Step 2: Two-Pass Transcribe { display-mode: \"form\" }\n",
    "#@markdown Connect Drive (Colab) or Setup Paths (Kaggle) â†’ Install â†’ Run passes â†’ Merge results\n",
    "\n",
    "import os, sys, subprocess, shlex, time, re, shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "from IPython.core.display import HTML\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"âœ“\" if ok else \"âœ—\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'â”€'*50}\\n{title}\\n{'â”€'*50}\")\n",
    "\n",
    "\n",
    "def tail_lines(path: Optional[Path], n: int = 30, max_chars: int = 4000) -> str:\n",
    "    \"\"\"Return the last N lines of a text file (best-effort) for notebook status panels.\"\"\"\n",
    "    if not path or not path.exists():\n",
    "        return \"\"\n",
    "    try:\n",
    "        text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            text = path.read_text(encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    lines = text.splitlines()\n",
    "    tail = \"\\n\".join(lines[-n:])\n",
    "    if max_chars and len(tail) > max_chars:\n",
    "        tail = tail[-max_chars:]\n",
    "    return tail\n",
    "\n",
    "\n",
    "def render_pass_panel(pass_states: Dict[int, Dict[str, Any]], tail_n: int = 4) -> None:\n",
    "    \"\"\"Render a compact, overwrite-in-place status panel for Kaggle/Colab notebooks.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    section(\"EXECUTION (LIVE STATUS)\")\n",
    "\n",
    "    for pass_num in sorted(pass_states.keys()):\n",
    "        st = pass_states[pass_num]\n",
    "        status_txt = st.get(\"status\", \"?\")\n",
    "        elapsed = st.get(\"elapsed\", 0.0)\n",
    "        rc = st.get(\"returncode\")\n",
    "        log_path = st.get(\"log_path\")\n",
    "        extra = \"\"\n",
    "        if rc is not None:\n",
    "            extra = f\" | rc={rc}\"\n",
    "        print(f\"Pass {pass_num}: {status_txt} | {elapsed:.0f}s{extra}\")\n",
    "        if log_path:\n",
    "            tail = tail_lines(Path(log_path), n=tail_n)\n",
    "            if tail:\n",
    "                print(\"--- log tail ---\")\n",
    "                print(tail)\n",
    "        print()\n",
    "\n",
    "# Check config\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "\n",
    "# Check for expert config (always present now)\n",
    "expert = WHISPERJAV_EXPERT_CONFIG if 'WHISPERJAV_EXPERT_CONFIG' in dir() else None\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PLATFORM DETECTION & FILE SYSTEM SETUP\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"DETECTING PLATFORM & PATHS\")\n",
    "\n",
    "def detect_platform():\n",
    "    if 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    elif os.path.exists('/kaggle'):\n",
    "        return 'kaggle'\n",
    "    else:\n",
    "        return 'local'\n",
    "\n",
    "PLATFORM = detect_platform()\n",
    "print(f\"Platform: {PLATFORM.upper()}\")\n",
    "\n",
    "if PLATFORM == 'colab':\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        base_dir = Path(f\"/content/drive/MyDrive\")\n",
    "        input_dir = base_dir / cfg['folder_name']\n",
    "        output_dir = input_dir # Colab writes back to same folder\n",
    "        input_dir.mkdir(parents=True, exist_ok=True)\n",
    "        status(f\"Drive Connected: {input_dir}\")\n",
    "    except Exception as e:\n",
    "        status(f\"Failed to connect Drive: {e}\", False)\n",
    "        raise SystemExit()\n",
    "\n",
    "elif PLATFORM == 'kaggle':\n",
    "    # Kaggle: Read-only Input, specific Output\n",
    "    # Search for input dataset matching folder_name or standard locations\n",
    "    potential_inputs = [\n",
    "        Path(f\"/kaggle/input/{cfg['folder_name']}\"), # Named dataset\n",
    "        Path(f\"/kaggle/input/whisperjav-media\"),    # Common dataset name\n",
    "        Path(f\"/kaggle/input\"),                     # Root input\n",
    "        Path(f\"/kaggle/working/input\")              # Manual upload location\n",
    "    ]\n",
    "    \n",
    "    input_dir = None\n",
    "    for p in potential_inputs:\n",
    "        if p.exists() and any(f for f in p.rglob('*') if f.suffix.lower() in {'.mp4', '.mkv', '.avi', '.mp3', '.wav', '.flac'}):\n",
    "            input_dir = p\n",
    "            break\n",
    "            \n",
    "    if not input_dir:\n",
    "        input_dir = Path(f\"/kaggle/input\")\n",
    "        print(\"  âš  Could not auto-detect specific input folder. Scanning /kaggle/input root.\")\n",
    "\n",
    "    output_dir = Path(f\"/kaggle/working/{cfg['folder_name']}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # KAGGLE FIX 1: Fix Temp Storage Overflow\n",
    "    # Redirect all temp files to writable working dir to avoid /tmp (ramdisk) filling up\n",
    "    kaggle_temp = Path(f\"/kaggle/working/temp\")\n",
    "    kaggle_temp.mkdir(parents=True, exist_ok=True)\n",
    "    os.environ['TMPDIR'] = str(kaggle_temp)\n",
    "    os.environ['TEMP'] = str(kaggle_temp)\n",
    "    os.environ['TMP'] = str(kaggle_temp)\n",
    "    print(f\"  ğŸ”§ Redirected temp storage to {kaggle_temp}\")\n",
    "\n",
    "    # KAGGLE CHECK 2: Internet Access\n",
    "    try:\n",
    "        import socket\n",
    "        socket.create_connection((\"www.google.com\", 80), timeout=2)\n",
    "    except OSError:\n",
    "        display(HTML('<div style=\"padding:10px;background:#fee2e2;border-radius:4px;border-left:4px solid #ef4444;color:#991b1b;\"><b>ğŸ›‘ Internet Disabled!</b><br>You must enable \"Internet\" in the Notebook Settings (right sidebar) > Internet > On.<br>Otherwise installation will fail.</div>'))\n",
    "        time.sleep(5)\n",
    "\n",
    "    status(f\"Input: {input_dir}\")\n",
    "    status(f\"Output: {output_dir}\")\n",
    "\n",
    "else:\n",
    "    # Local fallback\n",
    "    input_dir = Path(cfg['folder_name'])\n",
    "    output_dir = Path(f\"{cfg['folder_name']}_output\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status(f\"Local Mode: {input_dir} â†’ {output_dir}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHECK GPUs AND DETERMINE MODE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"GPU RESOURCE CHECK\")\n",
    "\n",
    "# Fail-fast: this notebook is intended for GPU runtimes (Kaggle/Colab).\n",
    "gpu_check = subprocess.run(\n",
    "    \"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    " )\n",
    "\n",
    "if gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n",
    "    status(\"No GPU detected. Switch runtime to GPU (T4 x2 or L4/A100).\", False)\n",
    "    if PLATFORM == 'kaggle':\n",
    "        print(\"  Kaggle: right sidebar â†’ Settings â†’ Accelerator â†’ GPU (T4 x2) â†’ Save.\")\n",
    "    elif PLATFORM == 'colab':\n",
    "        print(\"  Colab: Runtime â†’ Change runtime type â†’ Hardware accelerator: GPU.\")\n",
    "    else:\n",
    "        print(\"  Local: ensure NVIDIA drivers are installed and `nvidia-smi` works.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "gpu_lines = [line.strip() for line in gpu_check.stdout.strip().split('\\n') if line.strip()]\n",
    "num_gpus = len(gpu_lines)\n",
    "\n",
    "for i, gpu_info in enumerate(gpu_lines):\n",
    "    status(f\"GPU {i}: {gpu_info}\")\n",
    "\n",
    "# Adaptive mode selection\n",
    "if num_gpus >= 2:\n",
    "    PARALLEL_MODE = True\n",
    "    gpu_assignment = {1: \"0\", 2: \"1\"}\n",
    "    print(f\"\\n  âš¡ Parallel Mode: Pass 1 (GPU 0) | Pass 2 (GPU 1)\")\n",
    "else:\n",
    "    PARALLEL_MODE = False\n",
    "    gpu_assignment = {1: \"0\", 2: \"0\"}\n",
    "    print(f\"\\n  ğŸ“ Sequential Mode: Pass 1 â†’ Pass 2 (Single GPU)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INSTALL WHISPERJAV\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"INSTALLING (2-3 min)\")\n",
    "install_start = time.time()\n",
    "\n",
    "# Platform specific prereqs\n",
    "if PLATFORM == 'colab':\n",
    "    pre_cmds = \"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev libc++1 libc++abi1 > /dev/null 2>&1\"\n",
    "else:\n",
    "    # Kaggle usually has ffmpeg, but good to ensure\n",
    "    pre_cmds = \"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev > /dev/null 2>&1\"\n",
    "\n",
    "# ARCHITECTURE NOTE:\n",
    "# WhisperJAV requires NumPy>=2.0. Standard Kaggle/Colab environments use NumPy 1.x.\n",
    "# To satisfy the requirement without breaking the ecosystem (matplotlib/torch ABI mismatches),\n",
    "# we must perform a synchronized upgrade of the core stack (numpy, matplotlib, torch).\n",
    "# We rely on 'configure_cuda_runtime_paths()' (below) to fix any CUDA version mismatches \n",
    "# caused by replacing the system PyTorch.\n",
    "\n",
    "req_packages = [\n",
    "    # Core Audio/Video\n",
    "    \"ffmpeg-python\", \"auditok\", \"pysrt\", \"srt\", \"aiofiles\", \"pyloudnorm\", \"pydub\",\n",
    "    # AI/ML Stack (Synchronized Upgrade)\n",
    "    \"numpy\", \"matplotlib\", \"scipy\", \"torch\", \"torchaudio\", \"torchvision\",\n",
    "    \"faster-whisper\", \"transformers\", \"optimum\", \"accelerate\", \"huggingface-hub\", \n",
    "    \"pydantic\", \"ten-vad\", \"silero-vad\", \"modelscope\", \"addict\", \"tiktoken\"\n",
    " ]\n",
    "req_str = \" \".join(req_packages)\n",
    "\n",
    "steps = [\n",
    "    (pre_cmds, \"System tools\"),\n",
    "    # Install dependencies allowing upgrades to ensure consistency (Numpy 2.0 compatible stack)\n",
    "    (f\"pip install -q {req_str}\", \"Python packages (Core Stack)\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/meizhong986/WhisperJAV.git@main\", \"WhisperJAV\")\n",
    "]\n",
    "\n",
    "for cmd, name in steps:\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        status(f\"{name} failed\", False)\n",
    "        # On Kaggle, some apt-get might fail but environment is often usable\n",
    "        if PLATFORM != 'kaggle': \n",
    "            raise SystemExit()\n",
    "    else:\n",
    "        status(name)\n",
    "\n",
    "# Conditional installation of speech enhancer dependencies\n",
    "if expert:\n",
    "    extra_packages = set()\n",
    "    for enhancer in [expert.get('pass1_speech_enhancer'), expert.get('pass2_speech_enhancer')]:\n",
    "        if enhancer == 'clearvoice':\n",
    "            extra_packages.add('clearvoice')\n",
    "        elif enhancer == 'zipenhancer':\n",
    "            # zipenhancer uses modelscope which is already installed above\n",
    "            pass\n",
    "        elif enhancer == 'bs-roformer':\n",
    "            extra_packages.add('bs-roformer-infer')\n",
    "    \n",
    "    if extra_packages:\n",
    "        pkg_list = ' '.join(extra_packages)\n",
    "        result = subprocess.run(f\"pip install -q {pkg_list}\", shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            status(f\"Speech enhancer packages failed (continuing anyway)\", False)\n",
    "        else:\n",
    "            status(f\"Speech enhancer packages ({', '.join(extra_packages)})\")\n",
    "\n",
    "status(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RUNTIME CONFIGURATION (Fixing CUDA conflicts)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def configure_cuda_runtime_paths():\n",
    "    \"\"\"\n",
    "    Kaggle/Colab Fix: Link pip-installed NVIDIA libraries to LD_LIBRARY_PATH.\n",
    "    This prevents ctranslate2 (faster-whisper) from using incompatible system CUDA 11.8.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import site\n",
    "        nvidia_paths = []\n",
    "        for site_pkg in site.getsitepackages():\n",
    "            nvidia_root = Path(site_pkg) / 'nvidia'\n",
    "            if nvidia_root.exists():\n",
    "                nvidia_paths.extend([str(p / 'lib') for p in nvidia_root.iterdir() if (p / 'lib').exists()])\n",
    "        \n",
    "        if nvidia_paths:\n",
    "            current_ld = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "            new_ld = os.pathsep.join(nvidia_paths + [current_ld] if current_ld else nvidia_paths)\n",
    "            os.environ['LD_LIBRARY_PATH'] = new_ld\n",
    "            print(f\"  ğŸ”§ Runtime: Linked {len(nvidia_paths)} NVIDIA library paths for ctranslate2\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš  Runtime Warning: {e}\")\n",
    "\n",
    "configure_cuda_runtime_paths()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FIND MEDIA FILES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"SCANNING INPUT FILES\")\n",
    "video_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\n",
    "\n",
    "# Recursive scan for Kaggle datasets structure\n",
    "if PLATFORM == 'kaggle':\n",
    "    videos = []\n",
    "    for f in input_dir.rglob('*'):\n",
    "        if f.suffix.lower() in video_types:\n",
    "            videos.append(f)\n",
    "else:\n",
    "    videos = [f for f in input_dir.iterdir() if f.suffix.lower() in video_types]\n",
    "\n",
    "if not videos:\n",
    "    status(f\"No media files found in {input_dir}\", False)\n",
    "    if PLATFORM == 'kaggle':\n",
    "         print(\"  Hint: Did you add your dataset via '+ Add Data'?\")\n",
    "    raise SystemExit()\n",
    "\n",
    "status(f\"Found {len(videos)} file(s)\")\n",
    "for v in videos[:5]:\n",
    "    print(f\"  â€¢ {v.name}\")\n",
    "if len(videos) > 5:\n",
    "    print(f\"  ... and {len(videos)-5} more\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MERGE FUNCTIONS (from whisperjav/ensemble/merge.py)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Prefer WhisperJAV's own merge engine for correctness.\n",
    "\n",
    "_merge_srt_impl = None\n",
    "try:\n",
    "    from whisperjav.ensemble.merge import MergeEngine\n",
    "    _wj_merger_obj = MergeEngine()\n",
    "\n",
    "    def _merge_srt_impl(\n",
    "        srt1: Path,\n",
    "        srt2: Path,\n",
    "        output: Path,\n",
    "        strategy: str,\n",
    "        _merger: Any = _wj_merger_obj,\n",
    "    ) -> Dict[str, Any]:\n",
    "        return _merger.merge(srt1, srt2, output, strategy=strategy)\n",
    "except Exception as e:\n",
    "    print(f\"  âš  WhisperJAV MergeEngine unavailable, using notebook fallback merge: {e}\")\n",
    "\n",
    "merge_srt = _merge_srt_impl\n",
    "\n",
    "@dataclass\n",
    "class Subtitle:\n",
    "    index: int\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    text: str\n",
    "\n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return self.end_time - self.start_time\n",
    "\n",
    "def parse_srt(path: Path) -> List[Subtitle]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    subtitles = []\n",
    "    content = path.read_text(encoding='utf-8-sig')\n",
    "    blocks = re.split(r'\\n\\s*\\n', content.strip())\n",
    "    for block in blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        lines = block.strip().split('\\n')\n",
    "        if len(lines) < 3:\n",
    "            continue\n",
    "        try:\n",
    "            index = int(lines[0].strip())\n",
    "            ts_match = re.match(r'(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})\\s*-->\\s*(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})', lines[1].strip())\n",
    "            if not ts_match:\n",
    "                continue\n",
    "            g = ts_match.groups()\n",
    "            start = int(g[0])*3600 + int(g[1])*60 + int(g[2]) + int(g[3])/1000\n",
    "            end = int(g[4])*3600 + int(g[5])*60 + int(g[6]) + int(g[7])/1000\n",
    "            text = '\\n'.join(lines[2:]).strip()\n",
    "            subtitles.append(Subtitle(index, start, end, text))\n",
    "        except:\n",
    "            continue\n",
    "    return subtitles\n",
    "\n",
    "def write_srt(subtitles: List[Subtitle], path: Path):\n",
    "    def ts(seconds):\n",
    "        h, m = int(seconds // 3600), int((seconds % 3600) // 60)\n",
    "        s, ms = int(seconds % 60), int((seconds % 1) * 1000)\n",
    "        return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
    "    lines = []\n",
    "    for i, sub in enumerate(subtitles, 1):\n",
    "        lines.extend([str(i), f\"{ts(sub.start_time)} --> {ts(sub.end_time)}\", sub.text, ''])\n",
    "    path.write_text('\\n'.join(lines), encoding='utf-8')\n",
    "\n",
    "def preview_srt(path: Optional[Path], label: str, n: int = 3):\n",
    "    \"\"\"Beta-debug helper: print first/last few lines of an SRT for easy field reports.\"\"\"\n",
    "    if not path:\n",
    "        print(f\"        (preview) {label}: <none>\")\n",
    "        return\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"        (preview) {label}: missing ({p})\")\n",
    "        return\n",
    "    try:\n",
    "        text = p.read_text(encoding='utf-8-sig', errors='replace')\n",
    "    except TypeError:\n",
    "        text = p.read_text(encoding='utf-8-sig')\n",
    "    lines = text.splitlines()\n",
    "    if not lines:\n",
    "        print(f\"        (preview) {label}: empty\")\n",
    "        return\n",
    "    head = lines[:n]\n",
    "    tail = lines[-n:] if len(lines) > n else lines\n",
    "    print(f\"        (preview) {label}: head\")\n",
    "    for ln in head:\n",
    "        print(f\"          {ln}\")\n",
    "    if len(lines) > n:\n",
    "        print(f\"        (preview) {label}: tail\")\n",
    "        for ln in tail:\n",
    "            print(f\"          {ln}\")\n",
    "\n",
    "if _merge_srt_impl is None:\n",
    "    def _merge_srt_impl(srt1: Path, srt2: Path, output: Path, strategy: str) -> Dict[str, Any]:\n",
    "        subs1, subs2 = parse_srt(srt1), parse_srt(srt2)\n",
    "        \n",
    "        if strategy == 'full_merge':\n",
    "            merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs1 + subs2]\n",
    "        elif strategy == 'pass1_primary':\n",
    "            merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs1]\n",
    "            for s2 in subs2:\n",
    "                if not any(max(s1.start_time, s2.start_time) < min(s1.end_time, s2.end_time) for s1 in subs1):\n",
    "                    merged.append(Subtitle(0, s2.start_time, s2.end_time, s2.text))\n",
    "        elif strategy == 'pass2_primary':\n",
    "            merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs2]\n",
    "            for s1 in subs1:\n",
    "                if not any(max(s1.start_time, s2.start_time) < min(s1.end_time, s2.end_time) for s2 in subs2):\n",
    "                    merged.append(Subtitle(0, s1.start_time, s1.end_time, s1.text))\n",
    "        else:  # smart_merge\n",
    "            merged, used = [], set()\n",
    "            for s1 in subs1:\n",
    "                best_i, best_overlap = None, 0\n",
    "                for i, s2 in enumerate(subs2):\n",
    "                    if i in used: continue\n",
    "                    overlap = max(0, min(s1.end_time, s2.end_time) - max(s1.start_time, s2.start_time))\n",
    "                    if overlap > best_overlap:\n",
    "                        best_overlap, best_i = overlap, i\n",
    "                if best_i is not None and best_overlap > 0.3 * min(s1.duration, subs2[best_i].duration):\n",
    "                    used.add(best_i)\n",
    "                    chosen = s1 if s1.duration <= subs2[best_i].duration else subs2[best_i]\n",
    "                    merged.append(Subtitle(0, chosen.start_time, chosen.end_time, chosen.text))\n",
    "                else:\n",
    "                    merged.append(Subtitle(0, s1.start_time, s1.end_time, s1.text))\n",
    "            for i, s2 in enumerate(subs2):\n",
    "                if i not in used:\n",
    "                    merged.append(Subtitle(0, s2.start_time, s2.end_time, s2.text))\n",
    "        \n",
    "        merged.sort(key=lambda s: s.start_time)\n",
    "        write_srt(merged, output)\n",
    "        return {'pass1_count': len(subs1), 'pass2_count': len(subs2), 'merged_count': len(merged), 'strategy': strategy}\n",
    "\n",
    "    merge_srt = _merge_srt_impl\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TWO-PASS TRANSCRIPTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"EXECUTION\")\n",
    "\n",
    "def build_pass_command(pass_num: int, video_path: Path, work_dir: Path, cfg: dict, expert: Optional[dict] = None) -> Tuple[List[str], Path]:\n",
    "    \"\"\"Build whisperjav command for a single pass.\"\"\"\n",
    "    \n",
    "    # Separate directory for each pass to avoid conflicts\n",
    "    # On Kaggle we must write to /kaggle/working (passed as work_dir)\n",
    "    pass_output_dir = work_dir / f\"pass{pass_num}\"\n",
    "    pass_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Use unique local temp directory inside the working directory to ensure space\n",
    "    pass_temp_dir = work_dir / f\"temp_pass{pass_num}\"\n",
    "    pass_temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pipeline = cfg[f'pass{pass_num}_pipeline']\n",
    "    sensitivity = cfg[f'pass{pass_num}_sensitivity']\n",
    "    segmenter = cfg[f'pass{pass_num}_speech_segmenter']\n",
    "    model = cfg[f'pass{pass_num}_model']\n",
    "\n",
    "    cmd = [\n",
    "        'whisperjav', str(video_path),\n",
    "        '--output-dir', str(pass_output_dir),\n",
    "        '--temp-dir', str(pass_temp_dir),\n",
    "        '--ensemble',\n",
    "        '--pass1-pipeline', pipeline,\n",
    "        '--pass1-sensitivity', sensitivity,\n",
    "    ]\n",
    "    # Beta debug: extra visibility + artifacts\n",
    "    if cfg.get('beta_debug', True):\n",
    "        cmd.extend(['--log-level', 'DEBUG'])\n",
    "        cmd.extend(['--stats-file', str(pass_output_dir / 'stats.json')])\n",
    "        cmd.extend(['--trace-params', str(pass_output_dir / 'trace_params.jsonl')])\n",
    "\n",
    "    if segmenter:\n",
    "        cmd.extend(['--pass1-speech-segmenter', segmenter])\n",
    "\n",
    "    if model:\n",
    "        cmd.extend(['--pass1-model', model])\n",
    "\n",
    "    if expert:\n",
    "        scene_detector = expert.get(f'pass{pass_num}_scene_detector')\n",
    "        if scene_detector:\n",
    "            cmd.extend(['--pass1-scene-detector', scene_detector])\n",
    "        \n",
    "        speech_enhancer = expert.get(f'pass{pass_num}_speech_enhancer')\n",
    "        if speech_enhancer:\n",
    "            if speech_enhancer == 'ffmpeg-dsp':\n",
    "                effects = expert.get(f'pass{pass_num}_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'amplify'\n",
    "                cmd.extend(['--pass1-speech-enhancer', f'ffmpeg-dsp:{effects_str}'])\n",
    "            else:\n",
    "                cmd.extend(['--pass1-speech-enhancer', speech_enhancer])\n",
    "\n",
    "    if cfg['subtitle_language'] == 'direct-to-english':\n",
    "        cmd.extend(['--subs-language', 'direct-to-english'])\n",
    "    else:\n",
    "        cmd.extend(['--subs-language', 'native'])\n",
    "\n",
    "    lang = cfg.get('language')\n",
    "    if lang:\n",
    "        cmd.extend(['--language', str(lang)])\n",
    "\n",
    "    return cmd, pass_output_dir\n",
    "\n",
    "\n",
    "\n",
    "def build_twopass_command(video_path: Path, work_dir: Path, cfg: dict, expert: Optional[dict] = None) -> Tuple[List[str], Path]:\n",
    "    \"\"\"Build a single whisperjav command that runs Pass 1 + Pass 2 in one invocation (single GPU mode).\"\"\"\n",
    "\n",
    "    run_output_dir = work_dir / f\"twopass_{video_path.stem}\"\n",
    "    run_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    run_temp_dir = work_dir / f\"temp_twopass_{video_path.stem}\"\n",
    "    run_temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cmd: List[str] = [\n",
    "        'whisperjav', str(video_path),\n",
    "        '--output-dir', str(run_output_dir),\n",
    "        '--temp-dir', str(run_temp_dir),\n",
    "        '--ensemble',\n",
    "        '--pass1-pipeline', cfg['pass1_pipeline'],\n",
    "        '--pass1-sensitivity', cfg['pass1_sensitivity'],\n",
    "        '--pass2-pipeline', cfg['pass2_pipeline'],\n",
    "        '--pass2-sensitivity', cfg['pass2_sensitivity'],\n",
    "        '--merge-strategy', cfg['merge_strategy'],\n",
    "    ]\n",
    "\n",
    "    if cfg.get('beta_debug', True):\n",
    "        cmd.extend(['--log-level', 'DEBUG'])\n",
    "        cmd.extend(['--stats-file', str(run_output_dir / 'stats.json')])\n",
    "        cmd.extend(['--trace-params', str(run_output_dir / 'trace_params.jsonl')])\n",
    "\n",
    "    if cfg.get('pass1_speech_segmenter'):\n",
    "        cmd.extend(['--pass1-speech-segmenter', cfg['pass1_speech_segmenter']])\n",
    "    if cfg.get('pass2_speech_segmenter'):\n",
    "        cmd.extend(['--pass2-speech-segmenter', cfg['pass2_speech_segmenter']])\n",
    "\n",
    "    if cfg.get('pass1_model'):\n",
    "        cmd.extend(['--pass1-model', cfg['pass1_model']])\n",
    "    if cfg.get('pass2_model'):\n",
    "        cmd.extend(['--pass2-model', cfg['pass2_model']])\n",
    "\n",
    "    if expert:\n",
    "        p1_scene = expert.get('pass1_scene_detector')\n",
    "        if p1_scene:\n",
    "            cmd.extend(['--pass1-scene-detector', p1_scene])\n",
    "        p2_scene = expert.get('pass2_scene_detector')\n",
    "        if p2_scene:\n",
    "            cmd.extend(['--pass2-scene-detector', p2_scene])\n",
    "\n",
    "        p1_enh = expert.get('pass1_speech_enhancer')\n",
    "        if p1_enh:\n",
    "            if p1_enh == 'ffmpeg-dsp':\n",
    "                effects = expert.get('pass1_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'amplify'\n",
    "                cmd.extend(['--pass1-speech-enhancer', f'ffmpeg-dsp:{effects_str}'])\n",
    "            else:\n",
    "                cmd.extend(['--pass1-speech-enhancer', p1_enh])\n",
    "\n",
    "        p2_enh = expert.get('pass2_speech_enhancer')\n",
    "        if p2_enh:\n",
    "            if p2_enh == 'ffmpeg-dsp':\n",
    "                effects = expert.get('pass2_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'amplify'\n",
    "                cmd.extend(['--pass2-speech-enhancer', f'ffmpeg-dsp:{effects_str}'])\n",
    "            else:\n",
    "                cmd.extend(['--pass2-speech-enhancer', p2_enh])\n",
    "\n",
    "    if cfg.get('subtitle_language') == 'direct-to-english':\n",
    "        cmd.extend(['--subs-language', 'direct-to-english'])\n",
    "    else:\n",
    "        cmd.extend(['--subs-language', 'native'])\n",
    "\n",
    "    lang = cfg.get('language')\n",
    "    if lang:\n",
    "        cmd.extend(['--language', str(lang)])\n",
    "\n",
    "    return cmd, run_output_dir\n",
    "\n",
    "\n",
    "def run_twopass(video: Path, work_dir: Path, cfg: dict, expert: Optional[dict], gpu_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Run a single 2-pass whisperjav invocation (single GPU mode).\"\"\"\n",
    "\n",
    "    cmd, run_output_dir = build_twopass_command(video, work_dir, cfg, expert)\n",
    "    log_path = run_output_dir / 'run.log'\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    env['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "\n",
    "    full_cmd = shlex.join(cmd)\n",
    "    if cfg.get('beta_debug', True):\n",
    "        print(f\"      cmd(twopass): {full_cmd}\")\n",
    "        print(f\"      log(twopass): {log_path}\")\n",
    "\n",
    "    log_f = open(log_path, 'w', encoding='utf-8', errors='replace')\n",
    "    proc = subprocess.Popen(\n",
    "        full_cmd,\n",
    "        shell=True,\n",
    "        stdout=log_f,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        env=env,\n",
    "        bufsize=1,\n",
    "    )\n",
    "\n",
    "    ctx = {\n",
    "        'pass': 0,\n",
    "        'video': video.name,\n",
    "        'gpu': gpu_id,\n",
    "        'proc': proc,\n",
    "        'log_f': log_f,\n",
    "        'log_path': log_path,\n",
    "        'output_dir': run_output_dir,\n",
    "        'start_time': time.time(),\n",
    "    }\n",
    "\n",
    "    monitor_processes({1: ctx}, refresh_sec=8.0)\n",
    "    proc.wait()\n",
    "\n",
    "    try:\n",
    "        log_f.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    actual_output = find_output_srt(run_output_dir, video.name)\n",
    "    cue_count = len(parse_srt(actual_output)) if actual_output and actual_output.exists() else 0\n",
    "    output_bytes = actual_output.stat().st_size if actual_output and actual_output.exists() else 0\n",
    "\n",
    "    return {\n",
    "        'pass': 0,\n",
    "        'video': video.name,\n",
    "        'success': (proc.returncode == 0 and actual_output and actual_output.exists()),\n",
    "        'output': actual_output,\n",
    "        'output_dir': run_output_dir,\n",
    "        'elapsed': time.time() - ctx['start_time'],\n",
    "        'gpu': gpu_id,\n",
    "        'cue_count': cue_count,\n",
    "        'output_bytes': output_bytes,\n",
    "        'returncode': proc.returncode,\n",
    "        'log_path': log_path,\n",
    "    }\n",
    "\n",
    "\n",
    "def start_pass(pass_num: int, video: Path, work_dir: Path, cfg: dict, expert: Optional[dict], gpu_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Start a pass as a background process, logging stdout/stderr to a file.\"\"\"\n",
    "    cmd, pass_output_dir = build_pass_command(pass_num, video, work_dir, cfg, expert)\n",
    "    log_path = pass_output_dir / \"run.log\"\n",
    "    env = os.environ.copy()\n",
    "    env['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "\n",
    "    full_cmd = shlex.join(cmd)\n",
    "    if cfg.get('beta_debug', True):\n",
    "        print(f\"      cmd(pass{pass_num}): {full_cmd}\")\n",
    "        print(f\"      log(pass{pass_num}): {log_path}\")\n",
    "\n",
    "    log_f = open(log_path, \"w\", encoding=\"utf-8\", errors=\"replace\")\n",
    "    proc = subprocess.Popen(\n",
    "        full_cmd,\n",
    "        shell=True,\n",
    "        stdout=log_f,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        env=env,\n",
    "        bufsize=1,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"pass\": pass_num,\n",
    "        \"video\": video.name,\n",
    "        \"gpu\": gpu_id,\n",
    "        \"proc\": proc,\n",
    "        \"log_f\": log_f,\n",
    "        \"log_path\": log_path,\n",
    "        \"output_dir\": pass_output_dir,\n",
    "        \"start_time\": time.time(),\n",
    "    }\n",
    "\n",
    "\n",
    "def finalize_pass(pass_ctx: Dict[str, Any], video: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Collect results after a pass finishes.\"\"\"\n",
    "    proc = pass_ctx[\"proc\"]\n",
    "    pass_output_dir = pass_ctx[\"output_dir\"]\n",
    "    log_path = pass_ctx[\"log_path\"]\n",
    "    start_time = pass_ctx[\"start_time\"]\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    try:\n",
    "        pass_ctx[\"log_f\"].close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    actual_output = find_output_srt(pass_output_dir, video.name)\n",
    "    cue_count = len(parse_srt(actual_output)) if actual_output and actual_output.exists() else 0\n",
    "    output_bytes = actual_output.stat().st_size if actual_output and actual_output.exists() else 0\n",
    "\n",
    "    return {\n",
    "        'pass': pass_ctx['pass'],\n",
    "        'video': video.name,\n",
    "        'success': (proc.returncode == 0 and actual_output and actual_output.exists()),\n",
    "        'output': actual_output,\n",
    "        'output_dir': pass_output_dir,\n",
    "        'elapsed': elapsed,\n",
    "        'gpu': pass_ctx['gpu'],\n",
    "        'cue_count': cue_count,\n",
    "        'output_bytes': output_bytes,\n",
    "        'returncode': proc.returncode,\n",
    "        'log_path': log_path,\n",
    "    }\n",
    "\n",
    "\n",
    "def monitor_processes(pass_ctxs: Dict[int, Dict[str, Any]], refresh_sec: float = 8.0) -> None:\n",
    "    \"\"\"Poll running processes and show a live notebook status panel.\"\"\"\n",
    "    pass_states: Dict[int, Dict[str, Any]] = {}\n",
    "    for pnum, ctx in pass_ctxs.items():\n",
    "        pass_states[pnum] = {\n",
    "            \"status\": \"running\",\n",
    "            \"elapsed\": 0.0,\n",
    "            \"returncode\": None,\n",
    "            \"log_path\": ctx.get(\"log_path\"),\n",
    "        }\n",
    "\n",
    "    while True:\n",
    "        any_running = False\n",
    "        for pnum, ctx in pass_ctxs.items():\n",
    "            proc = ctx[\"proc\"]\n",
    "            rc = proc.poll()\n",
    "            elapsed = time.time() - ctx[\"start_time\"]\n",
    "            if rc is None:\n",
    "                any_running = True\n",
    "                pass_states[pnum].update({\"status\": \"running\", \"elapsed\": elapsed, \"returncode\": None})\n",
    "            else:\n",
    "                pass_states[pnum].update({\"status\": \"done\", \"elapsed\": elapsed, \"returncode\": rc})\n",
    "\n",
    "        render_pass_panel(pass_states, tail_n=4)\n",
    "        if not any_running:\n",
    "            break\n",
    "        time.sleep(refresh_sec)\n",
    "\n",
    "def find_output_srt(pass_output_dir: Path, video_name: str) -> Optional[Path]:\n",
    "    \"\"\"Find the generated SRT file.\"\"\"\n",
    "    base_name = Path(video_name).stem\n",
    "    patterns = [\n",
    "        f\"{base_name}.*.whisperjav.srt\",\n",
    "        f\"{base_name}.srt\",\n",
    "        f\"{base_name}*.srt\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        matches = list(pass_output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    all_srts = list(pass_output_dir.glob(\"*.srt\"))\n",
    "    return all_srts[0] if all_srts else None\n",
    "\n",
    "def run_pass(pass_num: int, video: Path, work_dir: Path, cfg: dict, expert: Optional[dict], gpu_id: str) -> Dict:\n",
    "    \"\"\"Run a single pass on a specific GPU (kept for sequential mode).\"\"\"\n",
    "    pass_ctx = start_pass(pass_num, video, work_dir, cfg, expert, gpu_id)\n",
    "    monitor_processes({pass_num: pass_ctx}, refresh_sec=8.0)\n",
    "    pass_ctx['proc'].wait()\n",
    "    return finalize_pass(pass_ctx, video)\n",
    "\n",
    "# Info display\n",
    "p1_info = cfg['_pass1_quality']\n",
    "if cfg['_pass1_speech_segmenter'] != 'automatic': p1_info += f\"/{cfg['_pass1_speech_segmenter']}\"\n",
    "p2_info = cfg['_pass2_quality']\n",
    "if cfg['_pass2_speech_segmenter'] != 'automatic': p2_info += f\"/{cfg['_pass2_speech_segmenter']}\"\n",
    "\n",
    "print(f\"Pass 1: {p1_info}\")\n",
    "print(f\"Pass 2: {p2_info}\")\n",
    "print(f\"Merge: {cfg['_merge_method']}\\n\")\n",
    "\n",
    "# Process each video\n",
    "all_results = []\n",
    "merged_outputs = []\n",
    "\n",
    "for video_idx, video in enumerate(videos, 1):\n",
    "    print(f\"\\n[{video_idx}/{len(videos)}] Processing: {video.name}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if PARALLEL_MODE:\n",
    "        # Option A: per-pass logs + a live notebook status panel (no interleaved output)\n",
    "        pass1_ctx = start_pass(1, video, output_dir, cfg, expert, gpu_assignment[1])\n",
    "        pass2_ctx = start_pass(2, video, output_dir, cfg, expert, gpu_assignment[2])\n",
    "        monitor_processes({1: pass1_ctx, 2: pass2_ctx}, refresh_sec=8.0)\n",
    "        pass1_ctx['proc'].wait()\n",
    "        pass2_ctx['proc'].wait()\n",
    "        results[1] = finalize_pass(pass1_ctx, video)\n",
    "        results[2] = finalize_pass(pass2_ctx, video)\n",
    "\n",
    "        # Print a small completion summary (permanent output)\n",
    "        for pass_num in [1, 2]:\n",
    "            result = results[pass_num]\n",
    "            status_icon = \"âœ“\" if result['success'] else \"âœ—\"\n",
    "            print(f\"    {status_icon} Pass {pass_num} (GPU {result['gpu']}): {result['elapsed']:.1f}s | cues={result.get('cue_count', 0)} | bytes={result.get('output_bytes', 0)}\")\n",
    "            if cfg.get('beta_debug', True) and result['success']:\n",
    "                preview_srt(result.get('output'), f\"pass{pass_num}\", n=3)\n",
    "            if not result['success']:\n",
    "                print(f\"        log: {result.get('log_path')}\")\n",
    "                tail = tail_lines(result.get('log_path'), n=25)\n",
    "                if tail:\n",
    "                    print(\"        log tail:\")\n",
    "                    print(tail)\n",
    "    else:\n",
    "        twopass = run_twopass(video, output_dir, cfg, expert, gpu_assignment[1])\n",
    "        status_icon = 'âœ“' if twopass['success'] else 'âœ—'\n",
    "        print(f\"    {status_icon} Two-pass (single GPU {twopass['gpu']}): {twopass['elapsed']:.1f}s | cues={twopass.get('cue_count', 0)} | bytes={twopass.get('output_bytes', 0)}\")\n",
    "        if cfg.get('beta_debug', True) and twopass['success']:\n",
    "            preview_srt(twopass.get('output'), 'twopass', n=3)\n",
    "        if not twopass['success']:\n",
    "            print(f\"        log: {twopass.get('log_path')}\")\n",
    "            tail = tail_lines(twopass.get('log_path'), n=25)\n",
    "            if tail:\n",
    "                print('        log tail:')\n",
    "                print(tail)\n",
    "\n",
    "    # Merge or copy\n",
    "    if not PARALLEL_MODE:\n",
    "        if twopass.get('success') and twopass.get('output') and Path(twopass['output']).exists():\n",
    "            final_output = output_dir / f\"{video.stem}.whisperjav.srt\"\n",
    "            shutil.copy2(twopass['output'], final_output)\n",
    "            merged_outputs.append(final_output)\n",
    "            print('    âœ“ Produced final subtitle from two-pass run')\n",
    "            if cfg.get('beta_debug', True):\n",
    "                preview_srt(final_output, 'final(twopass)', n=3)\n",
    "        else:\n",
    "            print('    âœ— Two-pass run failed!')\n",
    "        all_results.append({0: twopass})\n",
    "        continue\n",
    "\n",
    "    if results[1]['success'] and results[2]['success']:\n",
    "        merged_output = output_dir / f\"{video.stem}.merged.whisperjav.srt\"\n",
    "        stats = merge_srt(results[1]['output'], results[2]['output'], merged_output, cfg['merge_strategy'])\n",
    "        merged_cues = len(parse_srt(merged_output)) if merged_output.exists() else 0\n",
    "        merged_bytes = merged_output.stat().st_size if merged_output.exists() else 0\n",
    "        print(f\"    âœ“ Merged: {stats['pass1_count']} + {stats['pass2_count']} â†’ {stats.get('merged_count', 0)} subtitles | merged_cues={merged_cues} | bytes={merged_bytes}\")\n",
    "        if cfg.get('beta_debug', True):\n",
    "            preview_srt(merged_output, \"merged\", n=3)\n",
    "        merged_outputs.append(merged_output)\n",
    "    else:\n",
    "        for p in [1, 2]:\n",
    "            if results[p]['success']:\n",
    "                final_output = output_dir / f\"{video.stem}.whisperjav.srt\"\n",
    "                shutil.copy2(results[p]['output'], final_output)\n",
    "                merged_outputs.append(final_output)\n",
    "                print(f\"    âš  Using Pass {p} only (other pass failed)\")\n",
    "                if cfg.get('beta_debug', True):\n",
    "                    preview_srt(final_output, f\"final(pass{p})\", n=3)\n",
    "                break\n",
    "        else:\n",
    "            print(f\"    âœ— Both passes failed!\")\n",
    "\n",
    "    all_results.append(results)\n",
    "\n",
    "# Store context for Step 3\n",
    "WHISPERJAV_NEW_SRTS = merged_outputs\n",
    "WHISPERJAV_OUTPUT_DIR = output_dir \n",
    "\n",
    "status(f\"\\nCreated {len(merged_outputs)} merged subtitle file(s) in {output_dir}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ADD CREDITS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"ADDING CREDITS\")\n",
    "\n",
    "if cfg['opening_credit'] or cfg['closing_credit'] and merged_outputs:\n",
    "    credits_count = 0\n",
    "    for srt_file in merged_outputs:\n",
    "        try:\n",
    "            content = srt_file.read_text(encoding='utf-8')\n",
    "            if cfg['opening_credit']:\n",
    "                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n",
    "            if cfg['closing_credit']:\n",
    "                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n",
    "            srt_file.write_text(content, encoding='utf-8')\n",
    "            credits_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n",
    "    status(f\"Credits added to {credits_count} file(s)\")\n",
    "else:\n",
    "    status(\"No credits configured or no files generated\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# COMPLETE & PERSIST\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"TWO-PASS TRANSCRIPTION COMPLETE\")\n",
    "\n",
    "mode_text = \"parallel\" if PARALLEL_MODE else \"sequential\"\n",
    "msg = f\"<b>âœ“ Done ({mode_text})!</b> {len(merged_outputs)} subtitle(s).\"\n",
    "\n",
    "# Create a handy ZIP bundle (all SRTs + per-pass logs + any JSON metadata)\n",
    "zip_path = output_dir / f\"{cfg['folder_name']}_transcriptions.zip\"\n",
    "try:\n",
    "    import zipfile\n",
    "    files_to_zip = []\n",
    "    seen = set()\n",
    "    if output_dir.exists():\n",
    "        for pattern in ['*.srt', '*.log', '*.json']:\n",
    "            for p in output_dir.rglob(pattern):\n",
    "                if not p.is_file():\n",
    "                    continue\n",
    "                rp = str(p.resolve())\n",
    "                if rp in seen:\n",
    "                    continue\n",
    "                seen.add(rp)\n",
    "                files_to_zip.append(p)\n",
    "\n",
    "    if files_to_zip:\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for p in sorted(files_to_zip, key=lambda x: str(x)):\n",
    "                try:\n",
    "                    arcname = p.relative_to(output_dir)\n",
    "                except Exception:\n",
    "                    arcname = p.name\n",
    "                zipf.write(p, str(arcname))\n",
    "        status(f\"Bundled results into: {zip_path.name} ({len(files_to_zip)} files)\")\n",
    "    else:\n",
    "        status(\"No output/log/metadata files found to zip\", False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create zip: {e}\")\n",
    "\n",
    "if PLATFORM == 'kaggle':\n",
    "    msg += f\"<br>ğŸ“‚ <b>Kaggle Output:</b> Check the right sidebar 'Output' tab â†’ <code>{cfg['folder_name']}/</code>.<br>âš  <b>Important:</b> Download files manually NOW or use 'Save Version' to persist them!\"\n",
    "    if zip_path.exists():\n",
    "        from IPython.display import FileLink\n",
    "        # FileLink automatically serves relative to the working directory.\n",
    "        # Check if output_dir is relative or absolute under /kaggle/working\n",
    "        try:\n",
    "             # Calculate path relative to CWD (/kaggle/working)\n",
    "             rel_path = zip_path.relative_to(Path.cwd())\n",
    "             display(FileLink(str(rel_path), result_html_prefix=\"<b>â¬‡ Click to download ZIP: </b>\", result_html_suffix=\" (Kaggle)\"))\n",
    "        except ValueError:\n",
    "             # Fallback if path parsing fails\n",
    "             display(FileLink(zip_path.name, result_html_prefix=\"<b>â¬‡ Click to download ZIP: </b>\", result_html_suffix=\" (Kaggle)\"))\n",
    "\n",
    "else:\n",
    "    msg += f\" saved to Google Drive/...\"\n",
    "    if zip_path.exists():\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(str(zip_path)) \n",
    "        except: pass\n",
    "\n",
    "display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\">{msg}</div>'))\n",
    "\n",
    "if cfg['subtitle_language'] == 'llm' and not cfg['api_key']:\n",
    "    print(\"Note: AI translation skipped (no API key provided)\")\n",
    "\n",
    "if cfg['auto_disconnect']:\n",
    "    print(\"\\nAuto-disconnecting logic triggered...\")\n",
    "    time.sleep(10)\n",
    "    if PLATFORM == 'colab':\n",
    "        try:\n",
    "            from google.colab import runtime\n",
    "            runtime.unassign()\n",
    "        except: pass\n",
    "    elif PLATFORM == 'kaggle':\n",
    "        # Kaggle specific exit/cleanup if needed? \n",
    "        # Usually script ending is enough for Save Version interaction\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "_kg_hide-input": true,
    "cellView": "form",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation (if selected) { display-mode: \"form\" }\n",
    "#@markdown Translate each subtitle file using AI (only runs if \"English (AI translate)\" selected)\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"âœ“\" if ok else \"âœ—\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'â”€'*40}\\n{title}\\n{'â”€'*40}\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "output_dir = WHISPERJAV_OUTPUT_DIR if 'WHISPERJAV_OUTPUT_DIR' in dir() else Path(os.getcwd())\n",
    "\n",
    "# Re-detect platform just in case\n",
    "if 'google.colab' in sys.modules: PLATFORM = 'colab'\n",
    "elif os.path.exists('/kaggle'): PLATFORM = 'kaggle'\n",
    "else: PLATFORM = 'local'\n",
    "\n",
    "# Check if AI translation is needed\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>â„¹ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# API KEY RESOLUTION (SECRETS MANAGER)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def retrieve_key(service_name, config_key):\n",
    "    \"\"\"Try to get key from config, Kaggle secrets, or env.\"\"\"\n",
    "    if config_key and config_key.strip():\n",
    "        return config_key.strip()\n",
    "    \n",
    "    # Try Kaggle Secrets\n",
    "    if PLATFORM == 'kaggle':\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            secrets = UserSecretsClient()\n",
    "            candidates = [\n",
    "                f\"{service_name.upper()}_API_KEY\",\n",
    "                \"API_KEY\",\n",
    "                \"LLM_API_KEY\",\n",
    "                service_name.lower()\n",
    "            ]\n",
    "            for c in candidates:\n",
    "                try:\n",
    "                    k = secrets.get_secret(c)\n",
    "                    if k: return k\n",
    "                except: pass\n",
    "        except: pass\n",
    "        \n",
    "    return \"\"\n",
    "\n",
    "final_api_key = retrieve_key(cfg['translation_service'], cfg.get('api_key'))\n",
    "\n",
    "if not final_api_key:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided (checked Config and Kaggle Secrets)</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# Set up API key\n",
    "env_map = {\n",
    "    \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "    \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "    \"gemini\": \"GEMINI_API_KEY\",\n",
    "    \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "    \"gpt\": \"OPENAI_API_KEY\"\n",
    "}\n",
    "os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = final_api_key\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TRANSLATION LOOP\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"AI TRANSLATION\")\n",
    "print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['_translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "    translate_cmd = [\n",
    "        'whisperjav-translate',\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream'\n",
    "    ]\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "    print()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# COMPLETE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "section(\"COMPLETE\")\n",
    "\n",
    "# Create ZIP of translations\n",
    "if translated_files:\n",
    "    trans_zip_path = output_dir / f\"{cfg['folder_name']}_translations_{cfg['translation_service']}.zip\"\n",
    "    try:\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(trans_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for srt in translated_files:\n",
    "                zipf.write(srt, srt.name)\n",
    "        status(f\"Bundled translations into: {trans_zip_path.name}\")\n",
    "        \n",
    "        # Download triggers\n",
    "        if PLATFORM == 'kaggle':\n",
    "            try:\n",
    "                from IPython.display import FileLink\n",
    "                rel_path = trans_zip_path.relative_to(Path.cwd())\n",
    "                display(FileLink(str(rel_path), result_html_prefix=\"<b>â¬‡ Click to download Translations ZIP: </b>\", result_html_suffix=\" (Kaggle)\"))\n",
    "            except:\n",
    "                display(HTML(f'<b>â¬‡ <a href=\"{trans_zip_path.name}\" target=\"_blank\">Click here to download Translations ZIP</a></b> (Kaggle)'))\n",
    "\n",
    "        elif PLATFORM == 'colab':\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(str(trans_zip_path))\n",
    "            except: pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create zip: {e}\")\n",
    "\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>âš  Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    if PLATFORM == 'kaggle':\n",
    "        loc_msg = f\"<br>ğŸ“‚ <b>files in 'Output' tab â†’ <code>{cfg['folder_name']}/</code></b>. <br>âš  <b>Download now</b> or they will be lost when session ends!\"\n",
    "    else:\n",
    "        loc_msg = f\"in {output_dir}\"\n",
    "\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>âœ“ All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) {loc_msg}</div>'))\n",
    "\n",
    "# Auto-disconnect\n",
    "if cfg['auto_disconnect']:\n",
    "    print(\"\\nAuto-disconnecting in 10s...\")\n",
    "    time.sleep(10)\n",
    "    if PLATFORM == 'colab':\n",
    "        try:\n",
    "            from google.colab import runtime\n",
    "            runtime.unassign()\n",
    "        except: pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
