{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_parallel_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# WhisperJAV (parallel) Two-Pass Edition v1.7.4 (Expert)\n",
    "\n",
    "**Adaptive Two-Pass Processing** - Automatically optimizes for your platform.\n",
    "\n",
    "| Platform | GPUs | How it works |\n",
    "|----------|------|---------------|\n",
    "| **Kaggle** | 2x T4 (8GB each) | **Parallel** - Pass 1 on GPU 0, Pass 2 on GPU 1 simultaneously |\n",
    "| **Colab L4/A100** | 1x GPU (16-24GB) | **Sequential** - Pass 1 first, then Pass 2 (avoids memory issues) |\n",
    "\n",
    "| Option | What it controls |\n",
    "|--------|------------------|\n",
    "| **Scene Detection** | How to split audio into chunks (auditok, silero, semantic) |\n",
    "| **Speech Segmenter** | How to detect speech in audio (silero, ten) |\n",
    "| **Speech Enhancer** | Audio cleanup for noisy sources (ffmpeg-dsp, clearvoice, etc.) |\n",
    "| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n",
    "\n",
    "---\n",
    "<div style=\"font-size: 8px; line-height: 1.0;\">\n",
    "1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n",
    "2. Run <b>Step 1: Expert Configuration</b> (required)<br>\n",
    "3. Run <b>Step 2: Two-Pass Transcribe</b> and wait for completion<br>\n",
    "4. Run <b>Step 3: AI Translation</b> (if selected)\n",
    "</div>\n",
    "\n",
    "<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1: Expert Configuration { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ## üìÅ Files & Output\n",
    "folder_name = \"WhisperJAV\" #@param {type:\"string\"}\n",
    "subtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## 1Ô∏è‚É£ Pass 1 Configuration (GPU 0)\n",
    "pass1_quality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "pass1_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "pass1_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown **Expert Audio Setup (Pass 1)**\n",
    "pass1_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\n",
    "pass1_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "pass1_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n",
    "#@markdown <font size=\"1\">auditok=energy (fast), silero=VAD, semantic=texture (complex audio) | enhancer: ffmpeg-dsp(no GPU), clearvoice(48k), bs-roformer(vocal)</font>\n",
    "\n",
    "#@markdown **FFmpeg Filters (Pass 1)** *(only if enhancer is ffmpeg-dsp)*\n",
    "pass1_ffmpeg_amplify = True #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_compress = False #@param {type:\"boolean\"}\n",
    "pass1_ffmpeg_highpass = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## 2Ô∏è‚É£ Pass 2 Configuration (GPU 1)\n",
    "pass2_quality = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\n",
    "pass2_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\n",
    "pass2_model = \"kotoba-bilingual\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n",
    "\n",
    "#@markdown **Expert Audio Setup (Pass 2)**\n",
    "pass2_scene_detector = \"automatic\" #@param [\"automatic\", \"auditok\", \"silero\", \"semantic\"]\n",
    "pass2_speech_segmenter = \"ten\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\n",
    "pass2_speech_enhancer = \"none\" #@param [\"none\", \"ffmpeg-dsp\", \"clearvoice\", \"zipenhancer\", \"bs-roformer\"]\n",
    "\n",
    "#@markdown **FFmpeg Filters (Pass 2)** *(only if enhancer is ffmpeg-dsp)*\n",
    "pass2_ffmpeg_amplify = True #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_loudnorm = False #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_compress = False #@param {type:\"boolean\"}\n",
    "pass2_ffmpeg_highpass = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## üîó Merge Strategy\n",
    "merge_method = \"prefer first pass\" #@param [\"automatic\", \"keep all\", \"prefer first pass\", \"prefer second pass\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## ü§ñ AI Translation *(if selected)*\n",
    "translation_service = \"deepseek\" #@param [\"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\n",
    "api_key = \"\" #@param {type:\"string\"}\n",
    "translation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## ‚öôÔ∏è Session\n",
    "opening_credit = \"\" #@param {type:\"string\"}\n",
    "closing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n",
    "auto_disconnect = True #@param {type:\"boolean\"}\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CONFIGURATION LOGIC\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Mapping dictionaries\n",
    "combine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n",
    "               \"prefer first pass\": \"pass1_primary\", \"prefer second pass\": \"pass2_primary\"}\n",
    "language_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n",
    "                \"English (AI translate)\": \"llm\"}\n",
    "tone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n",
    "\n",
    "# Model mapping (None = use pipeline default)\n",
    "model_map = {\n",
    "    \"automatic\": None,\n",
    "    \"large-v2\": \"large-v2\",\n",
    "    \"large-v3\": \"large-v3\",\n",
    "    \"turbo\": \"large-v3-turbo\",\n",
    "    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n",
    "    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n",
    "    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n",
    "    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n",
    "}\n",
    "\n",
    "# Define model compatibility:\n",
    "KOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\n",
    "LEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n",
    "\n",
    "# Auto-correct incompatible model-pipeline combinations\n",
    "warnings_list = []\n",
    "\n",
    "# Check Pass 1 compatibility\n",
    "if pass1_model in KOTOBA_MODELS and pass1_quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 1: {pass1_model} requires 'transformers' pipeline. Auto-correcting from '{pass1_quality}' to 'transformers'.\")\n",
    "    pass1_quality = \"transformers\"\n",
    "\n",
    "# Check Pass 2 compatibility\n",
    "if pass2_model in KOTOBA_MODELS and pass2_quality in LEGACY_PIPELINES:\n",
    "    warnings_list.append(f\"Pass 2: {pass2_model} requires 'transformers' pipeline. Auto-correcting from '{pass2_quality}' to 'transformers'.\")\n",
    "    pass2_quality = \"transformers\"\n",
    "\n",
    "# Memory warning\n",
    "heavy_enhancers = {'clearvoice', 'bs-roformer', 'zipenhancer'}\n",
    "if pass1_speech_enhancer in heavy_enhancers and pass2_speech_enhancer in heavy_enhancers:\n",
    "    warnings_list.append(\"Using GPU-based enhancement on both passes may cause OOM on T4 GPU (Sequential Mode). Suggest using ffmpeg-dsp for one pass.\")\n",
    "\n",
    "# Helpers\n",
    "def build_ffmpeg_filters(amplify, loudnorm, compress, highpass):\n",
    "    \"\"\"Combine selected FFmpeg filters into comma-separated string.\"\"\"\n",
    "    filters = []\n",
    "    if amplify: filters.append(\"amplify\")\n",
    "    if loudnorm: filters.append(\"loudnorm\")\n",
    "    if compress: filters.append(\"compress\")\n",
    "    if highpass: filters.append(\"highpass\")\n",
    "    return \",\".join(filters) if filters else None\n",
    "\n",
    "def map_value(val):\n",
    "    return None if val == \"automatic\" else val\n",
    "\n",
    "def map_segmenter(val):\n",
    "    return \"none\" if val == \"none\" else map_value(val)\n",
    "\n",
    "# Unified Config Construction\n",
    "WHISPERJAV_CONFIG = {\n",
    "    'pass1_pipeline': pass1_quality,\n",
    "    'pass1_sensitivity': pass1_sensitivity,\n",
    "    'pass1_speech_segmenter': map_segmenter(pass1_speech_segmenter),\n",
    "    'pass1_model': model_map[pass1_model],\n",
    "    'pass2_pipeline': pass2_quality,\n",
    "    'pass2_sensitivity': pass2_sensitivity,\n",
    "    'pass2_speech_segmenter': map_segmenter(pass2_speech_segmenter),\n",
    "    'pass2_model': model_map[pass2_model],\n",
    "    'merge_strategy': combine_map[merge_method],\n",
    "    'folder_name': folder_name,\n",
    "    'subtitle_language': language_map[subtitle_language],\n",
    "    'translation_service': translation_service,\n",
    "    'api_key': api_key,\n",
    "    'translation_style': tone_map[translation_style],\n",
    "    'opening_credit': opening_credit,\n",
    "    'closing_credit': closing_credit,\n",
    "    'auto_disconnect': auto_disconnect,\n",
    "    # Compatibility checks for Step 2\n",
    "    '_pass1_quality': pass1_quality,\n",
    "    '_pass1_sensitivity': pass1_sensitivity,\n",
    "    '_pass1_speech_segmenter': pass1_speech_segmenter,\n",
    "    '_pass1_model': pass1_model,\n",
    "    '_pass2_quality': pass2_quality,\n",
    "    '_pass2_sensitivity': pass2_sensitivity,\n",
    "    '_pass2_speech_segmenter': pass2_speech_segmenter,\n",
    "    '_pass2_model': pass2_model,\n",
    "    '_merge_method': merge_method,\n",
    "    '_subtitle_language': subtitle_language,\n",
    "    '_translation_style': translation_style,\n",
    "}\n",
    "\n",
    "WHISPERJAV_EXPERT_CONFIG = {\n",
    "    # Pass 1 Expert\n",
    "    'pass1_scene_detector': map_value(pass1_scene_detector),\n",
    "    'pass1_speech_segmenter': map_segmenter(pass1_speech_segmenter), # Now unified\n",
    "    'pass1_speech_enhancer': None if pass1_speech_enhancer == \"none\" else pass1_speech_enhancer,\n",
    "    'pass1_ffmpeg_filters': build_ffmpeg_filters(pass1_ffmpeg_amplify, pass1_ffmpeg_loudnorm, pass1_ffmpeg_compress, pass1_ffmpeg_highpass) if pass1_speech_enhancer == \"ffmpeg-dsp\" else None,\n",
    "    # Pass 2 Expert\n",
    "    'pass2_scene_detector': map_value(pass2_scene_detector),\n",
    "    'pass2_speech_segmenter': map_segmenter(pass2_speech_segmenter), # Now unified\n",
    "    'pass2_speech_enhancer': None if pass2_speech_enhancer == \"none\" else pass2_speech_enhancer,\n",
    "    'pass2_ffmpeg_filters': build_ffmpeg_filters(pass2_ffmpeg_amplify, pass2_ffmpeg_loudnorm, pass2_ffmpeg_compress, pass2_ffmpeg_highpass) if pass2_speech_enhancer == \"ffmpeg-dsp\" else None,\n",
    "    # Display helpers\n",
    "    '_pass1_scene_detector': pass1_scene_detector,\n",
    "    '_pass1_speech_enhancer': pass1_speech_enhancer,\n",
    "    '_pass2_scene_detector': pass2_scene_detector,\n",
    "    '_pass2_speech_enhancer': pass2_speech_enhancer,\n",
    "}\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display warnings\n",
    "for warning in warnings_list:\n",
    "    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>‚ö†Ô∏è Auto-corrected:</b> {warning}</div>'))\n",
    "\n",
    "# Build status display\n",
    "p1_info = f\"{pass1_quality}\"\n",
    "if pass1_speech_segmenter != \"automatic\":\n",
    "    p1_info += f\"/{pass1_speech_segmenter}\"\n",
    "if pass1_model != \"automatic\":\n",
    "    p1_info += f\"/{pass1_model}\"\n",
    "\n",
    "p2_info = f\"{pass2_quality}\"\n",
    "if pass2_speech_segmenter != \"automatic\":\n",
    "    p2_info += f\"/{pass2_speech_segmenter}\"\n",
    "if pass2_model != \"automatic\":\n",
    "    p2_info += f\"/{pass2_model}\"\n",
    "\n",
    "display(HTML(f'<div style=\"padding:10px;background:#e0f2fe;border-radius:4px;font-size:11px\">'\n",
    "             f'<b>Parallel Configuration Loaded</b><br>'\n",
    "             f'Pass 1: {p1_info} | Pass 2: {p2_info}<br>'\n",
    "             f'Merge: {merge_method} | Folder: {folder_name}'\n",
    "             f'</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 2: Two-Pass Transcribe { display-mode: \"form\" }\n",
    "#@markdown Connect Drive ‚Üí Install ‚Üí Run passes (parallel on Kaggle, sequential on Colab) ‚Üí Merge results\n",
    "\n",
    "import os, sys, subprocess, shlex, time, re\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"‚úì\" if ok else \"‚úó\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'‚îÄ'*50}\\n{title}\\n{'‚îÄ'*50}\")\n",
    "\n",
    "# Check config\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "\n",
    "# Check for expert config (always present now)\n",
    "expert = WHISPERJAV_EXPERT_CONFIG if 'WHISPERJAV_EXPERT_CONFIG' in dir() else None\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CONNECT GOOGLE DRIVE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "section(\"CONNECTING GOOGLE DRIVE\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    status(f\"Connected: {folder_path}\")\n",
    "except Exception as e:\n",
    "    status(f\"Failed to connect: {e}\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CHECK GPUs AND DETERMINE MODE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "section(\"DETECTING PLATFORM\")\n",
    "gpu_check = subprocess.run(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\", shell=True, capture_output=True, text=True)\n",
    "if gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n",
    "    status(\"No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "gpu_lines = [line.strip() for line in gpu_check.stdout.strip().split('\\n') if line.strip()]\n",
    "num_gpus = len(gpu_lines)\n",
    "\n",
    "for i, gpu_info in enumerate(gpu_lines):\n",
    "    status(f\"GPU {i}: {gpu_info}\")\n",
    "\n",
    "# Adaptive mode selection\n",
    "if num_gpus >= 2:\n",
    "    PARALLEL_MODE = True\n",
    "    gpu_assignment = {1: \"0\", 2: \"1\"}\n",
    "    print(f\"\\n  ‚ö° Kaggle Mode: PARALLEL (Pass 1 ‚Üí GPU 0, Pass 2 ‚Üí GPU 1)\")\n",
    "else:\n",
    "    PARALLEL_MODE = False\n",
    "    gpu_assignment = {1: \"0\", 2: \"0\"}\n",
    "    print(f\"\\n  üìù Colab Mode: SEQUENTIAL (avoids memory contention)\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# INSTALL WHISPERJAV\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "section(\"INSTALLING (2-3 min)\")\n",
    "install_start = time.time()\n",
    "\n",
    "steps = [\n",
    "    (\"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev libc++1 libc++abi1 > /dev/null 2>&1\", \"System tools\"),\n",
    "    (\"pip install -q tqdm numba tiktoken ffmpeg-python soundfile auditok numpy scipy pysrt srt aiofiles jsonschema Pillow colorama librosa matplotlib pyloudnorm requests faster-whisper transformers optimum accelerate huggingface-hub pydantic ten-vad silero-vad pydub regex modelscope addict\", \"Python packages\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n",
    "    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n",
    "    (\"pip install -q git+https://github.com/meizhong986/WhisperJAV.git@main\", \"WhisperJAV\")\n",
    "]\n",
    "\n",
    "for cmd, name in steps:\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        status(f\"{name} failed\", False)\n",
    "        raise SystemExit()\n",
    "    status(name)\n",
    "\n",
    "# Conditional installation of speech enhancer dependencies\n",
    "if expert:\n",
    "    extra_packages = set()\n",
    "    for enhancer in [expert.get('pass1_speech_enhancer'), expert.get('pass2_speech_enhancer')]:\n",
    "        if enhancer == 'clearvoice':\n",
    "            extra_packages.add('clearvoice')\n",
    "        elif enhancer == 'zipenhancer':\n",
    "            # zipenhancer uses modelscope which is already installed above\n",
    "            pass\n",
    "        elif enhancer == 'bs-roformer':\n",
    "            extra_packages.add('bs-roformer-infer')\n",
    "    \n",
    "    if extra_packages:\n",
    "        pkg_list = ' '.join(extra_packages)\n",
    "        result = subprocess.run(f\"pip install -q {pkg_list}\", shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            status(f\"Speech enhancer packages failed (continuing anyway)\", False)\n",
    "        else:\n",
    "            status(f\"Speech enhancer packages ({', '.join(extra_packages)})\")\n",
    "\n",
    "status(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# FIND MEDIA FILES\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "section(\"SCANNING FILES\")\n",
    "video_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\n",
    "videos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n",
    "\n",
    "if not videos:\n",
    "    status(f\"No media files in {cfg['folder_name']}/\", False)\n",
    "    raise SystemExit()\n",
    "\n",
    "status(f\"Found {len(videos)} file(s)\")\n",
    "for v in videos[:5]:\n",
    "    print(f\"  ‚Ä¢ {v.name}\")\n",
    "if len(videos) > 5:\n",
    "    print(f\"  ... and {len(videos)-5} more\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# MERGE FUNCTIONS (from whisperjav/ensemble/merge.py)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "@dataclass\n",
    "class Subtitle:\n",
    "    index: int\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    text: str\n",
    "\n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return self.end_time - self.start_time\n",
    "\n",
    "def parse_srt(path: Path) -> List[Subtitle]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    subtitles = []\n",
    "    content = path.read_text(encoding='utf-8')\n",
    "    blocks = re.split(r'\\n\\s*\\n', content.strip())\n",
    "    for block in blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        lines = block.strip().split('\\n')\n",
    "        if len(lines) < 3:\n",
    "            continue\n",
    "        try:\n",
    "            index = int(lines[0].strip())\n",
    "            ts_match = re.match(r'(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})\\s*-->\\s*(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})', lines[1].strip())\n",
    "            if not ts_match:\n",
    "                continue\n",
    "            g = ts_match.groups()\n",
    "            start = int(g[0])*3600 + int(g[1])*60 + int(g[2]) + int(g[3])/1000\n",
    "            end = int(g[4])*3600 + int(g[5])*60 + int(g[6]) + int(g[7])/1000\n",
    "            text = '\\n'.join(lines[2:]).strip()\n",
    "            subtitles.append(Subtitle(index, start, end, text))\n",
    "        except:\n",
    "            continue\n",
    "    return subtitles\n",
    "\n",
    "def write_srt(subtitles: List[Subtitle], path: Path):\n",
    "    def ts(seconds):\n",
    "        h, m = int(seconds // 3600), int((seconds % 3600) // 60)\n",
    "        s, ms = int(seconds % 60), int((seconds % 1) * 1000)\n",
    "        return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
    "    lines = []\n",
    "    for i, sub in enumerate(subtitles, 1):\n",
    "        lines.extend([str(i), f\"{ts(sub.start_time)} --> {ts(sub.end_time)}\", sub.text, ''])\n",
    "    path.write_text('\\n'.join(lines), encoding='utf-8')\n",
    "\n",
    "def merge_srt(srt1: Path, srt2: Path, output: Path, strategy: str) -> Dict[str, Any]:\n",
    "    subs1, subs2 = parse_srt(srt1), parse_srt(srt2)\n",
    "    \n",
    "    if strategy == 'full_merge':\n",
    "        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs1 + subs2]\n",
    "    elif strategy == 'pass1_primary':\n",
    "        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs1]\n",
    "        for s2 in subs2:\n",
    "            if not any(max(s1.start_time, s2.start_time) < min(s1.end_time, s2.end_time) for s1 in subs1):\n",
    "                merged.append(Subtitle(0, s2.start_time, s2.end_time, s2.text))\n",
    "    elif strategy == 'pass2_primary':\n",
    "        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs2]\n",
    "        for s1 in subs1:\n",
    "            if not any(max(s1.start_time, s2.start_time) < min(s1.end_time, s2.end_time) for s2 in subs2):\n",
    "                merged.append(Subtitle(0, s1.start_time, s1.end_time, s1.text))\n",
    "    else:  # smart_merge\n",
    "        merged, used = [], set()\n",
    "        for s1 in subs1:\n",
    "            best_i, best_overlap = None, 0\n",
    "            for i, s2 in enumerate(subs2):\n",
    "                if i in used: continue\n",
    "                overlap = max(0, min(s1.end_time, s2.end_time) - max(s1.start_time, s2.start_time))\n",
    "                if overlap > best_overlap:\n",
    "                    best_overlap, best_i = overlap, i\n",
    "            if best_i is not None and best_overlap > 0.3 * min(s1.duration, subs2[best_i].duration):\n",
    "                used.add(best_i)\n",
    "                chosen = s1 if s1.duration <= subs2[best_i].duration else subs2[best_i]\n",
    "                merged.append(Subtitle(0, chosen.start_time, chosen.end_time, chosen.text))\n",
    "            else:\n",
    "                merged.append(Subtitle(0, s1.start_time, s1.end_time, s1.text))\n",
    "        for i, s2 in enumerate(subs2):\n",
    "            if i not in used:\n",
    "                merged.append(Subtitle(0, s2.start_time, s2.end_time, s2.text))\n",
    "    \n",
    "    merged.sort(key=lambda s: s.start_time)\n",
    "    write_srt(merged, output)\n",
    "    return {'pass1_count': len(subs1), 'pass2_count': len(subs2), 'merged_count': len(merged)}\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TWO-PASS TRANSCRIPTION (ADAPTIVE)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "section(\"TWO-PASS TRANSCRIPTION\" + (\" (PARALLEL)\" if PARALLEL_MODE else \" (SEQUENTIAL)\"))\n",
    "\n",
    "def build_pass_command(pass_num: int, video_path: Path, output_dir: Path, cfg: dict, expert: Optional[dict] = None) -> Tuple[List[str], Path]:\n",
    "    \"\"\"Build whisperjav command for a single pass.\n",
    "\n",
    "    Note: WhisperJAV doesn't have --output-name, so we use separate directories\n",
    "    for each pass to avoid conflicts when running in parallel.\n",
    "    Output naming is automatic: {basename}.{lang_code}.whisperjav.srt\n",
    "    \"\"\"\n",
    "    # Use separate directory for each pass to avoid conflicts\n",
    "    pass_output_dir = output_dir / f\"pass{pass_num}\"\n",
    "    pass_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pipeline = cfg[f'pass{pass_num}_pipeline']\n",
    "    sensitivity = cfg[f'pass{pass_num}_sensitivity']\n",
    "    segmenter = cfg[f'pass{pass_num}_speech_segmenter']\n",
    "    model = cfg[f'pass{pass_num}_model']\n",
    "\n",
    "    cmd = [\n",
    "        'whisperjav', str(video_path),\n",
    "        '--output-dir', str(pass_output_dir),\n",
    "        '--ensemble',\n",
    "        '--pass1-pipeline', pipeline,\n",
    "        '--pass1-sensitivity', sensitivity,\n",
    "    ]\n",
    "\n",
    "    # Add speech segmenter from basic config (now unified)\n",
    "    if segmenter:\n",
    "        cmd.extend(['--pass1-speech-segmenter', segmenter])\n",
    "\n",
    "    # Add model if specified\n",
    "    if model:\n",
    "        cmd.extend(['--pass1-model', model])\n",
    "\n",
    "    # Add expert options if provided\n",
    "    if expert:\n",
    "        # Scene detector\n",
    "        scene_detector = expert.get(f'pass{pass_num}_scene_detector')\n",
    "        if scene_detector:\n",
    "            cmd.extend(['--pass1-scene-detector', scene_detector])\n",
    "        \n",
    "        # Speech segmenter already handled from unified config\n",
    "        \n",
    "        # Speech enhancer (ensemble mode only)\n",
    "        speech_enhancer = expert.get(f'pass{pass_num}_speech_enhancer')\n",
    "        if speech_enhancer:\n",
    "            if speech_enhancer == 'ffmpeg-dsp':\n",
    "                effects = expert.get(f'pass{pass_num}_ffmpeg_filters')\n",
    "                effects_str = effects if effects else 'loudnorm'\n",
    "                cmd.extend(['--pass1-speech-enhancer', f'ffmpeg-dsp:{effects_str}'])\n",
    "            else:\n",
    "                cmd.extend(['--pass1-speech-enhancer', speech_enhancer])\n",
    "\n",
    "    # Set subtitle language\n",
    "    if cfg['subtitle_language'] == 'direct-to-english':\n",
    "        cmd.extend(['--subs-language', 'direct-to-english'])\n",
    "    else:\n",
    "        cmd.extend(['--subs-language', 'native'])\n",
    "\n",
    "    # Return the pass output directory - we'll find the SRT file after processing\n",
    "    return cmd, pass_output_dir\n",
    "\n",
    "def find_output_srt(pass_output_dir: Path, video_name: str) -> Path:\n",
    "    \"\"\"Find the generated SRT file in the pass output directory.\n",
    "\n",
    "    WhisperJAV auto-generates: {basename}.{lang}.whisperjav.srt\n",
    "    e.g., video.ja.whisperjav.srt or video.en.whisperjav.srt\n",
    "    \"\"\"\n",
    "    base_name = Path(video_name).stem\n",
    "    # Look for any SRT file matching the video name\n",
    "    patterns = [\n",
    "        f\"{base_name}.*.whisperjav.srt\",  # Standard format\n",
    "        f\"{base_name}.srt\",                # Fallback\n",
    "        f\"{base_name}*.srt\",               # Any SRT with base name\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        matches = list(pass_output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    # Last resort: any SRT in directory\n",
    "    all_srts = list(pass_output_dir.glob(\"*.srt\"))\n",
    "    return all_srts[0] if all_srts else None\n",
    "\n",
    "def run_pass(pass_num: int, video: Path, output_dir: Path, cfg: dict, expert: Optional[dict], gpu_id: str) -> Dict:\n",
    "    \"\"\"Run a single pass on a specific GPU.\"\"\"\n",
    "    cmd, pass_output_dir = build_pass_command(pass_num, video, output_dir, cfg, expert)\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    env['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(shlex.join(cmd), shell=True, capture_output=True, text=True, env=env)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Find the output SRT file\n",
    "    actual_output = find_output_srt(pass_output_dir, video.name)\n",
    "\n",
    "    return {\n",
    "        'pass': pass_num,\n",
    "        'video': video.name,\n",
    "        'success': result.returncode == 0 and actual_output and actual_output.exists(),\n",
    "        'output': actual_output,\n",
    "        'output_dir': pass_output_dir,\n",
    "        'elapsed': elapsed,\n",
    "        'gpu': gpu_id,\n",
    "        'stderr': result.stderr[-500:] if result.stderr else ''  # Last 500 chars for debugging\n",
    "    }\n",
    "\n",
    "# Display mode info\n",
    "p1_info = cfg['_pass1_quality']\n",
    "if cfg['_pass1_speech_segmenter'] != 'automatic':\n",
    "    p1_info += f\"/{cfg['_pass1_speech_segmenter']}\"\n",
    "if cfg['_pass1_model'] != 'automatic':\n",
    "    p1_info += f\"/{cfg['_pass1_model']}\"\n",
    "if expert:\n",
    "    if expert.get('_pass1_scene_detector') != 'automatic':\n",
    "        p1_info += f\" [scene:{expert['_pass1_scene_detector']}]\"\n",
    "    if expert.get('_pass1_speech_enhancer') != 'none':\n",
    "        p1_info += f\" [enh:{expert['_pass1_speech_enhancer']}]\"\n",
    "\n",
    "p2_info = cfg['_pass2_quality']\n",
    "if cfg['_pass2_speech_segmenter'] != 'automatic':\n",
    "    p2_info += f\"/{cfg['_pass2_speech_segmenter']}\"\n",
    "if cfg['_pass2_model'] != 'automatic':\n",
    "    p2_info += f\"/{cfg['_pass2_model']}\"\n",
    "if expert:\n",
    "    if expert.get('_pass2_scene_detector') != 'automatic':\n",
    "        p2_info += f\" [scene:{expert['_pass2_scene_detector']}]\"\n",
    "    if expert.get('_pass2_speech_enhancer') != 'none':\n",
    "        p2_info += f\" [enh:{expert['_pass2_speech_enhancer']}]\"\n",
    "\n",
    "print(f\"Pass 1: {p1_info}\")\n",
    "print(f\"Pass 2: {p2_info}\")\n",
    "print(f\"Merge: {cfg['_merge_method']}\\n\")\n",
    "\n",
    "# Process each video\n",
    "all_results = []\n",
    "merged_outputs = []\n",
    "\n",
    "for video_idx, video in enumerate(videos, 1):\n",
    "    print(f\"\\n[{video_idx}/{len(videos)}] Processing: {video.name}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if PARALLEL_MODE:\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # KAGGLE: Run both passes in parallel on separate GPUs\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            futures = {\n",
    "                executor.submit(run_pass, 1, video, folder_path, cfg, expert, gpu_assignment[1]): 1,\n",
    "                executor.submit(run_pass, 2, video, folder_path, cfg, expert, gpu_assignment[2]): 2\n",
    "            }\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                pass_num = futures[future]\n",
    "                result = future.result()\n",
    "                results[pass_num] = result\n",
    "                status_icon = \"‚úì\" if result['success'] else \"‚úó\"\n",
    "                print(f\"    {status_icon} Pass {pass_num} (GPU {result['gpu']}): {result['elapsed']:.1f}s\")\n",
    "                if not result['success'] and result['stderr']:\n",
    "                    print(f\"        Error: {result['stderr'][:200]}\")\n",
    "    else:\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # COLAB: Run passes sequentially on same GPU\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        for pass_num in [1, 2]:\n",
    "            result = run_pass(pass_num, video, folder_path, cfg, expert, gpu_assignment[pass_num])\n",
    "            results[pass_num] = result\n",
    "            status_icon = \"‚úì\" if result['success'] else \"‚úó\"\n",
    "            print(f\"    {status_icon} Pass {pass_num}: {result['elapsed']:.1f}s\")\n",
    "            if not result['success'] and result['stderr']:\n",
    "                print(f\"        Error: {result['stderr'][:200]}\")\n",
    "\n",
    "    # Merge results if both passes succeeded\n",
    "    if results[1]['success'] and results[2]['success']:\n",
    "        merged_output = folder_path / f\"{video.stem}.merged.whisperjav.srt\"\n",
    "        stats = merge_srt(results[1]['output'], results[2]['output'], merged_output, cfg['merge_strategy'])\n",
    "        print(f\"    ‚úì Merged: {stats['pass1_count']} + {stats['pass2_count']} ‚Üí {stats['merged_count']} subtitles\")\n",
    "        merged_outputs.append(merged_output)\n",
    "    else:\n",
    "        # Use whichever pass succeeded\n",
    "        for p in [1, 2]:\n",
    "            if results[p]['success']:\n",
    "                # Copy to main folder with consistent naming\n",
    "                final_output = folder_path / f\"{video.stem}.whisperjav.srt\"\n",
    "                import shutil\n",
    "                shutil.copy2(results[p]['output'], final_output)\n",
    "                merged_outputs.append(final_output)\n",
    "                print(f\"    ‚ö† Using Pass {p} only (other pass failed)\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"    ‚úó Both passes failed!\")\n",
    "\n",
    "    all_results.append(results)\n",
    "\n",
    "# Store for Step 3\n",
    "WHISPERJAV_NEW_SRTS = merged_outputs\n",
    "WHISPERJAV_FOLDER_PATH = folder_path\n",
    "\n",
    "status(f\"\\nCreated {len(merged_outputs)} merged subtitle file(s)\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# ADD CREDITS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "section(\"ADDING CREDITS\")\n",
    "\n",
    "if cfg['opening_credit'] or cfg['closing_credit']:\n",
    "    credits_count = 0\n",
    "    for srt_file in merged_outputs:\n",
    "        try:\n",
    "            content = srt_file.read_text(encoding='utf-8')\n",
    "            if cfg['opening_credit']:\n",
    "                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n",
    "            if cfg['closing_credit']:\n",
    "                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n",
    "            srt_file.write_text(content, encoding='utf-8')\n",
    "            credits_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n",
    "    status(f\"Credits added to {credits_count} file(s)\\\")\n",
    "else:\n",
    "    status(\"No credits configured\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# COMPLETE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "section(\"TWO-PASS TRANSCRIPTION COMPLETE\")\n",
    "\n",
    "mode_text = \"parallel\" if PARALLEL_MODE else \"sequential\"\n",
    "if cfg['subtitle_language'] == 'llm' and cfg['api_key']:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>‚úì Transcription done ({mode_text})!</b> {len(merged_outputs)} file(s). Run Step 3 next to start AI Translation.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>‚úì Done ({mode_text})!</b> {len(merged_outputs)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "    if cfg['subtitle_language'] == 'llm' and not cfg['api_key']:\n",
    "        print(\"Note: AI translation skipped (no API key provided)\")\n",
    "\n",
    "    if cfg['auto_disconnect']:\n",
    "        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            from google.colab import runtime\n",
    "            runtime.unassign()\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: AI Translation (if selected) { display-mode: \"form\" }\n",
    "#@markdown Translate each subtitle file using AI (only runs if \"English (AI translate)\" selected)\n",
    "\n",
    "import os, sys, subprocess, shlex, time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def status(msg, ok=True):\n",
    "    icon = \"‚úì\" if ok else \"‚úó\"\n",
    "    print(f\"{icon} {msg}\")\n",
    "\n",
    "def section(title):\n",
    "    print(f\"\\n{'‚îÄ'*40}\\n{title}\\n{'‚îÄ'*40}\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'WHISPERJAV_CONFIG' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if 'WHISPERJAV_NEW_SRTS' not in dir():\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "cfg = WHISPERJAV_CONFIG\n",
    "new_srts = WHISPERJAV_NEW_SRTS\n",
    "folder_path = WHISPERJAV_FOLDER_PATH\n",
    "\n",
    "# Check if AI translation is needed\n",
    "if cfg['subtitle_language'] != 'llm':\n",
    "    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>‚Ñπ Skipped:</b> AI translation not selected</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not cfg['api_key']:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for AI translation</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "if not new_srts:\n",
    "    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n",
    "    raise SystemExit()\n",
    "\n",
    "# Set up API key\n",
    "env_map = {\n",
    "    \"deepseek\": \"DEEPSEEK_API_KEY\",\n",
    "    \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "    \"gemini\": \"GEMINI_API_KEY\",\n",
    "    \"claude\": \"ANTHROPIC_API_KEY\",\n",
    "    \"gpt\": \"OPENAI_API_KEY\"\n",
    "}\n",
    "os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n",
    "\n",
    "# Translate each SRT file\n",
    "section(\"AI TRANSLATION\")\n",
    "print(f\"Provider: {cfg['translation_service']}\")\n",
    "print(f\"Style: {cfg['_translation_style']}\")\n",
    "print(f\"Files to translate: {len(new_srts)}\\n\")\n",
    "\n",
    "translated_files = []\n",
    "failed_files = []\n",
    "\n",
    "for i, srt_file in enumerate(new_srts, 1):\n",
    "    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n",
    "\n",
    "    translate_cmd = [\n",
    "        'whisperjav-translate',\n",
    "        '-i', str(srt_file),\n",
    "        '--provider', cfg['translation_service'],\n",
    "        '-t', 'english',\n",
    "        '--tone', cfg['translation_style'],\n",
    "        '--stream'\n",
    "    ]\n",
    "\n",
    "    full_cmd = shlex.join(translate_cmd)\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            full_cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        for line in process.stderr:\n",
    "            print(f\"    {line}\", end='')\n",
    "\n",
    "        stdout_output, _ = process.communicate()\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            output_path = stdout_output.strip()\n",
    "            if output_path:\n",
    "                translated_files.append(Path(output_path))\n",
    "            status(f\"Completed: {srt_file.name}\")\n",
    "        else:\n",
    "            status(f\"Failed: {srt_file.name}\", False)\n",
    "            failed_files.append(srt_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        status(f\"Error translating {srt_file.name}: {e}\", False)\n",
    "        failed_files.append(srt_file)\n",
    "\n",
    "    print()\n",
    "\n",
    "# Complete\n",
    "section(\"COMPLETE\")\n",
    "\n",
    "if failed_files:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>‚ö† Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\n",
    "else:\n",
    "    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>‚úì All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n",
    "\n",
    "# Auto-disconnect\n",
    "if cfg['auto_disconnect']:\n",
    "    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "    except: pass\n",
    "else:\n",
    "    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
