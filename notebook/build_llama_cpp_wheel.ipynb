{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build llama-cpp-python Wheel for Colab\n",
    "\n",
    "This notebook builds a CUDA-accelerated llama-cpp-python wheel that matches Colab's environment.\n",
    "\n",
    "**Output:** `llama_cpp_python-{version}+cu126-cp312-cp312-linux_x86_64.whl`\n",
    "\n",
    "**Steps:**\n",
    "1. Verify Colab environment (CUDA 12.6, Python 3.12)\n",
    "2. Install build dependencies\n",
    "3. Clone and build llama-cpp-python with CUDA support\n",
    "4. Test the wheel\n",
    "5. Download for upload to HuggingFace\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1: Verify Colab Environment { display-mode: \"form\" }\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COLAB ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check GPU and CUDA\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=name,driver_version,memory.total\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode != 0:\n",
    "    print(\"ERROR: No GPU detected!\")\n",
    "    print(\"Go to Runtime -> Change runtime type -> T4 GPU\")\n",
    "    raise SystemExit(\"GPU required for CUDA wheel build\")\n",
    "print(f\"GPU: {result.stdout.strip()}\")\n",
    "\n",
    "# Get CUDA version from nvcc\n",
    "result = subprocess.run([\"nvcc\", \"--version\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    import re\n",
    "    match = re.search(r'release (\\d+\\.\\d+)', result.stdout)\n",
    "    cuda_version = match.group(1) if match else \"unknown\"\n",
    "    print(f\"CUDA Version: {cuda_version}\")\n",
    "    \n",
    "    # Extract major.minor for wheel tag\n",
    "    cuda_tag = f\"cu{''.join(cuda_version.split('.')[:2])}\"\n",
    "    print(f\"CUDA Tag: {cuda_tag}\")\n",
    "else:\n",
    "    print(\"WARNING: nvcc not found, checking nvidia-smi...\")\n",
    "    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "    match = re.search(r'CUDA Version: (\\d+\\.\\d+)', result.stdout)\n",
    "    cuda_version = match.group(1) if match else \"12.6\"\n",
    "    cuda_tag = f\"cu{''.join(cuda_version.split('.')[:2])}\"\n",
    "    print(f\"CUDA Version (from driver): {cuda_version}\")\n",
    "    print(f\"CUDA Tag: {cuda_tag}\")\n",
    "\n",
    "# Python version\n",
    "py_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "py_tag = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "print(f\"\\nPython: {py_version} ({py_tag})\")\n",
    "\n",
    "# Platform\n",
    "import platform\n",
    "plat = platform.machine()\n",
    "print(f\"Platform: linux_{plat}\")\n",
    "\n",
    "# Store for later cells\n",
    "BUILD_CONFIG = {\n",
    "    'cuda_version': cuda_version,\n",
    "    'cuda_tag': cuda_tag,\n",
    "    'py_version': py_version,\n",
    "    'py_tag': py_tag,\n",
    "    'platform': f\"linux_{plat}\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Target wheel: llama_cpp_python-*+{cuda_tag}-{py_tag}-{py_tag}-linux_{plat}.whl\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 2: Install Build Dependencies { display-mode: \"form\" }\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INSTALLING BUILD DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install build tools\n",
    "packages = [\n",
    "    \"build\",\n",
    "    \"wheel\",\n",
    "    \"setuptools>=61.0\",\n",
    "    \"scikit-build-core[pyproject]>=0.5.0\",\n",
    "    \"cmake>=3.21\",\n",
    "    \"ninja\",\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    print(f\"Installing {pkg}...\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        print(f\"  WARNING: {result.stderr.strip()[:100]}\")\n",
    "    else:\n",
    "        print(f\"  OK\")\n",
    "\n",
    "# Verify cmake\n",
    "result = subprocess.run([\"cmake\", \"--version\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"\\ncmake: {result.stdout.strip().split(chr(10))[0]}\")\n",
    "else:\n",
    "    print(\"ERROR: cmake not available\")\n",
    "    raise SystemExit(\"cmake required\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Build dependencies: READY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: Clone llama-cpp-python { display-mode: \"form\" }\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Using JamePeng's fork for better CUDA support\n",
    "REPO_URL = \"https://github.com/JamePeng/llama-cpp-python.git\"\n",
    "REPO_PATH = \"/content/llama-cpp-python\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLONING LLAMA-CPP-PYTHON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(f\"Removing existing clone at {REPO_PATH}...\")\n",
    "    subprocess.run([\"rm\", \"-rf\", REPO_PATH], check=True)\n",
    "\n",
    "print(f\"Cloning {REPO_URL}...\")\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"--recursive\", REPO_URL, REPO_PATH],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"ERROR: Clone failed\")\n",
    "    print(result.stderr)\n",
    "    raise SystemExit(\"Clone failed\")\n",
    "\n",
    "print(f\"Cloned to {REPO_PATH}\")\n",
    "\n",
    "# Get version from pyproject.toml\n",
    "os.chdir(REPO_PATH)\n",
    "import re\n",
    "with open(\"pyproject.toml\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    match = re.search(r'version\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n",
    "    version = match.group(1) if match else \"unknown\"\n",
    "\n",
    "BUILD_CONFIG['llama_version'] = version\n",
    "print(f\"\\nVersion: {version}\")\n",
    "\n",
    "# Get latest commit\n",
    "result = subprocess.run([\"git\", \"log\", \"-1\", \"--oneline\"], capture_output=True, text=True)\n",
    "print(f\"Commit: {result.stdout.strip()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Repository: READY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 4: Build Wheel with CUDA Support { display-mode: \"form\" }\n",
    "#@markdown This takes ~7-10 minutes. Be patient!\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "REPO_PATH = \"/content/llama-cpp-python\"\n",
    "os.chdir(REPO_PATH)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BUILDING WHEEL WITH CUDA SUPPORT\")\n",
    "print(\"=\"*60)\n",
    "print(\"This takes ~7-10 minutes. Please wait...\\n\")\n",
    "\n",
    "# Set build environment for CUDA\n",
    "env = os.environ.copy()\n",
    "env[\"CMAKE_ARGS\"] = \"-DGGML_CUDA=on\"\n",
    "env[\"FORCE_CMAKE\"] = \"1\"\n",
    "\n",
    "# Optional: Set CUDA architectures (T4 = sm_75, but include common ones)\n",
    "# env[\"CMAKE_CUDA_ARCHITECTURES\"] = \"75;80;86;89;90\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Build wheel using pip wheel\n",
    "result = subprocess.run(\n",
    "    [\"pip\", \"wheel\", \".\", \"--no-deps\", \"-w\", \"dist/\", \"-v\"],\n",
    "    env=env,\n",
    "    capture_output=False,  # Show output in real-time\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"\\nERROR: Build failed after {elapsed:.0f}s\")\n",
    "    raise SystemExit(\"Build failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Build completed in {elapsed:.0f} seconds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# List built wheels\n",
    "import glob\n",
    "wheels = glob.glob(f\"{REPO_PATH}/dist/*.whl\")\n",
    "print(f\"\\nBuilt wheels:\")\n",
    "for w in wheels:\n",
    "    size_mb = os.path.getsize(w) / (1024 * 1024)\n",
    "    print(f\"  {os.path.basename(w)} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 5: Rename Wheel with CUDA Tag { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "REPO_PATH = \"/content/llama-cpp-python\"\n",
    "OUTPUT_DIR = \"/content/wheels\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RENAMING WHEEL WITH CUDA TAG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Find the built wheel\n",
    "wheels = glob.glob(f\"{REPO_PATH}/dist/llama_cpp_python*.whl\")\n",
    "if not wheels:\n",
    "    print(\"ERROR: No wheel found in dist/\")\n",
    "    raise SystemExit(\"No wheel found\")\n",
    "\n",
    "original_wheel = wheels[0]\n",
    "original_name = os.path.basename(original_wheel)\n",
    "print(f\"Original: {original_name}\")\n",
    "\n",
    "# Parse wheel name: llama_cpp_python-{version}-{pytag}-{pytag}-{platform}.whl\n",
    "# Add CUDA tag to version: llama_cpp_python-{version}+cu126-{pytag}-{pytag}-{platform}.whl\n",
    "import re\n",
    "match = re.match(r'(llama_cpp_python)-([^-]+)-(.+)', original_name)\n",
    "if match:\n",
    "    pkg_name = match.group(1)\n",
    "    version = match.group(2)\n",
    "    rest = match.group(3)\n",
    "    \n",
    "    cuda_tag = BUILD_CONFIG.get('cuda_tag', 'cu126')\n",
    "    new_version = f\"{version}+{cuda_tag}\"\n",
    "    new_name = f\"{pkg_name}-{new_version}-{rest}\"\n",
    "else:\n",
    "    print(\"WARNING: Could not parse wheel name, using original\")\n",
    "    new_name = original_name\n",
    "\n",
    "# Copy and rename\n",
    "new_wheel = os.path.join(OUTPUT_DIR, new_name)\n",
    "shutil.copy(original_wheel, new_wheel)\n",
    "\n",
    "size_mb = os.path.getsize(new_wheel) / (1024 * 1024)\n",
    "print(f\"New:      {new_name}\")\n",
    "print(f\"Size:     {size_mb:.1f} MB\")\n",
    "print(f\"Location: {new_wheel}\")\n",
    "\n",
    "BUILD_CONFIG['wheel_path'] = new_wheel\n",
    "BUILD_CONFIG['wheel_name'] = new_name\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Wheel renamed: DONE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 6: Test the Wheel { display-mode: \"form\" }\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING THE WHEEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "wheel_path = BUILD_CONFIG.get('wheel_path')\n",
    "if not wheel_path:\n",
    "    print(\"ERROR: No wheel path stored. Run previous cells first.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "# Install the wheel\n",
    "print(f\"Installing {os.path.basename(wheel_path)}...\")\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", wheel_path, \"--force-reinstall\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"ERROR: Install failed\")\n",
    "    print(result.stderr)\n",
    "    raise SystemExit(\"Install failed\")\n",
    "print(\"Installed successfully\")\n",
    "\n",
    "# Test import and CUDA detection\n",
    "print(\"\\nTesting import and CUDA...\")\n",
    "test_code = '''\n",
    "import llama_cpp\n",
    "print(f\"Version: {llama_cpp.__version__}\")\n",
    "\n",
    "# Check if CUDA/cuBLAS is available\n",
    "try:\n",
    "    # Try to check backend support\n",
    "    from llama_cpp import Llama\n",
    "    print(\"Llama class: OK\")\n",
    "    \n",
    "    # Check for GPU layers support (indicates CUDA build)\n",
    "    import inspect\n",
    "    sig = inspect.signature(Llama.__init__)\n",
    "    if 'n_gpu_layers' in sig.parameters:\n",
    "        print(\"n_gpu_layers parameter: FOUND (CUDA build confirmed)\")\n",
    "    else:\n",
    "        print(\"n_gpu_layers parameter: NOT FOUND\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "'''\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-c\", test_code],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(f\"Warnings: {result.stderr[:200]}\")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\nWARNING: Test had issues, but wheel may still work\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Wheel test: PASSED\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 7: Download Wheel { display-mode: \"form\" }\n",
    "#@markdown Downloads the wheel to your local machine via browser.\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOAD WHEEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "wheel_path = BUILD_CONFIG.get('wheel_path')\n",
    "if not wheel_path or not os.path.exists(wheel_path):\n",
    "    print(\"ERROR: Wheel not found. Run previous cells first.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "wheel_name = os.path.basename(wheel_path)\n",
    "size_mb = os.path.getsize(wheel_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"Wheel: {wheel_name}\")\n",
    "print(f\"Size:  {size_mb:.1f} MB\")\n",
    "print(\"\\nStarting download...\")\n",
    "\n",
    "files.download(wheel_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Download started!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to HuggingFace\n",
    "\n",
    "After downloading the wheel, upload it to HuggingFace:\n",
    "\n",
    "### Option A: Web UI\n",
    "1. Go to https://huggingface.co/meizhong986/whisperjav-wheels (create if needed)\n",
    "2. Click \"Add file\" -> \"Upload files\"\n",
    "3. Upload the `.whl` file\n",
    "\n",
    "### Option B: CLI\n",
    "```bash\n",
    "pip install huggingface_hub\n",
    "huggingface-cli login\n",
    "huggingface-cli upload meizhong986/whisperjav-wheels llama_cpp_python-*.whl .\n",
    "```\n",
    "\n",
    "### Install URL\n",
    "Once uploaded, users can install with:\n",
    "```bash\n",
    "pip install https://huggingface.co/meizhong986/whisperjav-wheels/resolve/main/llama_cpp_python-{version}+cu126-cp312-cp312-linux_x86_64.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Optional) Upload to HuggingFace via CLI { display-mode: \"form\" }\n",
    "#@markdown Enter your HuggingFace token to upload directly.\n",
    "\n",
    "hf_token = \"\" #@param {type:\"string\"}\n",
    "repo_id = \"meizhong986/whisperjav-wheels\" #@param {type:\"string\"}\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"No token provided. Download the wheel manually and upload via web UI.\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"UPLOADING TO HUGGINGFACE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Install huggingface_hub\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"huggingface_hub\"], check=True)\n",
    "    \n",
    "    from huggingface_hub import HfApi, create_repo\n",
    "    \n",
    "    api = HfApi(token=hf_token)\n",
    "    \n",
    "    # Create repo if it doesn't exist\n",
    "    try:\n",
    "        create_repo(repo_id, repo_type=\"model\", exist_ok=True, token=hf_token)\n",
    "        print(f\"Repository: {repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {e}\")\n",
    "    \n",
    "    # Upload wheel\n",
    "    wheel_path = BUILD_CONFIG.get('wheel_path')\n",
    "    wheel_name = os.path.basename(wheel_path)\n",
    "    \n",
    "    print(f\"Uploading {wheel_name}...\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=wheel_path,\n",
    "        path_in_repo=wheel_name,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "        token=hf_token,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UPLOAD COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nInstall URL:\")\n",
    "    print(f\"pip install https://huggingface.co/{repo_id}/resolve/main/{wheel_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Summary { display-mode: \"form\" }\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BUILD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nllama-cpp-python version: {BUILD_CONFIG.get('llama_version', 'unknown')}\")\n",
    "print(f\"CUDA version: {BUILD_CONFIG.get('cuda_version', 'unknown')}\")\n",
    "print(f\"Python version: {BUILD_CONFIG.get('py_version', 'unknown')}\")\n",
    "print(f\"Platform: {BUILD_CONFIG.get('platform', 'unknown')}\")\n",
    "print(f\"\\nWheel: {BUILD_CONFIG.get('wheel_name', 'not built')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTEGRATION INSTRUCTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "wheel_name = BUILD_CONFIG.get('wheel_name', 'llama_cpp_python-VERSION+cu126-cp312-cp312-linux_x86_64.whl')\n",
    "print(f\"\"\"\n",
    "1. Upload wheel to HuggingFace:\n",
    "   https://huggingface.co/meizhong986/whisperjav-wheels\n",
    "\n",
    "2. Update install_colab.sh to install from HuggingFace:\n",
    "   \n",
    "   WHEEL_URL=\"https://huggingface.co/meizhong986/whisperjav-wheels/resolve/main/{wheel_name}\"\n",
    "   uv pip install --python \"$VENV_PATH/bin/python\" \"$WHEEL_URL\"\n",
    "\n",
    "3. Users get fast llama-cpp-python installation (~seconds vs ~7 minutes)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
