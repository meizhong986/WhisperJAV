{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Build llama-cpp-python Wheel for Colab (Optimized)\n\nThis notebook builds a CUDA-accelerated llama-cpp-python wheel that matches Colab's environment.\n\n**Build Optimizations** (from PR #135):\n- `CMAKE_BUILD_PARALLEL_LEVEL`: Uses parallel compilation (max 4 jobs on Colab to avoid OOM)\n- `CMAKE_CUDA_ARCHITECTURES`: Targets specific GPU (T4 = sm_75) for faster builds and smaller binary\n- **Anti-idle**: Keeps Colab connected during long builds\n- Estimated build time: **7-10 minutes**\n\n**Output structure** (matches `whisperjav/translate/local_backend.py`):\n```\nllama-cpp-python/\n  cu126/\n    llama_cpp_python-{version}-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n```\n\n**Steps:**\n1. Verify Colab environment (CUDA version, GPU architecture, Python version)\n2. Install build dependencies + enable anti-idle\n3. Clone llama-cpp-python (JamePeng's fork)\n4. Build wheel with CUDA support (optimized)\n5. Prepare wheel with correct manylinux tag\n6. Test the wheel\n7. Download wheel (to your PC for manual upload)\n8. Upload to HuggingFace `mei986/whisperjav-wheels`\n\n**Tips:**\n- Keep the browser tab **active and visible** during build\n- If Colab disconnects, check `/content/llama-cpp-python/dist/` - the wheel may have been built\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 1: Verify Colab Environment { display-mode: \"form\" }\n\nimport subprocess\nimport sys\nimport os\nimport re\nimport multiprocessing\n\nprint(\"=\"*60)\nprint(\"COLAB ENVIRONMENT CHECK\")\nprint(\"=\"*60)\n\n# Check GPU and CUDA\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=name,driver_version,memory.total\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\nif result.returncode != 0:\n    print(\"ERROR: No GPU detected!\")\n    print(\"Go to Runtime -> Change runtime type -> T4 GPU\")\n    raise SystemExit(\"GPU required for CUDA wheel build\")\nprint(f\"GPU: {result.stdout.strip()}\")\n\n# Detect GPU compute capability for optimized builds\ncuda_arch = None\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=compute_cap\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\nif result.returncode == 0:\n    cap = result.stdout.strip().split('\\n')[0].strip()\n    if '.' in cap:\n        major, minor = cap.split('.')\n        cuda_arch = f\"{major}{minor}\"\n        print(f\"GPU Compute Capability: {cap} (sm_{cuda_arch})\")\n\n# Fallback: infer from GPU name if compute_cap not available\nif not cuda_arch:\n    result = subprocess.run(\n        [\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"],\n        capture_output=True, text=True\n    )\n    if result.returncode == 0:\n        gpu_name = result.stdout.strip().lower()\n        # Map GPU series to compute capability\n        if any(x in gpu_name for x in [\"rtx 40\", \"rtx 50\", \"ada\", \"l40\"]):\n            cuda_arch = \"89\"  # Ada Lovelace\n        elif any(x in gpu_name for x in [\"rtx 30\", \"a100\", \"a10\", \"a30\", \"a40\"]):\n            cuda_arch = \"86\"  # Ampere\n        elif any(x in gpu_name for x in [\"rtx 20\", \"gtx 16\", \"t4\", \"quadro rtx\"]):\n            cuda_arch = \"75\"  # Turing (Colab T4)\n        elif any(x in gpu_name for x in [\"v100\", \"titan v\"]):\n            cuda_arch = \"70\"  # Volta\n        elif any(x in gpu_name for x in [\"gtx 10\", \"p100\", \"p40\", \"p4\"]):\n            cuda_arch = \"61\"  # Pascal\n        if cuda_arch:\n            print(f\"GPU Architecture (inferred): sm_{cuda_arch}\")\n\nif not cuda_arch:\n    print(\"WARNING: Could not detect GPU architecture, will build for all architectures (slower)\")\n\n# Get CUDA version from nvcc\nresult = subprocess.run([\"nvcc\", \"--version\"], capture_output=True, text=True)\nif result.returncode == 0:\n    match = re.search(r'release (\\d+)\\.(\\d+)', result.stdout)\n    if match:\n        cuda_major = match.group(1)\n        cuda_minor = match.group(2)\n        cuda_version = f\"{cuda_major}.{cuda_minor}\"\n        cuda_tag = f\"cu{cuda_major}{cuda_minor}\"  # e.g., cu126\n    else:\n        cuda_version = \"unknown\"\n        cuda_tag = \"cu126\"  # default for Colab\n    print(f\"CUDA Version: {cuda_version}\")\n    print(f\"Backend Tag: {cuda_tag}\")\nelse:\n    print(\"WARNING: nvcc not found, assuming CUDA 12.6\")\n    cuda_version = \"12.6\"\n    cuda_tag = \"cu126\"\n\n# Python version\npy_major = sys.version_info.major\npy_minor = sys.version_info.minor\npy_tag = f\"cp{py_major}{py_minor}\"\nprint(f\"\\nPython: {py_major}.{py_minor} ({py_tag})\")\n\n# Platform - use manylinux tag for compatibility\nimport platform\narch = platform.machine()\nif arch == \"x86_64\":\n    plat_tag = \"manylinux_2_17_x86_64.manylinux2014_x86_64\"\nelse:\n    plat_tag = f\"linux_{arch}\"\nprint(f\"Platform: {plat_tag}\")\n\n# Compute optimal parallel build level\n# Colab has ~12GB RAM - too many parallel nvcc jobs can cause OOM\n# Use conservative setting: max 4 jobs for Colab to avoid memory issues\ncores = multiprocessing.cpu_count()\n# Detect if running on Colab\nis_colab = 'COLAB_GPU' in os.environ or os.path.exists('/content')\nif is_colab:\n    # Conservative for Colab: max 4 parallel jobs to avoid OOM\n    parallel_level = min(4, max(2, cores // 2))\n    print(f\"\\nColab detected: using conservative parallel level\")\nelse:\n    # Local machine: use 75% of cores\n    parallel_level = max(2, min(16, int(cores * 0.75)))\n\nprint(f\"CPU Cores: {cores}\")\nprint(f\"Parallel Build Level: {parallel_level} jobs\")\n\n# Store config for later cells\nBUILD_CONFIG = {\n    'cuda_version': cuda_version,\n    'cuda_tag': cuda_tag,\n    'cuda_arch': cuda_arch,  # e.g., \"75\" for T4\n    'py_major': py_major,\n    'py_minor': py_minor,\n    'py_tag': py_tag,\n    'plat_tag': plat_tag,\n    'parallel_level': parallel_level,\n    'is_colab': is_colab,\n    # HuggingFace config (must match local_backend.py)\n    'hf_repo': 'mei986/whisperjav-wheels',\n    'hf_repo_type': 'dataset',\n}\n\nprint(\"\\n\" + \"=\"*60)\nexpected_wheel = f\"llama_cpp_python-VERSION-{py_tag}-{py_tag}-{plat_tag}.whl\"\nprint(f\"Expected wheel name: {expected_wheel}\")\nprint(f\"Upload path: llama-cpp-python/{cuda_tag}/\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 2: Install Build Dependencies { display-mode: \"form\" }\n\nimport subprocess\nimport sys\n\nprint(\"=\"*60)\nprint(\"INSTALLING BUILD DEPENDENCIES\")\nprint(\"=\"*60)\n\n# Install build tools\npackages = [\n    \"build\",\n    \"wheel\",\n    \"setuptools>=61.0\",\n    \"scikit-build-core[pyproject]>=0.5.0\",\n    \"cmake>=3.21\",\n    \"ninja\",\n    \"auditwheel\",  # For repairing wheel tags\n]\n\nfor pkg in packages:\n    print(f\"Installing {pkg}...\")\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n        capture_output=True, text=True\n    )\n    if result.returncode != 0:\n        print(f\"  WARNING: {result.stderr.strip()[:100]}\")\n    else:\n        print(f\"  OK\")\n\n# Verify cmake\nresult = subprocess.run([\"cmake\", \"--version\"], capture_output=True, text=True)\nif result.returncode == 0:\n    print(f\"\\ncmake: {result.stdout.strip().split(chr(10))[0]}\")\nelse:\n    print(\"ERROR: cmake not available\")\n    raise SystemExit(\"cmake required\")\n\n# Set up anti-idle to prevent Colab from disconnecting during long builds\nprint(\"\\n\" + \"-\"*60)\nprint(\"Setting up anti-idle to prevent Colab timeout...\")\nfrom IPython.display import display, Javascript\ndisplay(Javascript('''\n    function ClickConnect(){\n        console.log(\"Keeping Colab alive... \" + new Date().toLocaleTimeString());\n    }\n    // Click every 60 seconds to prevent idle timeout\n    window.colabAntiIdle = setInterval(ClickConnect, 60000);\n    console.log(\"Anti-idle enabled - Colab will stay connected during build\");\n'''))\nprint(\"Anti-idle enabled (pings every 60s)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Build dependencies: READY\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: Clone llama-cpp-python { display-mode: \"form\" }\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Using JamePeng's fork for better CUDA support\n",
    "REPO_URL = \"https://github.com/JamePeng/llama-cpp-python.git\"\n",
    "REPO_PATH = \"/content/llama-cpp-python\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLONING LLAMA-CPP-PYTHON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(f\"Removing existing clone at {REPO_PATH}...\")\n",
    "    subprocess.run([\"rm\", \"-rf\", REPO_PATH], check=True)\n",
    "\n",
    "print(f\"Cloning {REPO_URL}...\")\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"--recursive\", REPO_URL, REPO_PATH],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"ERROR: Clone failed\")\n",
    "    print(result.stderr)\n",
    "    raise SystemExit(\"Clone failed\")\n",
    "\n",
    "print(f\"Cloned to {REPO_PATH}\")\n",
    "\n",
    "# Get version from pyproject.toml\n",
    "os.chdir(REPO_PATH)\n",
    "with open(\"pyproject.toml\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    match = re.search(r'version\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n",
    "    version = match.group(1) if match else \"unknown\"\n",
    "\n",
    "BUILD_CONFIG['llama_version'] = version\n",
    "print(f\"\\nVersion: {version}\")\n",
    "\n",
    "# Get latest commit\n",
    "result = subprocess.run([\"git\", \"log\", \"-1\", \"--oneline\"], capture_output=True, text=True)\n",
    "print(f\"Commit: {result.stdout.strip()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Repository: READY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 4: Build Wheel with CUDA Support (Optimized) { display-mode: \"form\" }\n#@markdown Uses parallel builds and targeted GPU architecture for faster compilation.\n#@markdown Build time: ~5-7 minutes (down from ~10-15 with optimizations).\n\nimport subprocess\nimport os\nimport time\n\nREPO_PATH = \"/content/llama-cpp-python\"\nos.chdir(REPO_PATH)\n\nprint(\"=\"*60)\nprint(\"BUILDING WHEEL WITH CUDA SUPPORT (OPTIMIZED)\")\nprint(\"=\"*60)\n\n# Get build configuration from Step 1\ncuda_arch = BUILD_CONFIG.get('cuda_arch')\nparallel_level = BUILD_CONFIG.get('parallel_level', 4)\n\nprint(f\"\\nBuild Optimizations:\")\nprint(f\"  • Parallel jobs: {parallel_level}\")\nif cuda_arch:\n    print(f\"  • Target architecture: sm_{cuda_arch}\")\nelse:\n    print(f\"  • Target architecture: all (no specific GPU detected)\")\n\n# Build CMAKE_ARGS with optimizations\ncmake_args_parts = [\"-DGGML_CUDA=on\"]\nif cuda_arch:\n    # Target specific GPU architecture for faster build and smaller binary\n    cmake_args_parts.append(f\"-DCMAKE_CUDA_ARCHITECTURES={cuda_arch}\")\n\ncmake_args = \" \".join(cmake_args_parts)\nprint(f\"  • CMAKE_ARGS: {cmake_args}\")\n\nprint(f\"\\nBuilding... (estimated {5 if cuda_arch else 10}-{7 if cuda_arch else 15} minutes)\")\nprint(\"-\"*60 + \"\\n\")\n\n# Set build environment for CUDA with optimizations\nenv = os.environ.copy()\nenv[\"CMAKE_ARGS\"] = cmake_args\nenv[\"CMAKE_BUILD_PARALLEL_LEVEL\"] = str(parallel_level)\nenv[\"FORCE_CMAKE\"] = \"1\"\n\nstart_time = time.time()\n\n# Build wheel using pip wheel\nresult = subprocess.run(\n    [\"pip\", \"wheel\", \".\", \"--no-deps\", \"-w\", \"dist/\", \"-v\"],\n    env=env,\n    capture_output=False,  # Show output in real-time\n)\n\nelapsed = time.time() - start_time\n\nif result.returncode != 0:\n    print(f\"\\nERROR: Build failed after {elapsed:.0f}s\")\n    raise SystemExit(\"Build failed\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Build completed in {elapsed:.0f} seconds ({elapsed/60:.1f} minutes)\")\nprint(\"=\"*60)\n\n# List built wheels\nimport glob\nwheels = glob.glob(f\"{REPO_PATH}/dist/*.whl\")\nprint(f\"\\nBuilt wheels:\")\nfor w in wheels:\n    size_mb = os.path.getsize(w) / (1024 * 1024)\n    print(f\"  {os.path.basename(w)} ({size_mb:.1f} MB)\")\n\n# Store build time for summary\nBUILD_CONFIG['build_time_seconds'] = elapsed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 5: Prepare Wheel for Upload { display-mode: \"form\" }\n",
    "#@markdown Renames wheel to match manylinux tag expected by local_backend.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "REPO_PATH = \"/content/llama-cpp-python\"\n",
    "OUTPUT_DIR = \"/content/wheels\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPARING WHEEL FOR UPLOAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create output directory structure matching HuggingFace repo\n",
    "cuda_tag = BUILD_CONFIG['cuda_tag']\n",
    "upload_dir = f\"{OUTPUT_DIR}/llama-cpp-python/{cuda_tag}\"\n",
    "os.makedirs(upload_dir, exist_ok=True)\n",
    "\n",
    "# Find the built wheel\n",
    "wheels = glob.glob(f\"{REPO_PATH}/dist/llama_cpp_python*.whl\")\n",
    "if not wheels:\n",
    "    print(\"ERROR: No wheel found in dist/\")\n",
    "    raise SystemExit(\"No wheel found\")\n",
    "\n",
    "original_wheel = wheels[0]\n",
    "original_name = os.path.basename(original_wheel)\n",
    "print(f\"Original: {original_name}\")\n",
    "\n",
    "# Parse and rebuild wheel name with correct manylinux tag\n",
    "# Original: llama_cpp_python-{ver}-{py}-{py}-linux_x86_64.whl\n",
    "# Target:   llama_cpp_python-{ver}-{py}-{py}-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "\n",
    "match = re.match(r'(llama_cpp_python)-([^-]+)-([^-]+)-([^-]+)-(.+)\\.whl', original_name)\n",
    "if match:\n",
    "    pkg_name = match.group(1)\n",
    "    version = match.group(2)\n",
    "    py_tag1 = match.group(3)\n",
    "    py_tag2 = match.group(4)\n",
    "    old_plat = match.group(5)\n",
    "    \n",
    "    # Use the correct manylinux tag\n",
    "    new_plat = BUILD_CONFIG['plat_tag']\n",
    "    new_name = f\"{pkg_name}-{version}-{py_tag1}-{py_tag2}-{new_plat}.whl\"\n",
    "    \n",
    "    print(f\"Renaming platform tag: {old_plat} -> {new_plat}\")\n",
    "else:\n",
    "    print(\"WARNING: Could not parse wheel name, using original\")\n",
    "    new_name = original_name\n",
    "\n",
    "# Copy to upload directory\n",
    "dest_wheel = os.path.join(upload_dir, new_name)\n",
    "shutil.copy(original_wheel, dest_wheel)\n",
    "\n",
    "size_mb = os.path.getsize(dest_wheel) / (1024 * 1024)\n",
    "print(f\"\\nFinal wheel: {new_name}\")\n",
    "print(f\"Size: {size_mb:.1f} MB\")\n",
    "print(f\"Location: {dest_wheel}\")\n",
    "\n",
    "BUILD_CONFIG['wheel_path'] = dest_wheel\n",
    "BUILD_CONFIG['wheel_name'] = new_name\n",
    "BUILD_CONFIG['upload_dir'] = upload_dir\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Ready for upload to: {BUILD_CONFIG['hf_repo']}\")\n",
    "print(f\"Path in repo: llama-cpp-python/{cuda_tag}/{new_name}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 6: Test the Wheel { display-mode: \"form\" }\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING THE WHEEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "wheel_path = BUILD_CONFIG.get('wheel_path')\n",
    "if not wheel_path:\n",
    "    print(\"ERROR: No wheel path stored. Run previous cells first.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "# Install the wheel\n",
    "print(f\"Installing {os.path.basename(wheel_path)}...\")\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", wheel_path, \"--force-reinstall\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"ERROR: Install failed\")\n",
    "    print(result.stderr)\n",
    "    raise SystemExit(\"Install failed\")\n",
    "print(\"Installed successfully\")\n",
    "\n",
    "# Test import and CUDA detection\n",
    "print(\"\\nTesting import and CUDA...\")\n",
    "test_code = '''\n",
    "import llama_cpp\n",
    "print(f\"Version: {llama_cpp.__version__}\")\n",
    "\n",
    "# Check if CUDA/cuBLAS is available\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "    print(\"Llama class: OK\")\n",
    "    \n",
    "    # Check for GPU layers support (indicates CUDA build)\n",
    "    import inspect\n",
    "    sig = inspect.signature(Llama.__init__)\n",
    "    if 'n_gpu_layers' in sig.parameters:\n",
    "        print(\"n_gpu_layers parameter: FOUND (CUDA build confirmed)\")\n",
    "    else:\n",
    "        print(\"n_gpu_layers parameter: NOT FOUND\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "'''\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-c\", test_code],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(f\"Warnings: {result.stderr[:200]}\")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\nWARNING: Test had issues, but wheel may still work\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Wheel test: PASSED\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 7: Download Wheel (Manual Upload) { display-mode: \"form\" }\n",
    "#@markdown Downloads the wheel to your local machine for manual upload.\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOAD WHEEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "wheel_path = BUILD_CONFIG.get('wheel_path')\n",
    "if not wheel_path or not os.path.exists(wheel_path):\n",
    "    print(\"ERROR: Wheel not found. Run previous cells first.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "wheel_name = os.path.basename(wheel_path)\n",
    "size_mb = os.path.getsize(wheel_path) / (1024 * 1024)\n",
    "cuda_tag = BUILD_CONFIG['cuda_tag']\n",
    "\n",
    "print(f\"Wheel: {wheel_name}\")\n",
    "print(f\"Size:  {size_mb:.1f} MB\")\n",
    "print(f\"\\nUpload to HuggingFace:\")\n",
    "print(f\"  Repo: {BUILD_CONFIG['hf_repo']}\")\n",
    "print(f\"  Path: llama-cpp-python/{cuda_tag}/{wheel_name}\")\n",
    "print(\"\\nStarting download...\")\n",
    "\n",
    "files.download(wheel_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Download started!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 8: Upload to HuggingFace (Direct) { display-mode: \"form\" }\n",
    "#@markdown Enter your HuggingFace token to upload directly.\n",
    "\n",
    "hf_token = \"\" #@param {type:\"string\"}\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"No token provided. Use Step 7 to download and upload manually.\")\n",
    "    print(\"\\nManual upload instructions:\")\n",
    "    print(f\"  1. Go to https://huggingface.co/datasets/{BUILD_CONFIG['hf_repo']}\")\n",
    "    print(f\"  2. Navigate to: llama-cpp-python/{BUILD_CONFIG['cuda_tag']}/\")\n",
    "    print(f\"  3. Upload: {BUILD_CONFIG['wheel_name']}\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"UPLOADING TO HUGGINGFACE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Install huggingface_hub\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"huggingface_hub\"], check=True)\n",
    "    \n",
    "    from huggingface_hub import HfApi\n",
    "    \n",
    "    api = HfApi(token=hf_token)\n",
    "    \n",
    "    wheel_path = BUILD_CONFIG['wheel_path']\n",
    "    wheel_name = BUILD_CONFIG['wheel_name']\n",
    "    cuda_tag = BUILD_CONFIG['cuda_tag']\n",
    "    repo_id = BUILD_CONFIG['hf_repo']\n",
    "    \n",
    "    # Path in repo: llama-cpp-python/{cuda_tag}/{wheel_name}\n",
    "    path_in_repo = f\"llama-cpp-python/{cuda_tag}/{wheel_name}\"\n",
    "    \n",
    "    print(f\"Repo: {repo_id}\")\n",
    "    print(f\"Path: {path_in_repo}\")\n",
    "    print(f\"Uploading...\")\n",
    "    \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=wheel_path,\n",
    "        path_in_repo=path_in_repo,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "        token=hf_token,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UPLOAD COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nWheel is now available at:\")\n",
    "    print(f\"  https://huggingface.co/datasets/{repo_id}/tree/main/llama-cpp-python/{cuda_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Summary & Next Steps { display-mode: \"form\" }\n\nprint(\"=\"*60)\nprint(\"BUILD SUMMARY\")\nprint(\"=\"*60)\n\nprint(f\"\\nllama-cpp-python version: {BUILD_CONFIG.get('llama_version', 'unknown')}\")\nprint(f\"CUDA version: {BUILD_CONFIG.get('cuda_version', 'unknown')}\")\nprint(f\"Backend tag: {BUILD_CONFIG.get('cuda_tag', 'unknown')}\")\nprint(f\"GPU architecture: sm_{BUILD_CONFIG.get('cuda_arch', 'unknown')}\")\nprint(f\"Python: {BUILD_CONFIG.get('py_major', '?')}.{BUILD_CONFIG.get('py_minor', '?')}\")\nprint(f\"Platform: {BUILD_CONFIG.get('plat_tag', 'unknown')}\")\n\nbuild_time = BUILD_CONFIG.get('build_time_seconds')\nif build_time:\n    print(f\"\\nBuild time: {build_time:.0f}s ({build_time/60:.1f} minutes)\")\n    \nprint(f\"\\nBuild optimizations used:\")\nprint(f\"  • Parallel jobs: {BUILD_CONFIG.get('parallel_level', 'unknown')}\")\ncuda_arch = BUILD_CONFIG.get('cuda_arch')\nif cuda_arch:\n    print(f\"  • Target architecture: sm_{cuda_arch} (optimized)\")\nelse:\n    print(f\"  • Target architecture: all (generic)\")\n\nprint(f\"\\nWheel: {BUILD_CONFIG.get('wheel_name', 'not built')}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"INTEGRATION WITH WHISPERJAV\")\nprint(\"=\"*60)\n\nwheel_name = BUILD_CONFIG.get('wheel_name', 'llama_cpp_python-VERSION-cpXX-cpXX-manylinux.whl')\nversion = BUILD_CONFIG.get('llama_version', 'X.X.X')\ncuda_tag = BUILD_CONFIG.get('cuda_tag', 'cu126')\n\nprint(f\"\"\"\nAfter uploading to HuggingFace, update local_backend.py:\n\n1. Update WHEEL_VERSION constant:\n   WHEEL_VERSION = \"{version}\"\n\n2. The wheel will be auto-downloaded when users run:\n   whisperjav-translate -i input.srt --provider local\n\n3. Wheel download path:\n   {BUILD_CONFIG['hf_repo']}/llama-cpp-python/{cuda_tag}/{wheel_name}\n\n4. Verify with:\n   python -c \"from whisperjav.translate.local_backend import ensure_llama_cpp_installed; ensure_llama_cpp_installed()\"\n\"\"\")\n\nprint(\"=\"*60)\nprint(\"DONE!\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}