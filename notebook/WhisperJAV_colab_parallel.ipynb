{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/meizhong986/WhisperJAV/blob/main/notebook/WhisperJAV_colab_parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# WhisperJAV Two-Pass Edition v1.8.2\n\n**Adaptive Two-Pass Processing** - Automatically optimizes for your platform.\n\n| Platform | GPUs | How it works |\n|----------|------|---------------|\n| **Kaggle** | 2x T4 (8GB each) | **Parallel** - Pass 1 on GPU 0, Pass 2 on GPU 1 simultaneously |\n| **Colab L4/A100** | 1x GPU (16-24GB) | **Sequential** - Pass 1 first, then Pass 2 (avoids memory issues) |\n\n| Option | What it controls |\n|--------|------------------|\n| **Speech Segmenter** | How to detect speech in audio (silero, ten, none) |\n| **Model** | Which AI model to use (large-v2, large-v3, turbo, kotoba) |\n\n---\n<div style=\"font-size: 8px; line-height: 1.0;\">\n1. Upload your videos to <code>Google Drive/WhisperJAV/</code><br>\n2. Select settings and Click <b>Runtime ‚Üí Run all</b> in the menu<br>\n3. <b>Connect Google Drive</b> when prompted<br>\n4. Wait for your subtitles!\n</div>\n\n<small>The notebook will automatically disconnect when finished to save your GPU credits.</small>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 1: Settings { display-mode: \"form\" }\n\n#@markdown **Pass 1 Configuration**\npass1_quality = \"balanced\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\npass1_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\npass1_speech_segmenter = \"automatic\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\npass1_model = \"automatic\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n\n#@markdown ---\n#@markdown **Pass 2 Configuration**\npass2_quality = \"transformers\" #@param [\"faster\", \"fast\", \"balanced\", \"fidelity\", \"transformers\"]\npass2_sensitivity = \"aggressive\" #@param [\"conservative\", \"balanced\", \"aggressive\"]\npass2_speech_segmenter = \"ten\" #@param [\"automatic\", \"silero\", \"ten\", \"none\"]\npass2_model = \"kotoba-bilingual\" #@param [\"automatic\", \"large-v2\", \"large-v3\", \"turbo\", \"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"]\n\n#@markdown ---\n#@markdown **Merge Strategy**\nmerge_method = \"prefer first pass\" #@param [\"automatic\", \"keep all\", \"prefer first pass\", \"prefer second pass\"]\n\n#@markdown ---\n#@markdown **Files & Output**\nfolder_name = \"WhisperJAV\" #@param {type:\"string\"}\nsubtitle_language = \"Japanese\" #@param [\"Japanese\", \"English (auto-translate)\", \"English (AI translate)\"]\n\n#@markdown ---\n#@markdown **AI Translation** *(if selected \"English (AI translate)\")*\ntranslation_service = \"local\" #@param [\"local\", \"deepseek\", \"openrouter\", \"gemini\", \"claude\", \"gpt\"]\nlocal_model = \"gemma-9b\" #@param [\"gemma-9b\", \"llama-8b\", \"llama-3b\", \"auto\"]\n#@markdown <font size=\"1\">local: Free, runs on GPU. gemma-9b (8GB+ VRAM), llama-8b (6GB+), llama-3b (3GB+). Cloud providers require API key.</font>\napi_key = \"\" #@param {type:\"string\"}\ntranslation_style = \"standard\" #@param [\"standard\", \"explicit\"]\n\n#@markdown ---\n#@markdown **Credits**\nopening_credit = \"\" #@param {type:\"string\"}\nclosing_credit = \"Subs by WhisperJAV\" #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown **Session**\nauto_disconnect = True #@param {type:\"boolean\"}\n#@markdown ‚òùÔ∏è Auto-disconnect when done (saves GPU credits)\n\n# Mapping dictionaries\ncombine_map = {\"automatic\": \"smart_merge\", \"keep all\": \"full_merge\",\n               \"prefer first pass\": \"pass1_primary\", \"prefer second pass\": \"pass2_primary\"}\nlanguage_map = {\"Japanese\": \"native\", \"English (auto-translate)\": \"direct-to-english\",\n                \"English (AI translate)\": \"llm\"}\ntone_map = {\"standard\": \"standard\", \"explicit\": \"pornify\"}\n\n# Speech segmenter mapping (None = use pipeline default)\nsegmenter_map = {\"automatic\": None, \"silero\": \"silero\", \"ten\": \"ten\", \"none\": \"none\"}\n\n# Model mapping (None = use pipeline default)\nmodel_map = {\n    \"automatic\": None,\n    \"large-v2\": \"large-v2\",\n    \"large-v3\": \"large-v3\",\n    \"turbo\": \"large-v3-turbo\",\n    \"kotoba-bilingual\": \"kotoba-tech/kotoba-whisper-bilingual-v1.0\",\n    \"kotoba-v2.0\": \"kotoba-tech/kotoba-whisper-v2.0\",\n    \"kotoba-v2.1\": \"kotoba-tech/kotoba-whisper-v2.1\",\n    \"kotoba-v2.2\": \"kotoba-tech/kotoba-whisper-v2.2\"\n}\n\n# Define model compatibility:\n# - Kotoba models (HuggingFace) ONLY work with \"transformers\" pipeline\n# - Legacy models (large-v2/v3/turbo) work with ALL pipelines (faster, fast, balanced, fidelity, transformers)\nKOTOBA_MODELS = {\"kotoba-bilingual\", \"kotoba-v2.0\", \"kotoba-v2.1\", \"kotoba-v2.2\"}\nLEGACY_PIPELINES = {\"faster\", \"fast\", \"balanced\", \"fidelity\"}\n\n# Auto-correct incompatible model-pipeline combinations\nwarnings_list = []\n\n# Check Pass 1 compatibility\nif pass1_model in KOTOBA_MODELS and pass1_quality in LEGACY_PIPELINES:\n    warnings_list.append(f\"Pass 1: {pass1_model} requires 'transformers' pipeline. Auto-correcting from '{pass1_quality}' to 'transformers'.\")\n    pass1_quality = \"transformers\"\n\n# Check Pass 2 compatibility\nif pass2_model in KOTOBA_MODELS and pass2_quality in LEGACY_PIPELINES:\n    warnings_list.append(f\"Pass 2: {pass2_model} requires 'transformers' pipeline. Auto-correcting from '{pass2_quality}' to 'transformers'.\")\n    pass2_quality = \"transformers\"\n\nWHISPERJAV_CONFIG = {\n    'pass1_pipeline': pass1_quality,\n    'pass1_sensitivity': pass1_sensitivity,\n    'pass1_speech_segmenter': segmenter_map[pass1_speech_segmenter],\n    'pass1_model': model_map[pass1_model],\n    'pass2_pipeline': pass2_quality,\n    'pass2_sensitivity': pass2_sensitivity,\n    'pass2_speech_segmenter': segmenter_map[pass2_speech_segmenter],\n    'pass2_model': model_map[pass2_model],\n    'merge_strategy': combine_map[merge_method],\n    'folder_name': folder_name,\n    'subtitle_language': language_map[subtitle_language],\n    'translation_service': translation_service,\n    'local_model': local_model,\n    'api_key': api_key,\n    'translation_style': tone_map[translation_style],\n    'opening_credit': opening_credit,\n    'closing_credit': closing_credit,\n    'auto_disconnect': auto_disconnect,\n    # Display values\n    '_pass1_quality': pass1_quality,\n    '_pass1_sensitivity': pass1_sensitivity,\n    '_pass1_speech_segmenter': pass1_speech_segmenter,\n    '_pass1_model': pass1_model,\n    '_pass2_quality': pass2_quality,\n    '_pass2_sensitivity': pass2_sensitivity,\n    '_pass2_speech_segmenter': pass2_speech_segmenter,\n    '_pass2_model': pass2_model,\n    '_merge_method': merge_method,\n    '_subtitle_language': subtitle_language,\n    '_translation_style': translation_style,\n}\n\nfrom IPython.display import display, HTML\n\n# Display any auto-correction warnings\nfor warning in warnings_list:\n    display(HTML(f'<div style=\"padding:6px 10px;background:#fef9c3;border-radius:4px;font-size:10px;margin-bottom:4px\"><b>‚ö† Auto-corrected:</b> {warning}</div>'))\n\n# Build status display\np1_info = f\"{pass1_quality}\"\nif pass1_speech_segmenter != \"automatic\":\n    p1_info += f\"/{pass1_speech_segmenter}\"\nif pass1_model != \"automatic\":\n    p1_info += f\"/{pass1_model}\"\n\np2_info = f\"{pass2_quality}\"\nif pass2_speech_segmenter != \"automatic\":\n    p2_info += f\"/{pass2_speech_segmenter}\"\nif pass2_model != \"automatic\":\n    p2_info += f\"/{pass2_model}\"\n\ndisplay(HTML(f'<div style=\"padding:6px 10px;background:#e0f2fe;border-radius:4px;font-size:10px\"><b>Parallel Mode:</b> Pass1({p1_info}) ‚áÜ Pass2({p2_info}) | Merge: {merge_method} | Folder: {folder_name}</div>'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 2: Two-Pass Transcribe { display-mode: \"form\" }\n#@markdown Connect Drive ‚Üí Install ‚Üí Run passes (parallel on Kaggle, sequential on Colab) ‚Üí Merge results\n\nimport os, sys, subprocess, shlex, time, re\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom IPython.display import display, HTML, clear_output\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Tuple\n\ndef status(msg, ok=True):\n    icon = \"‚úì\" if ok else \"‚úó\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'‚îÄ'*50}\\n{title}\\n{'‚îÄ'*50}\")\n\n# Check config\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\ncfg = WHISPERJAV_CONFIG\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PRE-FLIGHT CHECKS\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"PRE-FLIGHT CHECKS\")\n\n# Python version check - WhisperJAV requires Python 3.10-3.12\npy_version = sys.version_info\nprint(f\"Python: {sys.version.split()[0]}\")\nif py_version >= (3, 13):\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Python 3.13+ not supported. WhisperJAV requires Python 3.10-3.12 (openai-whisper incompatible)</div>'))\n    raise SystemExit(f\"Python {sys.version.split()[0]} not supported\")\nelif py_version < (3, 10):\n    raise SystemExit(f\"Python {sys.version.split()[0]} too old. Requires 3.10-3.12.\")\nstatus(f\"Python version OK\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# CONNECT GOOGLE DRIVE\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"CONNECTING GOOGLE DRIVE\")\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    folder_path = Path(f\"/content/drive/MyDrive/{cfg['folder_name']}\")\n    folder_path.mkdir(parents=True, exist_ok=True)\n    status(f\"Connected: {folder_path}\")\nexcept Exception as e:\n    status(f\"Failed to connect: {e}\", False)\n    raise SystemExit()\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# CHECK GPUs AND DETERMINE MODE\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"DETECTING PLATFORM\")\ngpu_check = subprocess.run(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\", shell=True, capture_output=True, text=True)\nif gpu_check.returncode != 0 or not gpu_check.stdout.strip():\n    status(\"No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\", False)\n    raise SystemExit()\n\ngpu_lines = [line.strip() for line in gpu_check.stdout.strip().split('\\n') if line.strip()]\nnum_gpus = len(gpu_lines)\n\nfor i, gpu_info in enumerate(gpu_lines):\n    status(f\"GPU {i}: {gpu_info}\")\n\n# Adaptive mode selection\nif num_gpus >= 2:\n    PARALLEL_MODE = True\n    gpu_assignment = {1: \"0\", 2: \"1\"}\n    print(f\"\\n  ‚ö° Kaggle Mode: PARALLEL (Pass 1 ‚Üí GPU 0, Pass 2 ‚Üí GPU 1)\")\nelse:\n    PARALLEL_MODE = False\n    gpu_assignment = {1: \"0\", 2: \"0\"}\n    print(f\"\\n  üìù Colab Mode: SEQUENTIAL (avoids memory contention)\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# INSTALL WHISPERJAV\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"INSTALLING (2-3 min)\")\ninstall_start = time.time()\n\n# Pinned to v1.8.2rc1 for stability\nsteps = [\n    (\"apt-get update -qq && apt-get install -y -qq ffmpeg portaudio19-dev libc++1 libc++abi1 > /dev/null 2>&1\", \"System tools\"),\n    (\"pip install -q tqdm numba>=0.58.0,<0.60.0 tiktoken ffmpeg-python soundfile auditok numpy>=1.26.0,<2.0 scipy pysrt srt aiofiles jsonschema Pillow colorama librosa matplotlib pyloudnorm requests faster-whisper transformers optimum accelerate huggingface-hub pydantic ten-vad silero-vad pydub regex modelscope addict\", \"Python packages\"),\n    (\"pip install -q --no-deps git+https://github.com/openai/whisper.git@main\", \"Whisper\"),\n    (\"pip install -q --no-deps git+https://github.com/meizhong986/stable-ts-fix-setup.git@main\", \"Stable-TS\"),\n    (\"pip install -q git+https://github.com/meizhong986/WhisperJAV.git@v1.8.2rc1\", \"WhisperJAV\")\n]\n\nfor cmd, name in steps:\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        status(f\"{name} failed\", False)\n        raise SystemExit()\n    status(name)\n\nstatus(f\"Installation complete ({time.time()-install_start:.0f}s)\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# FIND MEDIA FILES\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"SCANNING FILES\")\nvideo_types = {'.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mp3', '.wav', '.flac', '.m4a'}\nvideos = [f for f in folder_path.iterdir() if f.suffix.lower() in video_types]\n\nif not videos:\n    status(f\"No media files in {cfg['folder_name']}/\", False)\n    raise SystemExit()\n\nstatus(f\"Found {len(videos)} file(s)\")\nfor v in videos[:5]:\n    print(f\"  ‚Ä¢ {v.name}\")\nif len(videos) > 5:\n    print(f\"  ... and {len(videos)-5} more\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# MERGE FUNCTIONS (from whisperjav/ensemble/merge.py)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n@dataclass\nclass Subtitle:\n    index: int\n    start_time: float\n    end_time: float\n    text: str\n\n    @property\n    def duration(self) -> float:\n        return self.end_time - self.start_time\n\ndef parse_srt(path: Path) -> List[Subtitle]:\n    if not path.exists():\n        return []\n    subtitles = []\n    content = path.read_text(encoding='utf-8')\n    blocks = re.split(r'\\n\\s*\\n', content.strip())\n    for block in blocks:\n        if not block.strip():\n            continue\n        lines = block.strip().split('\\n')\n        if len(lines) < 3:\n            continue\n        try:\n            index = int(lines[0].strip())\n            ts_match = re.match(r'(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})\\s*-->\\s*(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})', lines[1].strip())\n            if not ts_match:\n                continue\n            g = ts_match.groups()\n            start = int(g[0])*3600 + int(g[1])*60 + int(g[2]) + int(g[3])/1000\n            end = int(g[4])*3600 + int(g[5])*60 + int(g[6]) + int(g[7])/1000\n            text = '\\n'.join(lines[2:]).strip()\n            subtitles.append(Subtitle(index, start, end, text))\n        except:\n            continue\n    return subtitles\n\ndef write_srt(subtitles: List[Subtitle], path: Path):\n    def ts(seconds):\n        h, m = int(seconds // 3600), int((seconds % 3600) // 60)\n        s, ms = int(seconds % 60), int((seconds % 1) * 1000)\n        return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n    lines = []\n    for i, sub in enumerate(subtitles, 1):\n        lines.extend([str(i), f\"{ts(sub.start_time)} --> {ts(sub.end_time)}\", sub.text, ''])\n    path.write_text('\\n'.join(lines), encoding='utf-8')\n\ndef merge_srt(srt1: Path, srt2: Path, output: Path, strategy: str) -> Dict[str, Any]:\n    subs1, subs2 = parse_srt(srt1), parse_srt(srt2)\n    \n    if strategy == 'full_merge':\n        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs1 + subs2]\n    elif strategy == 'pass1_primary':\n        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs1]\n        for s2 in subs2:\n            if not any(max(s1.start_time, s2.start_time) < min(s1.end_time, s2.end_time) for s1 in subs1):\n                merged.append(Subtitle(0, s2.start_time, s2.end_time, s2.text))\n    elif strategy == 'pass2_primary':\n        merged = [Subtitle(0, s.start_time, s.end_time, s.text) for s in subs2]\n        for s1 in subs1:\n            if not any(max(s1.start_time, s2.start_time) < min(s1.end_time, s2.end_time) for s2 in subs2):\n                merged.append(Subtitle(0, s1.start_time, s1.end_time, s1.text))\n    else:  # smart_merge\n        merged, used = [], set()\n        for s1 in subs1:\n            best_i, best_overlap = None, 0\n            for i, s2 in enumerate(subs2):\n                if i in used: continue\n                overlap = max(0, min(s1.end_time, s2.end_time) - max(s1.start_time, s2.start_time))\n                if overlap > best_overlap:\n                    best_overlap, best_i = overlap, i\n            if best_i is not None and best_overlap > 0.3 * min(s1.duration, subs2[best_i].duration):\n                used.add(best_i)\n                chosen = s1 if s1.duration <= subs2[best_i].duration else subs2[best_i]\n                merged.append(Subtitle(0, chosen.start_time, chosen.end_time, chosen.text))\n            else:\n                merged.append(Subtitle(0, s1.start_time, s1.end_time, s1.text))\n        for i, s2 in enumerate(subs2):\n            if i not in used:\n                merged.append(Subtitle(0, s2.start_time, s2.end_time, s2.text))\n    \n    merged.sort(key=lambda s: s.start_time)\n    write_srt(merged, output)\n    return {'pass1_count': len(subs1), 'pass2_count': len(subs2), 'merged_count': len(merged)}\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# TWO-PASS TRANSCRIPTION (ADAPTIVE)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"TWO-PASS TRANSCRIPTION\" + (\" (PARALLEL)\" if PARALLEL_MODE else \" (SEQUENTIAL)\"))\n\ndef build_pass_command(pass_num: int, video_path: Path, output_dir: Path, cfg: dict) -> Tuple[List[str], Path]:\n    \"\"\"Build whisperjav command for a single pass.\n\n    Note: WhisperJAV doesn't have --output-name, so we use separate directories\n    for each pass to avoid conflicts when running in parallel.\n    Output naming is automatic: {basename}.{lang_code}.whisperjav.srt\n    \"\"\"\n    # Use separate directory for each pass to avoid conflicts\n    pass_output_dir = output_dir / f\"pass{pass_num}\"\n    pass_output_dir.mkdir(parents=True, exist_ok=True)\n\n    pipeline = cfg[f'pass{pass_num}_pipeline']\n    sensitivity = cfg[f'pass{pass_num}_sensitivity']\n    segmenter = cfg[f'pass{pass_num}_speech_segmenter']\n    model = cfg[f'pass{pass_num}_model']\n\n    cmd = ['whisperjav', str(video_path), '--output-dir', str(pass_output_dir),\n           '--mode', pipeline, '--sensitivity', sensitivity]\n\n    if segmenter:\n        cmd.extend(['--speech-segmenter', segmenter])\n\n    if model:\n        cmd.extend(['--model', model])\n\n    if cfg['subtitle_language'] == 'direct-to-english':\n        cmd.extend(['--subs-language', 'direct-to-english'])\n    else:\n        cmd.extend(['--subs-language', 'native'])\n\n    # Return the pass output directory - we'll find the SRT file after processing\n    return cmd, pass_output_dir\n\ndef find_output_srt(pass_output_dir: Path, video_name: str) -> Path:\n    \"\"\"Find the generated SRT file in the pass output directory.\n\n    WhisperJAV auto-generates: {basename}.{lang}.whisperjav.srt\n    e.g., video.ja.whisperjav.srt or video.en.whisperjav.srt\n    \"\"\"\n    base_name = Path(video_name).stem\n    # Look for any SRT file matching the video name\n    patterns = [\n        f\"{base_name}.*.whisperjav.srt\",  # Standard format\n        f\"{base_name}.srt\",                # Fallback\n        f\"{base_name}*.srt\",               # Any SRT with base name\n    ]\n    for pattern in patterns:\n        matches = list(pass_output_dir.glob(pattern))\n        if matches:\n            return matches[0]\n    # Last resort: any SRT in directory\n    all_srts = list(pass_output_dir.glob(\"*.srt\"))\n    return all_srts[0] if all_srts else None\n\ndef run_pass(pass_num: int, video: Path, output_dir: Path, cfg: dict, gpu_id: str) -> Dict:\n    \"\"\"Run a single pass on a specific GPU.\"\"\"\n    cmd, pass_output_dir = build_pass_command(pass_num, video, output_dir, cfg)\n\n    env = os.environ.copy()\n    env['CUDA_VISIBLE_DEVICES'] = gpu_id\n\n    start_time = time.time()\n    result = subprocess.run(shlex.join(cmd), shell=True, capture_output=True, text=True, env=env)\n    elapsed = time.time() - start_time\n\n    # Find the output SRT file\n    actual_output = find_output_srt(pass_output_dir, video.name)\n\n    return {\n        'pass': pass_num,\n        'video': video.name,\n        'success': result.returncode == 0 and actual_output and actual_output.exists(),\n        'output': actual_output,\n        'output_dir': pass_output_dir,\n        'elapsed': elapsed,\n        'gpu': gpu_id,\n        'stderr': result.stderr[-500:] if result.stderr else ''  # Last 500 chars for debugging\n    }\n\n# Process each video\nall_results = []\nmerged_outputs = []\n\nfor video_idx, video in enumerate(videos, 1):\n    print(f\"\\n[{video_idx}/{len(videos)}] Processing: {video.name}\")\n\n    results = {}\n\n    if PARALLEL_MODE:\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # KAGGLE: Run both passes in parallel on separate GPUs\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            futures = {\n                executor.submit(run_pass, 1, video, folder_path, cfg, gpu_assignment[1]): 1,\n                executor.submit(run_pass, 2, video, folder_path, cfg, gpu_assignment[2]): 2\n            }\n\n            for future in as_completed(futures):\n                pass_num = futures[future]\n                result = future.result()\n                results[pass_num] = result\n                status_icon = \"‚úì\" if result['success'] else \"‚úó\"\n                print(f\"    {status_icon} Pass {pass_num} (GPU {result['gpu']}): {result['elapsed']:.1f}s\")\n                if not result['success'] and result['stderr']:\n                    print(f\"        Error: {result['stderr'][:200]}\")\n    else:\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # COLAB: Run passes sequentially on same GPU\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        for pass_num in [1, 2]:\n            result = run_pass(pass_num, video, folder_path, cfg, gpu_assignment[pass_num])\n            results[pass_num] = result\n            status_icon = \"‚úì\" if result['success'] else \"‚úó\"\n            print(f\"    {status_icon} Pass {pass_num}: {result['elapsed']:.1f}s\")\n            if not result['success'] and result['stderr']:\n                print(f\"        Error: {result['stderr'][:200]}\")\n\n    # Merge results if both passes succeeded\n    if results[1]['success'] and results[2]['success']:\n        merged_output = folder_path / f\"{video.stem}.merged.whisperjav.srt\"\n        stats = merge_srt(results[1]['output'], results[2]['output'], merged_output, cfg['merge_strategy'])\n        print(f\"    ‚úì Merged: {stats['pass1_count']} + {stats['pass2_count']} ‚Üí {stats['merged_count']} subtitles\")\n        merged_outputs.append(merged_output)\n    else:\n        # Use whichever pass succeeded\n        for p in [1, 2]:\n            if results[p]['success']:\n                # Copy to main folder with consistent naming\n                final_output = folder_path / f\"{video.stem}.whisperjav.srt\"\n                import shutil\n                shutil.copy2(results[p]['output'], final_output)\n                merged_outputs.append(final_output)\n                print(f\"    ‚ö† Using Pass {p} only (other pass failed)\")\n                break\n        else:\n            print(f\"    ‚úó Both passes failed!\")\n\n    all_results.append(results)\n\n# Store for Step 3\nWHISPERJAV_NEW_SRTS = merged_outputs\nWHISPERJAV_FOLDER_PATH = folder_path\n\nstatus(f\"\\nCreated {len(merged_outputs)} merged subtitle file(s)\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ADD CREDITS\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"ADDING CREDITS\")\n\nif cfg['opening_credit'] or cfg['closing_credit']:\n    credits_count = 0\n    for srt_file in merged_outputs:\n        try:\n            content = srt_file.read_text(encoding='utf-8')\n            if cfg['opening_credit']:\n                content = f\"0\\n00:00:00,000 --> 00:00:00,500\\n{cfg['opening_credit']}\\n\\n\" + content\n            if cfg['closing_credit']:\n                content += f\"\\n9999\\n23:59:58,000 --> 23:59:59,000\\n{cfg['closing_credit']}\\n\"\n            srt_file.write_text(content, encoding='utf-8')\n            credits_count += 1\n        except Exception as e:\n            print(f\"  Warning: Could not add credits to {srt_file.name}: {e}\")\n    status(f\"Credits added to {credits_count} file(s)\")\nelse:\n    status(\"No credits configured\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# COMPLETE\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nsection(\"TWO-PASS TRANSCRIPTION COMPLETE\")\n\nmode_text = \"parallel\" if PARALLEL_MODE else \"sequential\"\nif cfg['subtitle_language'] == 'llm' and cfg['api_key']:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>‚úì Transcription done ({mode_text})!</b> {len(merged_outputs)} file(s). AI Translation will start next...</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>‚úì Done ({mode_text})!</b> {len(merged_outputs)} subtitle(s) saved to Google Drive/{cfg[\"folder_name\"]}/</div>'))\n    if cfg['subtitle_language'] == 'llm' and not cfg['api_key']:\n        print(\"Note: AI translation skipped (no API key provided)\")\n\n    if cfg['auto_disconnect']:\n        print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n        time.sleep(10)\n        try:\n            from google.colab import runtime\n            runtime.unassign()\n        except: pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 3: AI Translation (if selected) { display-mode: \"form\" }\n#@markdown Translate each subtitle file using AI (only runs if \"English (AI translate)\" selected)\n\nimport os, sys, subprocess, shlex, time\nfrom pathlib import Path\nfrom IPython.display import display, HTML\n\ndef status(msg, ok=True):\n    icon = \"‚úì\" if ok else \"‚úó\"\n    print(f\"{icon} {msg}\")\n\ndef section(title):\n    print(f\"\\n{'‚îÄ'*40}\\n{title}\\n{'‚îÄ'*40}\")\n\n# Check prerequisites\nif 'WHISPERJAV_CONFIG' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 1 first</div>'))\n    raise SystemExit()\n\nif 'WHISPERJAV_NEW_SRTS' not in dir():\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> Run Step 2 first</div>'))\n    raise SystemExit()\n\ncfg = WHISPERJAV_CONFIG\nnew_srts = WHISPERJAV_NEW_SRTS\nfolder_path = WHISPERJAV_FOLDER_PATH\n\n# Check if AI translation is needed\nif cfg['subtitle_language'] != 'llm':\n    display(HTML('<div style=\"padding:8px 10px;background:#f0f9ff;border-radius:4px;border-left:2px solid #3b82f6;font-size:10px\"><b>‚Ñπ Skipped:</b> AI translation not selected</div>'))\n    raise SystemExit()\n\n# Check API key requirement (not needed for local provider)\nis_local = cfg['translation_service'] == 'local'\nif not is_local and not cfg['api_key']:\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No API key provided for cloud translation. Use \"local\" provider for free GPU translation.</div>'))\n    raise SystemExit()\n\nif not new_srts:\n    display(HTML('<div style=\"padding:8px;background:#fef2f2;border-radius:4px;color:#991b1b;font-size:10px\"><b>Error:</b> No subtitle files to translate</div>'))\n    raise SystemExit()\n\n# Set up API key (not needed for local)\nif not is_local:\n    env_map = {\n        \"deepseek\": \"DEEPSEEK_API_KEY\",\n        \"openrouter\": \"OPENROUTER_API_KEY\",\n        \"gemini\": \"GEMINI_API_KEY\",\n        \"claude\": \"ANTHROPIC_API_KEY\",\n        \"gpt\": \"OPENAI_API_KEY\"\n    }\n    os.environ[env_map.get(cfg['translation_service'], \"API_KEY\")] = cfg['api_key']\n\n# Translate each SRT file\nsection(\"AI TRANSLATION\")\nif is_local:\n    print(f\"Provider: local ({cfg.get('local_model', 'gemma-9b')})\")\n    print(\"Note: First run downloads model (~5GB) and llama-cpp-python (~700MB)\")\nelse:\n    print(f\"Provider: {cfg['translation_service']}\")\nprint(f\"Style: {cfg['_translation_style']}\")\nprint(f\"Files to translate: {len(new_srts)}\\n\")\n\ntranslated_files = []\nfailed_files = []\n\nfor i, srt_file in enumerate(new_srts, 1):\n    print(f\"[{i}/{len(new_srts)}] Translating: {srt_file.name}\")\n\n    translate_cmd = [\n        'whisperjav-translate',\n        '-i', str(srt_file),\n        '--provider', cfg['translation_service'],\n        '-t', 'english',\n        '--tone', cfg['translation_style'],\n        '--stream'\n    ]\n\n    # Add model for local provider\n    if is_local:\n        translate_cmd.extend(['--model', cfg.get('local_model', 'gemma-9b')])\n\n    full_cmd = shlex.join(translate_cmd)\n\n    try:\n        process = subprocess.Popen(\n            full_cmd,\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            bufsize=1,\n            universal_newlines=True\n        )\n\n        for line in process.stderr:\n            print(f\"    {line}\", end='')\n\n        stdout_output, _ = process.communicate()\n\n        if process.returncode == 0:\n            output_path = stdout_output.strip()\n            if output_path:\n                translated_files.append(Path(output_path))\n            status(f\"Completed: {srt_file.name}\")\n        else:\n            status(f\"Failed: {srt_file.name}\", False)\n            failed_files.append(srt_file)\n\n    except Exception as e:\n        status(f\"Error translating {srt_file.name}: {e}\", False)\n        failed_files.append(srt_file)\n\n    print()\n\n# Complete\nsection(\"COMPLETE\")\n\nif failed_files:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#fef9c3;border-radius:4px;border-left:2px solid #ca8a04;font-size:10px\"><b>‚ö† Partially done!</b> {len(translated_files)}/{len(new_srts)} translated. {len(failed_files)} failed.</div>'))\nelse:\n    display(HTML(f'<div style=\"padding:8px 10px;background:#f0fdf4;border-radius:4px;border-left:2px solid #16a34a;font-size:10px\"><b>‚úì All done!</b> {len(new_srts)} Japanese + {len(translated_files)} English subtitle(s) in Google Drive/{cfg[\"folder_name\"]}/</div>'))\n\n# Auto-disconnect\nif cfg['auto_disconnect']:\n    print(\"\\nAuto-disconnecting in 10s to save GPU credits...\")\n    time.sleep(10)\n    try:\n        from google.colab import runtime\n        runtime.unassign()\n    except: pass\nelse:\n    print(\"\\nRemember to disconnect manually to save GPU credits.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}