<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Qwen3-ASR Pipeline — Parameter Guide</title>
<style>
  :root {
    --bg: #1a1b2e;
    --bg-card: #232540;
    --bg-code: #191a2e;
    --border: #353760;
    --text: #e0e0ef;
    --text-dim: #9a9bbc;
    --accent: #6366f1;
    --accent-dim: #4f46e5;
    --green: #34d399;
    --orange: #fb923c;
    --red: #f87171;
    --blue: #60a5fa;
  }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.6;
    padding: 32px 24px;
    max-width: 900px;
    margin: 0 auto;
  }
  h1 {
    font-size: 22px;
    font-weight: 700;
    margin-bottom: 6px;
    color: #fff;
  }
  .subtitle {
    color: var(--text-dim);
    font-size: 13px;
    margin-bottom: 28px;
  }

  /* Flow diagram */
  .flow {
    background: var(--bg-code);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 16px 20px;
    margin-bottom: 32px;
    font-family: 'Cascadia Code', 'Fira Code', monospace;
    font-size: 12px;
    line-height: 2;
    color: var(--text-dim);
    overflow-x: auto;
  }
  .flow .stage {
    color: var(--accent);
    font-weight: 600;
  }
  .flow .arrow { color: var(--text-dim); }

  /* Sections */
  .section {
    margin-bottom: 32px;
  }
  .section-header {
    display: flex;
    align-items: center;
    gap: 10px;
    margin-bottom: 16px;
    padding-bottom: 8px;
    border-bottom: 1px solid var(--border);
  }
  .section-num {
    background: var(--accent);
    color: #fff;
    width: 26px; height: 26px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 13px;
    font-weight: 700;
    flex-shrink: 0;
  }
  .section-num.row-icon {
    background: var(--green);
  }
  .section-title {
    font-size: 17px;
    font-weight: 600;
    color: #fff;
  }
  .section-tab {
    font-size: 11px;
    color: var(--text-dim);
    background: var(--bg-code);
    border: 1px solid var(--border);
    border-radius: 4px;
    padding: 2px 8px;
    margin-left: auto;
  }

  /* Parameter cards */
  .param {
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 14px 18px;
    margin-bottom: 10px;
  }
  .param-name {
    font-weight: 600;
    font-size: 14px;
    color: #fff;
  }
  .param-default {
    font-size: 12px;
    color: var(--green);
    margin-left: 8px;
    font-weight: 400;
  }
  .param-desc {
    font-size: 13px;
    color: var(--text);
    margin-top: 6px;
  }
  .param-when {
    font-size: 12px;
    color: var(--orange);
    margin-top: 6px;
    padding-left: 12px;
    border-left: 2px solid var(--orange);
  }
  .param-when strong { color: var(--orange); }
  .param-cli {
    font-size: 12px;
    color: var(--text-dim);
    margin-top: 6px;
    padding-left: 12px;
    border-left: 2px solid var(--blue);
  }
  .param-cli strong { color: var(--blue); }

  /* Subsection headers within a section */
  .group-header {
    font-size: 13px;
    font-weight: 600;
    color: var(--accent);
    text-transform: uppercase;
    letter-spacing: 0.5px;
    margin: 18px 0 8px 0;
  }

  /* Inline code */
  code {
    background: var(--bg-code);
    border: 1px solid var(--border);
    border-radius: 3px;
    padding: 1px 5px;
    font-family: 'Cascadia Code', 'Fira Code', monospace;
    font-size: 12px;
  }

  /* Tables */
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 10px 0;
    font-size: 13px;
  }
  th {
    background: var(--bg-code);
    color: var(--accent);
    font-weight: 600;
    text-align: left;
    padding: 8px 12px;
    border: 1px solid var(--border);
    font-size: 12px;
    text-transform: uppercase;
    letter-spacing: 0.3px;
  }
  td {
    padding: 7px 12px;
    border: 1px solid var(--border);
    vertical-align: top;
  }
  tr:nth-child(even) td {
    background: rgba(35, 37, 64, 0.5);
  }
  td code {
    font-size: 11px;
  }

  /* Divider between row controls and modal tabs */
  .divider {
    border: none;
    border-top: 2px solid var(--accent-dim);
    margin: 40px 0 36px;
  }
  .divider-label {
    text-align: center;
    color: var(--accent);
    font-size: 12px;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 1px;
    margin-bottom: 28px;
  }

  /* Footer */
  .footer {
    margin-top: 40px;
    padding-top: 16px;
    border-top: 1px solid var(--border);
    color: var(--text-dim);
    font-size: 12px;
    text-align: center;
  }

  /* CLI note */
  .cli-note {
    font-size: 12px;
    color: var(--text-dim);
    margin-top: 6px;
    padding-left: 12px;
    border-left: 2px solid var(--border);
  }
  .cli-note strong { color: var(--blue); }
</style>
</head>
<body>

<h1>Qwen3-ASR Pipeline &mdash; Parameter Guide</h1>
<p class="subtitle">Complete reference for all Qwen3-ASR controls: ensemble row dropdowns and the Customize Parameters modal. Defaults work well for most JAV content.</p>

<!-- Pipeline flow diagram -->
<div class="flow">
  <span class="stage">Audio</span> <span class="arrow">&rarr;</span>
  <span class="stage">Scene Detection</span> <span class="arrow">&rarr;</span>
  <span class="stage">Enhancement</span> <span class="arrow">&rarr;</span>
  <span class="stage">VAD Segmentation</span> <span class="arrow">&rarr;</span>
  <span class="stage">Temporal Framing</span> <span class="arrow">&rarr;</span>
  <span class="stage">Text Generation</span> <span class="arrow">&rarr;</span>
  <span class="stage">Text Cleaning</span> <span class="arrow">&rarr;</span>
  <span class="stage">Forced Alignment</span> <span class="arrow">&rarr;</span>
  <span class="stage">Timestamp Resolution</span> <span class="arrow">&rarr;</span>
  <span class="stage">SRT</span>
</div>

<!-- ================================================================== -->
<!-- ENSEMBLE ROW CONTROLS                                               -->
<!-- ================================================================== -->

<!-- ====== Row: Sensitivity ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num row-icon">R</span>
    <span class="section-title">Sensitivity</span>
    <span class="section-tab">Ensemble Row</span>
  </div>

  <div class="param">
    <span class="param-name">Sensitivity Preset</span>
    <span class="param-default">Balanced</span>
    <div class="param-desc">
      Controls how the speech segmenter (VAD) detects speech vs. silence. The preset
      automatically adjusts detection thresholds, padding, and duration filters for the
      selected segmenter backend. This is the single most impactful setting for capturing
      or ignoring quiet speech.
    </div>
    <div class="param-desc" style="margin-top: 10px;">
      <table>
        <tr>
          <th>Preset</th>
          <th>Behaviour</th>
          <th>Best for</th>
        </tr>
        <tr>
          <td><strong>Aggressive</strong></td>
          <td>Low threshold (0.2), tight padding (150ms), keeps very short utterances (50ms+).
              Detects speech early &mdash; less padding needed since boundaries are already accurate.
              Captures soft whispers, faint moaning, trailing particles.</td>
          <td>Noisy audio, soft speech, whispered dialogue, maximum subtitle coverage</td>
        </tr>
        <tr>
          <td><strong>Balanced</strong></td>
          <td>Default thresholds (0.35), moderate padding (250ms), 100ms min speech.
              Good tradeoff between capturing speech and avoiding false positives.</td>
          <td>Most JAV content, general use</td>
        </tr>
        <tr>
          <td><strong>Conservative</strong></td>
          <td>High threshold (0.5), compensating padding (350ms), 150ms min speech.
              Detects speech later &mdash; extra padding recovers the clipped edges.
              Only captures clear, confident speech.</td>
          <td>Clean studio audio, or when you want fewer subtitles (skip background chatter)</td>
        </tr>
      </table>
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Start with Balanced. Switch to Aggressive if subtitles are
      missing quiet dialogue. Switch to Conservative if you see too many spurious subtitle lines
      triggered by music or background noise.
    </div>
    <div class="cli-note">
      <strong>CLI:</strong> <code>--qwen-sensitivity aggressive|balanced|conservative</code>
    </div>
  </div>

  <div class="param">
    <span class="param-name">Sensitivity + Custom Parameters</span>
    <div class="param-desc">
      Sensitivity acts as a <em>base layer</em>. If you also set custom segmenter parameters
      in the Customize Parameters modal (e.g., a specific <code>threshold</code> value), your
      custom values override the matching preset values. Non-overridden preset values are kept.
      This lets you start from a preset and fine-tune individual parameters.
    </div>
  </div>
</div>

<!-- ====== Row: Scene Detector ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num row-icon">R</span>
    <span class="section-title">Scene Detector</span>
    <span class="section-tab">Ensemble Row</span>
  </div>

  <div class="param">
    <span class="param-name">Scene Detection Method</span>
    <span class="param-default">Semantic</span>
    <div class="param-desc">
      Splits the full audio into scenes (12&ndash;48s chunks) before any other processing. Each scene
      is processed independently through the pipeline. The method controls <em>how</em> boundaries
      are found:
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>Semantic</strong> &mdash; Uses audio embeddings and clustering to find natural scene
      transitions. Has true merge logic that guarantees the minimum scene duration is respected.
      Best for Qwen (default).<br>
      <strong>Auditok</strong> &mdash; Energy-based silence detection with a two-pass coarse+fine
      strategy. Fast and reliable; default for Whisper pipelines.<br>
      <strong>Silero</strong> &mdash; Uses the Silero VAD model for scene-level silence detection.
      Different from the segmenter &mdash; this runs at scene scale, not frame scale.<br>
      <strong>None</strong> &mdash; No scene splitting. Entire audio is processed as one scene.
      Only use for very short files (&lt;3 minutes).
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Leave on Semantic for Qwen. Switch to Auditok if
      semantic detection produces unnatural splits (rare). Use None only for short clips.
    </div>
  </div>
</div>

<!-- ====== Row: Speech Enhancer ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num row-icon">R</span>
    <span class="section-title">Speech Enhancer</span>
    <span class="section-tab">Ensemble Row</span>
  </div>

  <div class="param">
    <span class="param-name">Speech Enhancement Backend</span>
    <span class="param-default">None</span>
    <div class="param-desc">
      Optional audio preprocessing that runs per-scene after scene detection. Cleans up audio
      before it reaches the VAD segmenter and ASR model. Enhancement uses additional VRAM.
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>None</strong> &mdash; No enhancement. Raw audio goes directly to segmentation. Recommended
      for most content as Qwen3-ASR handles noise well on its own.<br>
      <strong>FFmpeg DSP</strong> &mdash; Lightweight audio filters (loudnorm, denoise) running in FFmpeg.
      Zero GPU cost. Good for normalizing volume levels.<br>
      <strong>ZipEnhancer (Torch)</strong> &mdash; Neural speech enhancement at 16kHz. Effective but
      adds ~1GB VRAM. Good for noisy recordings.<br>
      <strong>ClearVoice</strong> &mdash; Multiple denoising models (FRCRN 16kHz, MossFormerGAN 16kHz).
      Use FRCRN for general denoising.<br>
      <strong>BS-RoFormer</strong> &mdash; Vocal isolation (separates speech from music/background).
      Not yet available.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Only enable if the audio has significant background noise,
      music, or volume inconsistencies that are causing missed or garbled subtitles.
    </div>
  </div>
</div>

<!-- ====== Row: Speech Segmenter ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num row-icon">R</span>
    <span class="section-title">Speech Segmenter</span>
    <span class="section-tab">Ensemble Row</span>
  </div>

  <div class="param">
    <span class="param-name">Segmenter Backend</span>
    <span class="param-default">Silero v6.2</span>
    <div class="param-desc">
      Within each scene, the segmenter runs Voice Activity Detection (VAD) to find exactly where
      speech occurs. Detected speech regions are grouped into frames (max 6s each) and sent to the
      ASR model. The sensitivity preset above controls this backend's parameters.
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>Silero v6.2</strong> &mdash; Recommended. Force-splits long speech at internal silences
      (<code>max_speech_duration_s</code>), hysteresis via <code>neg_threshold</code> for stable
      segmentation in noisy audio. Handles fast Japanese dialogue well.<br>
      <strong>TEN VAD</strong> &mdash; Previous default. Lightweight energy-based VAD. Fast but can
      miss pauses in rapid dialogue with background audio.<br>
      <strong>Silero v4.0 / v3.1</strong> &mdash; Older Silero versions without force-split or hysteresis.
      Use only if v6.2 causes issues.<br>
      <strong>Whisper VAD</strong> &mdash; Uses a Whisper model internally for VAD. Accurate but slow
      and VRAM-heavy. Variants: tiny, small, medium.<br>
      <strong>NeMo Lite</strong> &mdash; NVIDIA NeMo-based VAD. Requires <code>nemo_toolkit</code>
      (not included in default install).<br>
      <strong>None</strong> &mdash; Skip segmentation. Entire scene goes to ASR as one chunk.
      Only use with Full Scene framing.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Leave on Silero v6.2 for most content. Try TEN if Silero
      produces too many tiny segments. Use None only with Full Scene framing for short scenes.
    </div>
  </div>
</div>

<!-- ====== Row: Model ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num row-icon">R</span>
    <span class="section-title">Model</span>
    <span class="section-tab">Ensemble Row</span>
  </div>

  <div class="param">
    <span class="param-name">ASR Model</span>
    <span class="param-default">Qwen3-ASR-1.7B</span>
    <div class="param-desc">
      Quick model selection in the ensemble row. The 1.7B model requires ~8GB VRAM; the 0.6B requires
      ~4GB. This is the same as the ASR Model setting in the Customize Parameters modal (Tab 1) &mdash;
      changing it in either place updates both.
    </div>
  </div>
</div>


<hr class="divider">
<div class="divider-label">Customize Parameters Modal</div>


<!-- ================================================================== -->
<!-- CUSTOMIZE PARAMETERS MODAL (5 tabs)                                -->
<!-- ================================================================== -->

<!-- ====== Section 1: Model ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num">1</span>
    <span class="section-title">Model</span>
    <span class="section-tab">Tab 1</span>
  </div>

  <div class="param">
    <span class="param-name">ASR Model</span>
    <span class="param-default">Qwen3-ASR-1.7B</span>
    <div class="param-desc">
      The speech recognition model that converts audio to text. The 1.7B parameter model is more
      accurate and handles complex speech better. The 0.6B model is faster and uses roughly half
      the VRAM (4GB vs 8GB).
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Switch to 0.6B if your GPU has less than 8GB VRAM,
      or if processing speed matters more than accuracy.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Language</span>
    <span class="param-default">Japanese</span>
    <div class="param-desc">
      Forces the model to transcribe in a specific language instead of auto-detecting. For JAV
      content, forcing Japanese avoids the model occasionally switching to Chinese for similar-sounding
      phonemes.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Use Auto-detect for multilingual content, or force English/Chinese
      if the primary language is not Japanese.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Context Hints</span>
    <span class="param-default">(empty)</span>
    <div class="param-desc">
      Free-text hints that help the model recognize specific names and terms. The model uses
      this as context for improved accuracy on proper nouns. Enter actress names, studio names,
      or domain-specific terminology.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Always fill this in when you know the actress name or the video
      contains specialized vocabulary. Example: <code>七沢みあ MOODYZ</code>
    </div>
  </div>

  <p class="group-header">Hardware</p>

  <div class="param">
    <span class="param-name">Device</span>
    <span class="param-default">Auto</span>
    <div class="param-desc">
      Where the model runs. Auto detects your GPU; CUDA forces GPU; CPU forces processor-only
      (much slower). Most users should leave this on Auto.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Force CPU if you encounter CUDA out-of-memory errors and
      don't want to switch to the 0.6B model.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Data Type</span>
    <span class="param-default">Auto</span>
    <div class="param-desc">
      Model precision. Float16 is fastest on most NVIDIA GPUs. BFloat16 is best on Ampere+ GPUs
      (RTX 30xx/40xx). Float32 uses more memory and is slower but may be needed on older hardware.
      Auto selects the best option for your GPU.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Try Float16 explicitly if Auto seems slow, or Float32 if you
      see NaN/garbled output (rare).
    </div>
  </div>

  <div class="param">
    <span class="param-name">Attention</span>
    <span class="param-default">Auto</span>
    <div class="param-desc">
      The attention algorithm used internally. SDPA (Scaled Dot-Product Attention) is fastest on
      most GPUs. Flash Attention 2 requires a separate install and specific hardware.
      Eager is the slowest but most compatible fallback.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Only if you have Flash Attention 2 installed and want to try
      it, or if you encounter attention-related errors (switch to Eager).
    </div>
  </div>
</div>

<!-- ====== Section 2: Audio ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num">2</span>
    <span class="section-title">Audio</span>
    <span class="section-tab">Tab 2</span>
  </div>

  <p class="group-header">Temporal Framing</p>

  <div class="param">
    <span class="param-name">Temporal Framing</span>
    <span class="param-default">VAD Grouped</span>
    <div class="param-desc">
      Controls how audio is divided into frames for text generation. Each frame is sent to
      the ASR model as an independent unit. This setting determines what the model &ldquo;sees&rdquo;
      at a time.
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>VAD Grouped</strong> &mdash; Groups VAD speech segments into frames up to Max Group Duration
      (default 6s). Each frame contains only detected speech regions with natural pause boundaries.
      Best accuracy for most content.<br>
      <strong>Full Scene</strong> &mdash; Sends the entire scene (12&ndash;48s) as a single frame. The model
      gets maximum audio context but may struggle with precise timing on long scenes. Can produce
      better results for content with continuous dialogue and few pauses.<br>
      <strong>SRT Source</strong> &mdash; Uses an existing SRT file to define frame boundaries. Useful
      for re-transcription or translation workflows where timing from a reference subtitle is desired.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Leave on VAD Grouped for most content. Try Full Scene if
      VAD Grouped is splitting mid-sentence in rapid dialogue. Use SRT Source only for re-processing
      with reference timing.
    </div>
    <div class="cli-note">
      <strong>CLI:</strong> <code>--qwen-framer vad-grouped|full-scene|srt-source</code>
    </div>
  </div>

  <p class="group-header">Scene Detection</p>

  <div class="param">
    <span class="param-name">Safe Chunking</span>
    <span class="param-default">On</span>
    <div class="param-desc">
      Enforces scene boundaries so no audio segment exceeds the ForcedAligner's 180-second (3-minute)
      processing limit. When enabled, any scene longer than 180s is automatically re-split at silence
      boundaries. Disabling this risks aligner failures on long scenes.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Only disable if you've also disabled the ForcedAligner entirely
      (Aligner Backend = None). Otherwise, always keep this on.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Min Scene Duration</span>
    <span class="param-default">12s</span>
    <div class="param-desc">
      Minimum length for a detected scene. Scenes shorter than this are merged with their neighbors.
      Too low and you get fragmented tiny scenes with lost context. Too high and the detector
      can't split at natural boundaries.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Lower to 6-8s for rapid dialogue with many short exchanges.
      Raise to 20-30s for long monologue content.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Max Scene Duration</span>
    <span class="param-default">48s</span>
    <div class="param-desc">
      Maximum length for a detected scene. Scenes longer than this are forcibly split. The default
      of 48s keeps scenes well within the aligner's 180s limit while preserving context.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Increase to 90-120s if you see natural conversations getting
      cut mid-sentence. Decrease to 30s if scenes feel too long for accurate alignment.
    </div>
  </div>

  <p class="group-header">VAD Grouping</p>

  <div class="param">
    <span class="param-name">Frame Gap Threshold</span>
    <span class="param-default">1.0s</span>
    <div class="param-desc">
      When using VAD Grouped framing, this sets the maximum silence gap (in seconds) allowed within a
      single frame. If a silence gap between two speech segments exceeds this threshold, a new frame starts.
      Lower values produce more, smaller frames with tighter sentence boundaries. Higher values produce
      fewer, larger frames with more context per frame.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Decrease to 0.3-0.5s for rapid dialogue with clear pauses between
      sentences. Increase to 2-3s for monologue or content with long pauses within the same thought. The
      default of 1.0s works well for most conversational content.
    </div>
    <div class="param-cli">
      <strong>CLI:</strong> <code>--qwen-chunk-threshold 0.5</code>
    </div>
  </div>

  <div class="param">
    <span class="param-name">Max Group Duration</span>
    <span class="param-default">6s</span>
    <div class="param-desc">
      When using VAD Grouped framing, this sets the maximum duration of each group. VAD speech
      segments within a scene are grouped together until they hit this limit, then a new group
      starts. Shorter groups mean more precise text generation and alignment but add processing
      overhead.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Increase to 10-15s if you get sentence fragments or words
      split across subtitle lines. Decrease to 3-4s for very rapid dialogue where precision matters.
    </div>
  </div>

  <p class="group-header">VAD Settings</p>

  <div class="param">
    <span class="param-name">VAD Threshold</span>
    <span class="param-default">0.35</span>
    <div class="param-desc">
      Speech detection probability threshold for the VAD model. A frame of audio is classified as
      speech when the model's confidence exceeds this value. Lower values detect quieter or more
      ambiguous speech (more sensitive), higher values require clearer speech signals (fewer false
      positives). This slider <strong>overrides</strong> the value set by the sensitivity preset.
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>0.05&ndash;0.15</strong> &mdash; Very sensitive. Captures whispers and background murmuring.
      May produce false positives from music or ambient noise.<br>
      <strong>0.20&ndash;0.35</strong> &mdash; Balanced range. Good for most JAV content with varying
      vocal intensity.<br>
      <strong>0.40&ndash;0.60</strong> &mdash; Conservative. Only clear speech is detected. Quiet moans
      or whispered dialogue may be missed.<br>
      <strong>0.65&ndash;0.80</strong> &mdash; Very strict. Only loud, clear speech passes. Use for
      extremely noisy source material.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> If the sensitivity preset is close but not quite right for your
      content, use this slider to fine-tune. For example, use &ldquo;aggressive&rdquo; sensitivity but
      raise the threshold from 0.2 to 0.25 to reduce a few false positives.
    </div>
    <div class="cli-note">
      <strong>CLI:</strong> <code>--qwen-vad-threshold 0.25</code>
    </div>
  </div>

  <div class="param">
    <span class="param-name">VAD Padding (ms)</span>
    <span class="param-default">250ms</span>
    <div class="param-desc">
      Milliseconds of audio added before and after each detected speech segment. Padding ensures the
      ASR model sees the full onset and release of speech, preventing clipped words. This slider
      <strong>overrides</strong> the value set by the sensitivity preset.
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>50&ndash;150ms</strong> &mdash; Tight padding. Use with aggressive sensitivity (which
      already detects speech early) to avoid excessive silence in frames.<br>
      <strong>200&ndash;300ms</strong> &mdash; Standard range. Good default for balanced sensitivity.<br>
      <strong>350&ndash;600ms</strong> &mdash; Wide padding. Use with conservative sensitivity (which
      detects speech late) to capture soft onsets and trailing particles.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> If words are getting clipped at the start or end of subtitle
      lines, increase padding. If subtitle timestamps feel too loose with excessive silence, decrease
      padding.
    </div>
    <div class="cli-note">
      <strong>CLI:</strong> <code>--qwen-vad-padding 300</code>
    </div>
  </div>

  <div class="param">
    <span class="param-name">Sensitivity + VAD Sliders</span>
    <div class="param-desc">
      The sensitivity dropdown sets a complete preset of VAD parameters. The VAD Threshold and
      VAD Padding sliders <strong>override only the specific values you change</strong>, leaving
      other preset parameters (like <code>neg_threshold</code>, <code>min_speech_duration_ms</code>)
      intact. If you don't touch the sliders, the sensitivity preset applies fully.
    </div>
  </div>
</div>

<!-- ====== Section 3: Generation ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num">3</span>
    <span class="section-title">Generation</span>
    <span class="section-tab">Tab 3</span>
  </div>

  <div class="param">
    <span class="param-name">Batch Size</span>
    <span class="param-default">1</span>
    <div class="param-desc">
      How many audio frames are processed simultaneously by the ASR model. Batch size 1 processes
      frames one at a time, giving the model full attention and best accuracy. Higher values
      use more VRAM but process faster.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Increase to 2-4 if you have VRAM headroom (16GB+) and want
      faster processing. Keep at 1 for maximum accuracy or if VRAM is limited.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Max New Tokens</span>
    <span class="param-default">4096</span>
    <div class="param-desc">
      Maximum number of text tokens the model can generate per frame. 4096 tokens covers
      roughly 5-10 minutes of spoken audio. This is a safety ceiling, not a target &mdash; most
      frames use far fewer tokens.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Increase to 6144-8192 only if you see transcriptions getting
      cut off (truncated mid-sentence at the end of a frame). This is rare with default scene bounds.
    </div>
  </div>

  <p class="group-header">Generation Safety</p>

  <div class="param">
    <span class="param-name">Repetition Penalty</span>
    <span class="param-default">1.1</span>
    <div class="param-desc">
      Penalizes the model for repeating the same tokens. A value of 1.0 disables the penalty entirely.
      Values above 1.0 make repetition progressively less likely. Repetition manifests as the model
      generating the same word or phrase in a loop (e.g., <code>あああああ</code> or the same sentence repeated).
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Increase to 1.2-1.3 if you notice repetitive output loops.
      Decrease toward 1.0 if the model seems to avoid naturally repeated words in conversation
      (like <code>はい、はい</code>).
    </div>
  </div>

  <div class="param">
    <span class="param-name">Token Budget</span>
    <span class="param-default">20.0 tokens/sec</span>
    <div class="param-desc">
      Maximum tokens the model is allowed to generate per second of audio. This is a safety net
      that stops runaway generation &mdash; if the model hallucinates, it starts producing far more
      text than real speech warrants. Normal Japanese speech produces roughly 5-10 tokens per second.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Increase to 30-40 only if legitimate speech is being truncated
      (very fast speakers). Lower to 10-15 if you see excessive hallucinated text.
    </div>
  </div>
</div>

<!-- ====== Section 4: Alignment ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num">4</span>
    <span class="section-title">Alignment</span>
    <span class="section-tab">Tab 4</span>
  </div>

  <p class="group-header">Forced Aligner</p>

  <div class="param">
    <span class="param-name">Aligner Backend</span>
    <span class="param-default">Qwen3 ForcedAligner</span>
    <div class="param-desc">
      The ForcedAligner takes the generated text and the original audio, then produces precise
      word-level timestamps. This is what makes subtitles appear at the right moment. Disabling it
      (None) falls back to VAD-based timing, which is less precise. When set to None, the aligner
      model is not loaded at all, saving ~2GB VRAM.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Set to None if you have severe VRAM constraints (the aligner
      loads a separate 0.6B model) or if alignment is consistently failing on your content.
      Selecting None automatically switches timestamp mode to VAD Only.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Aligner Model</span>
    <span class="param-default">Qwen3-ForcedAligner-0.6B</span>
    <div class="param-desc">
      The model used for forced alignment. Currently only the 0.6B variant is available.
      It has a hard processing limit of 180 seconds per segment (enforced by Safe Chunking
      in the Audio tab).
    </div>
    <div class="param-when">
      <strong>When to change:</strong> No alternative currently available. Leave as-is.
    </div>
  </div>

  <p class="group-header">Text Cleaner</p>

  <div class="param">
    <span class="param-name">Text Cleaner</span>
    <span class="param-default">Qwen3 AssemblyTextCleaner</span>
    <div class="param-desc">
      Cleans up ASR output before alignment. Removes artifacts like stray punctuation, repeated
      filler sequences, and formatting issues that would confuse the aligner. Passthrough skips
      all cleaning and sends raw ASR output directly to alignment.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Switch to Passthrough if the cleaner is removing text you want
      to keep, or for debugging alignment issues (to rule out the cleaner as the cause).
    </div>
  </div>

  <p class="group-header">Timestamp Resolution</p>

  <div class="param">
    <span class="param-name">Timestamp Mode</span>
    <span class="param-default">Aligner + VAD Fallback</span>
    <div class="param-desc">
      How word timestamps are resolved from the aligner's output. The four modes offer different
      tradeoffs between precision and coverage:
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>Aligner + VAD Fallback</strong> (recommended) &mdash; Uses aligner timestamps when available;
      for words the aligner couldn't place, falls back to VAD segment boundaries with speech-aware
      gap filling. Best overall accuracy and coverage.<br>
      <strong>Aligner + Interpolation</strong> &mdash; Uses aligner timestamps, fills gaps by
      interpolating evenly between placed words. Better for sparse aligner output where VAD regions
      aren't available.<br>
      <strong>Aligner Only</strong> &mdash; Uses raw aligner timestamps with no recovery. If the
      aligner collapses, subtitles may have timing issues. Useful for studying raw aligner quality.<br>
      <strong>VAD Only</strong> &mdash; Skips the aligner entirely (not loaded, saves VRAM). All word
      timestamps come from VAD frame boundaries. Fastest but least precise. Use when aligner is
      disabled or VRAM is critical.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Leave on Aligner + VAD Fallback for best results. Try
      Aligner + Interpolation if you see subtitle lines with jagged timing. Use VAD Only if you've
      disabled the aligner backend or need to minimize VRAM usage.
    </div>
  </div>

  <p class="group-header">Step-Down Retry</p>

  <div class="param">
    <span class="param-name">Adaptive Step-Down</span>
    <span class="param-default">On</span>
    <div class="param-desc">
      When alignment fails for a scene (all word timestamps collapse to a single point), step-down
      automatically retries the scene with tighter framing &mdash; breaking the audio into smaller
      segments and re-running generation + alignment. This recovers most collapsed scenes without
      user intervention. Hidden when timestamp mode is VAD Only (not applicable).
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Disable only for debugging, or if step-down retries are making
      processing unacceptably slow on very long videos.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Tier 1 Duration</span>
    <span class="param-default">6.0s</span>
    <div class="param-desc">
      The initial group duration used when step-down is triggered. The scene is re-framed into
      segments of this length and re-processed. If this tier also fails, Tier 2 is attempted.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Decrease to 4-5s if Tier 1 retries are still collapsing.
      Increase to 10-15s if step-down is splitting sentences unnaturally.
    </div>
  </div>

  <div class="param">
    <span class="param-name">Tier 2 Duration</span>
    <span class="param-default">6.0s</span>
    <div class="param-desc">
      The fallback group duration if Tier 1 step-down also fails. This is the tightest framing
      before giving up on alignment for a scene. Smaller values give the aligner less audio
      context but are more likely to avoid collapse.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Decrease to 4-5s for maximum recovery rate. Values below 4s
      may not contain enough audio for meaningful alignment.
    </div>
  </div>
</div>

<!-- ====== Section 5: Output ====== -->
<div class="section">
  <div class="section-header">
    <span class="section-num">5</span>
    <span class="section-title">Output</span>
    <span class="section-tab">Tab 5</span>
  </div>

  <div class="param">
    <span class="param-name">Subtitle Regrouping</span>
    <span class="param-default">Standard</span>
    <div class="param-desc">
      Controls how the raw word-level output from the aligner is grouped into subtitle lines. This is
      independent of the Post-processing Preset below, which fine-tunes gap thresholds and merge behavior
      within the selected regrouping mode.
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>Standard</strong> &mdash; Full regrouping: gap-based splitting, fragment merging,
      punctuation-based splitting, and duration/character safety caps. Best readability for most content.
      Branch A uses REGROUP_JAV; Branch B uses text-only splitting (no gap heuristics, since
      VAD-only timestamps are synthetic).<br>
      <strong>Sentence Only</strong> &mdash; Text-only splitting: punctuation-based splitting and
      safety caps (80 characters, 8 seconds) without gap heuristics. Use when gap-based splitting
      causes problems like merging separate lines or splitting mid-thought. Natural sentence boundaries
      are preserved.<br>
      <strong>Off</strong> &mdash; No regrouping at all. Each word becomes its own subtitle segment.
      Produces raw word-level output for analysis, debugging, or external post-processing tools.
      Not suitable for direct viewing.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Switch to Sentence Only if you notice subtitle lines being
      merged across speaker turns or split in unnatural places. Use Off only for quality analysis or
      when feeding output into your own post-processing pipeline.
    </div>
    <div class="param-cli">
      <strong>CLI:</strong> <code>--qwen-regroup sentence_only</code> or <code>--qwen-regroup off</code>
    </div>
  </div>

  <div class="param">
    <span class="param-name">Post-processing Preset</span>
    <span class="param-default">High Moan (JAV optimized)</span>
    <div class="param-desc">
      Controls how subtitle lines are regrouped and formatted in the final SRT output. Each preset
      tunes gap-splitting thresholds, maximum subtitle duration, and merge behavior:
    </div>
    <div class="param-desc" style="margin-top: 8px; padding-left: 12px;">
      <strong>High Moan</strong> &mdash; Optimized for JAV content. Splits aggressively at short
      gaps (1.5s), caps subtitle lines at 8 seconds, and preserves breathy/moaning segments as
      separate subtitles.<br>
      <strong>Default</strong> &mdash; General-purpose regrouping. Standard gap thresholds and merge
      behavior suitable for most spoken content.<br>
      <strong>Narrative</strong> &mdash; Optimized for story-driven content with longer dialogue.
      Allows longer subtitle lines and merges more aggressively to maintain sentence flow.
    </div>
    <div class="param-when">
      <strong>When to change:</strong> Switch to Default or Narrative for interview, documentary,
      or drama content where dialogue is continuous and moaning segments are absent.
    </div>
  </div>
</div>

<div class="footer">
  WhisperJAV &mdash; Qwen3-ASR Parameter Guide &mdash; v1.8.5
</div>

</body>
</html>
